language processing and python it is easy to get our hands on millions of words of text what can we do with it assuming we can write some simple programs in this chapter we will address the following questions what can we achieve by combining simple programming techniques with large quantities of text how can we automatically extract key words and phrases that sum up the style and content of a text what tools and techniques does the python programming language provide for such work what are some of the interesting challenges of natural language processing this chapter is divided into sections that skip between two quite different styles in the computing with language sections we will take on some linguistically motivated programming tasks without necessarily explaining how they work in the closer look at python sections we will systematically review key programming concepts we ll flag the two styles in the section titles but later chapters will mix both styles without being so up front about it we hope this style of introduction gives you an authentic taste of what will come later while covering a range of elementary concepts in linguistics and computer science if you have basic familiarity with both areas you can skip to we will repeat any important points in later chapters and if you miss anything you can easily consult the online reference material at http nltk org if the material is completely new to you this chapter will raise more questions than it answers questions that are addressed in the rest of this book word computing with language texts and words we are all very familiar with text since we read and write it every day here we will treat text as raw data for the programs we write programs that manipulate and analyze it in a variety of interesting ways but before we can do this we have to get started with the python interpreter getting started with python one of the friendly things about python is that it allows you to type directly into the interactive interpreter the program that will be running your python programs you can access the python interpreter using a simple graphical interface called the interactive development environment idle on a mac you can find this under applications macpython and on windows under all programs python under unix you can run python from the shell by typing idle if this is not installed try typing python the interpreter will print a blurb about your python version simply check that you are running python or later here it is for note if you are unable to run the python interpreter you probably do not have python installed correctly please visit http python org for detailed instructions nltk works for python and if you are using one of these older versions note that the operator rounds fractional results downwards so will give you in order to get the expected behavior of division you need to type from future import division the prompt indicates that the python interpreter is now waiting for input when copying emples from this book don t type the yourself now let s begin by using python as a calculator once the interpreter has finished calculating the answer and displaying it the prompt reappears this means the python interpreter is waiting for another instruction note your turn enter a few more expressions of your own you can use asterisk for multiplication and slash for division and parentheses for bracketing expressions the preceding emples demonstrate how you can work interactively with the python interpreter experimenting with various expressions in the language to see what they do now let try a nonsensical expression to see how the interpreter handles it this produced a syntax error in python it doesn t make sense to end an instruction with a plus sign the python interpreter indicates the line where the problem occurred line of stdin which stands for standard input now that we can use the python interpreter we are ready to start working with language data getting started with nltk before going further you should install nltk downloadable for free from http nltk org follow the instructions there to download the version required for your platform once you have installed nltk start up the python interpreter as before and install the data required for the book by typing the following two commands at the python prompt then selecting the book collection as shown in figure downloading the nltk book collection browse the available packages using nltk download the collections tab on the downloader shows how the packages are grouped into sets and you should select the line labeled book to obtain all data required for the emples and exercises in this book it consists of about compressed files requiring about mb disk space the full collection of data i e all in the downloader is nearly ten times this size at the time of writing and continues to expand once the data is downloaded to your machine you can load some of it using the python interpreter the first step is to type a special command at the python prompt which tells the interpreter to load some texts for us to explore from nltk book import this says from nltk s book module load all items the book module contains all the data you will need as you read this chapter after printing a welcome message it loads the text of several books this will take a few seconds here s the command again together with the output that you will see take care to get spelling and punctuation right and remember that you don t type the any time we want to find out about these texts we just have to enter their names at the python prompt now that we can use the python interpreter and have some data to work with we are ready to get started searching text there are many ways to emine the context of a text apart from simply reading it a concordance view shows us every occurrence of a given word together with some context here we look up the word monstrous in moby dick by entering text followed by a period then the term concordance and then placing monstrous in parentheses the first time you use a concordance on a particular text it takes a few extra seconds to build an index so that subsequent searches are fast note your turn try searching for other words to save re typing you might be able to use up arrow ctrl up arrow or alt p to access the previous command and modify the word being searched you can also try searches on some of the other texts we have included for emple search sense and sensibility for the word affection using text concordance affection search the book of genesis to find out how long some people lived using text concordance lived you could look at text the inaugural address corpus to see emples of english going back to and search for words like nation terror god to see how these words have been used differently over time we ve also included text the nps chat corpus search this for unconventional words like im ur lol note that this corpus is uncensored once you have spent a little while emining these texts we hope you have a new sense of the richness and diversity of language in the next chapter you will learn how to access a broader range of text including text in languages other than english a concordance permits us to see words in context for emple we saw that monstrous occurred in contexts such as the pictures and a size what other words appear in a similar range of contexts we can find out by appending the term similar to the name of the text in question then inserting the relevant word in parentheses observe that we get different results for different texts austen uses this word quite differently from melville for her monstrous has positive connotations and sometimes functions as an intensifier like the word very the term common contexts allows us to emine just the contexts that are shared by two or more words such as monstrous and very we have to enclose these words by square brackets as well as parentheses and separate them with a comma note your turn pick another pair of words and compare their usage in two different texts using the similar and common contexts functions it is one thing to automatically detect that a particular word occurs in a text and to display some words that appear in the same context however we can also determine the location of a word in the text how many words from the beginning it appears this positional information can be displayed using a dispersion plot each stripe represents an instance of a word and each row represents the entire text in we see some striking patterns of word usage over the last years in an artificial text constructed by joining the texts of the inaugural address corpus end to end you can produce this plot as shown below you might like to try more words e g liberty constitution and different texts can you predict the dispersion of a word before you view it as before take care to get the quotes commas brackets and parentheses ectly right figure lexical dispersion plot for words in u s presidential inaugural addresses this can be used to investigate changes in language use over time note important you need to have python numpy and matplotlib packages installed in order to produce the graphical plots used in this book please see http nltk org for installation instructions note you can also plot the frequency of word usage through time using https books google com ngrams now just for fun let try generating some random text in the various styles we have just seen to do this we type the name of the text followed by the term generate we need to include the parentheses but there is nothing that goes between them note the generate method is not available in nltk but will be reinstated in a subsequent version counting vocabulary the most obvious fact about texts that emerges from the preceding emples is that they differ in the vocabulary they use in this section we will see how to use the computer to count the words in a text in a variety of useful ways as before you will jump right in and experiment with the python interpreter even though you may not have studied python systematically yet test your understanding by modifying the emples and trying the exercises at the end of the chapter let begin by finding out the length of a text from start to finish in terms of the words and punctuation symbols that appear we use the term len to get the length of something which we will apply here to the book of genesis so genesis has words and punctuation symbols or tokens a token is the technical name for a sequence of characters such as hairy his or that we want to treat as a group when we count the number of tokens in a text say the phrase to be or not to be we are counting occurrences of these sequences thus in our emple phrase there are two occurrences of to two of be and one each of or and not but there are only four distinct vocabulary items in this phrase how many distinct words does the book of genesis contain to work this out in python we have to pose the question slightly differently the vocabulary of a text is just the set of tokens that it uses since in a set all duplicates are collapsed together in python we can obtain the vocabulary items of text with the command set text when you do this many screens of words will fly past now try the following by wrapping sorted around the python expression set text we obtain a sorted list of vocabulary items beginning with various punctuation symbols and continuing with words starting with a all capitalized words precede lowercase words we discover the size of the vocabulary indirectly by asking for the number of items in the set and again we can use len to obtain this number although it has tokens this book has only distinct words or word types a word type is the form or spelling of the word independently of its specific occurrences in a text that is the word considered as a unique item of vocabulary our count of items will include punctuation symbols so we will generally call these unique items types instead of word types now let calculate a measure of the lexical richness of the text the next emple shows us that the number of distinct words is just of the total number of words or equivalently that each word is used times on average remember if you are using python to start with from future import division next let focus on particular words we can count how often a word occurs in a text and compute what percentage of the text is taken up by a specific word note your turn how many times does the word lol appear in text how much is this as a percentage of the total number of words in this text you may want to repeat such calculations on several texts but it is tedious to keep retyping the formula instead you can come up with your own name for a task like lexical diversity or percentage and associate it with a block of code now you only have to type a short name instead of one or more complete lines of python code and you can re use it as often as you like the block of code that does a task for us is called a function and we define a short name for our function with the keyword def the next emple shows how to define two new functions lexical diversity and percentage caution the python interpreter changes the prompt from to after encountering the colon at the end of the first line the prompt indicates that python expects an indented code block to appear next it is up to you to do the indentation by typing four spaces or hitting the tab key to finish the indented block just enter a blank line in the definition of lexical diversity we specify a parameter named text this parameter is a placeholder for the actual text whose lexical diversity we want to compute and reoccurs in the block of code that will run when the function is used similarly percentage is defined to take two parameters named count and total once python knows that lexical diversity and percentage are the names for specific blocks of code we can go ahead and use these functions to recap we use or call a function such as lexical diversity by typing its name followed by an open parenthesis the name of the text and then a close parenthesis these parentheses will show up often their role is to separate the name of a task such as lexical diversity from the data that the task is to be performed on such as text the data value that we place in the parentheses when we call a function is an argument to the function you have already encountered several functions in this chapter such as len set and sorted by convention we will always add an empty pair of parentheses after a function name as in len just to make clear that what we are talking about is a function rather than some other kind of python expression functions are an important concept in programming and we only mention them at the outset to give newcomers a sense of the power and creativity of programming do not worry if you find it a bit confusing right now later we will see how to use functions when tabulating data as in each row of the table will involve the same computation but with different data and we will do this repetitive work using a function table lexical diversity of various genres in the brown corpus a closer look at python texts as lists of words you have seen some important elements of the python programming language let take a few moments to review them systematically lists what is a text at one level it is a sequence of symbols on a page such as this one at another level it is a sequence of chapters made up of a sequence of sections where each section is a sequence of paragraphs and so on however for our purposes we will think of a text as nothing more than a sequence of words and punctuation here is how we represent text in python in this case the opening sentence of moby dick after the prompt we have given a name we made up sent followed by the equals sign and then some quoted words separated with commas and surrounded with brackets this bracketed material is known as a list in python it is how we store a text we can inspect it by typing the name we can ask for its length we can even apply our own lexical diversity function to it some more lists have been defined for you one for the opening sentence of each of our texts sent sent we inspect two of them here you can see the rest for yourself using the python interpreter if you get an error which says that sent is not defined you need to first type from nltk book import note your turn make up a few sentences of your own by typing a name equals sign and a list of words like this ex monty python and the holy grail repeat some of the other python operations we saw earlier in e g sorted ex len set ex ex count the a pleasant surprise is that we can use python addition operator on lists adding two lists creates a new list with everything from the first list followed by everything from the second list note this special use of the addition operation is called concatenation it combines the lists together into a single list we can concatenate sentences to build up a text we do not have to literally type the lists either we can use short names that refer to pre defined lists what if we want to add a single item to a list this is known as appending when we append to a list the list itself is updated as a result of the operation indexing lists as we have seen a text in python is a list of words represented using a combination of brackets and quotes just as with an ordinary page of text we can count up the total number of words in text with len text and count the occurrences in a text of a particular word say heaven using text count heaven with some patience we can pick out the st rd or even th word in a printed text analogously we can identify the elements of a python list by their order of occurrence in the list the number that represents this position is the item index we instruct python to show us the item that occurs at an index such as in a text by writing the name of the text followed by the index inside square brackets we can do the converse given a word find the index of when it first occurs indexes are a common way to access the words of a text or more generally the elements of any list python permits us to access sublists as well extracting manageable pieces of language from large texts a technique known as slicing indexes have some subtleties and we will explore these with the help of an artificial sentence notice that our indexes start from zero sent element zero written sent is the first word word whereas sent element is word the reason is simple the moment python accesses the content of a list from the computer memory it is already at the first element we have to tell it how many elements forward to go thus zero steps forward leaves it at the first element note this practice of counting from zero is initially confusing but typical of modern programming languages you will quickly get the hang of it if you have mastered the system of counting centuries where xy is a year in the th century or if you live in a country where the floors of a building are numbered from and so walking up n flights of stairs takes you to level n now if we accidentally use an index that is too large we get an error this time it is not a syntax error because the program fragment is syntactically correct instead it is a runtime error and it produces a traceback message that shows the context of the error followed by the name of the error indexerror and a brief explanation let take a closer look at slicing using our artificial sentence again here we verify that the slice includes sent elements at indexes and by convention m n means elements m n as the next emple shows we can omit the first number if the slice begins at the start of the list and we can omit the second number if the slice goes to the end we can modify an element of a list by assigning to one of its index values in the next emple we put sent on the left of the equals sign we can also replace an entire slice with new material a consequence of this last change is that the list only has four elements and accessing a later value generates an error note your turn take a few minutes to define a sentence of your own and modify individual words and groups of words slices using the same methods used earlier check your understanding by trying the exercises on lists at the end of this chapter variables from the start of you have had access to texts called text text and so on it saved a lot of typing to be able to refer to a word book with a short name like this in general we can make up names for anything we care to calculate we did this ourselves in the previous sections e g defining a variable sent as follows such lines have the form variable expression python will evaluate the expression and save its result to the variable this process is called assignment it does not generate any output you have to type the variable on a line of its own to inspect its contents the equals sign is slightly misleading since information is moving from the right side to the left it might help to think of it as a left arrow the name of the variable can be anything you like e g my sent sentence xyzzy it must start with a letter and can include numbers and underscores here are some emples of variables and assignments remember that capitalized words appear before lowercase words in sorted lists note notice in the previous emple that we split the definition of my sent over two lines python expressions can be split across multiple lines so long as this happens within any kind of brackets python uses the prompt to indicate that more input is expected it doesn t matter how much indentation is used in these continuation lines but some indentation usually makes them easier to read it is good to choose meaningful variable names to remind you and to help anyone else who reads your python code what your code is meant to do python does not try to make sense of the names it blindly follows your instructions and does not object if you do something confusing such as one two or two the only restriction is that a variable name cannot be any of python reserved words such as def if not and import if you use a reserved word python will produce a syntax error we will often use variables to hold intermediate steps of a computation especially when this makes the code easier to follow thus len set text could also be written caution take care with your choice of names or identifiers for python variables first you should start the name with a letter optionally followed by digits to or letters thus abc is fine but abc will cause a syntax error names are case sensitive which means that myvar and myvar are distinct variables variable names cannot contain whitespace but you can separate words using an underscore e g my var be careful not to insert a hyphen instead of an underscore my var is wrong since python interprets the as a minus sign strings some of the methods we used to access the elements of a list also work with individual words or strings for emple we can assign a string to a variable index a string and slice a string we can also perform multiplication and addition with strings we can join the words of a list to make a single string or split a string into a list as follows we will come back to the topic of strings in for the time being we have two important building blocks lists and strings and are ready to get back to some language analysis computing with language simple statistics let return to our exploration of the ways we can bring our computational resources to bear on large quantities of text we began this discussion in and saw how to search for words in context how to compile the vocabulary of a text how to generate random text in the same style and so on in this section we pick up the question of what makes a text distinct and use automatic methods to find characteristic words and expressions of a text as in you can try new features of the python language by copying them into the interpreter and you will learn about these features systematically in the following section before continuing further you might like to check your understanding of the last section by predicting the output of the following code you can use the interpreter to check whether you got it right if you are not sure how to do this task it would be a good idea to review the previous section before continuing further frequency distributions how can we automatically identify the words of a text that are most informative about the topic and genre of the text imagine how you might go about finding the most frequent words of a book one method would be to keep a tally for each vocabulary item like that shown in the tally would need thousands of rows and it would be an exceedingly laborious process so laborious that we would rather assign the task to a machine figure counting words appearing in a text a frequency distribution the table in is known as a frequency distribution and it tells us the frequency of each vocabulary item in the text in general it could count any kind of observable event it is a distribution because it tells us how the total number of word tokens in the text are distributed across the vocabulary items since we often need frequency distributions in language processing nltk provides built in support for them let s use a freqdist to find the most frequent words of moby dick when we first invoke freqdist we pass the name of the text as an argument we can inspect the total number of words outcomes that have been counted up in the case of moby dick the expression most common gives us a list of the most frequently occurring types in the text note your turn try the preceding frequency distribution emple for yourself for text be careful to use the correct parentheses and uppercase letters if you get an error message nameerror name freqdist is not defined you need to start your work with from nltk book import do any words produced in the last emple help us grasp the topic or genre of this text only one word whale is slightly informative it occurs over times the rest of the words tell us nothing about the text they re just english plumbing what proportion of the text is taken up with such words we can generate a cumulative frequency plot for these words using fdist plot cumulative true to produce the graph in these words account for nearly half the book figure cumulative frequency plot for most frequently words in moby dick these account for nearly half of the tokens if the frequent words do not help us how about the words that occur once only the so called hapaxes view them by typing fdist hapaxes this list contains lexicographer cetological contraband expostulations and about others it seems that there are too many rare words and without seeing the context we probably ca not guess what half of the hapaxes mean in any case since neither frequent nor infrequent words help we need to try something else fine grained selection of words next let s look at the long words of a text perhaps these will be more characteristic and informative for this we adapt some notation from set theory we would like to find the words from the vocabulary of the text that are more than characters long let s call this property p so that p w is true if and only if w is more than characters long now we can express the words of interest using mathematical set notation as shown in a this means the set of all w such that w is an element of v the vocabulary and w has property p the corresponding python expression is given in b note that it produces a list not a set which means that duplicates are possible observe how similar the two notations are let go one more step and write executable python code for each word w in the vocabulary v we check whether len w is greater than all other words will be ignored we will discuss this syntax more carefully later note your turn try out the previous statements in the python interpreter and experiment with changing the text and changing the length condition does it make a difference to your results if you change the variable names e g using word for word in vocab if let return to our task of finding words that characterize a text notice that the long words in text reflect its national focus constitutionally transcontinental whereas those in text reflect its informal content boooooooooooglyyyyyy and yuuuuuuuuuuuummmmmmmmmmmm have we succeeded in automatically extracting words that typify a text well these very long words are often hapaxes i e unique and perhaps it would be better to find frequently occurring long words this seems promising since it eliminates frequent short words e g the and infrequent long words e g antiphilosophists here are all words from the chat corpus that are longer than seven characters that occur more than seven times notice how we have used two conditions len w ensures that the words are longer than seven letters and fdist w ensures that these words occur more than seven times at last we have managed to automatically identify the frequently occurring content bearing words of the text it is a modest but important milestone a tiny piece of code processing tens of thousands of words produces some informative output collocations and bigrams a collocation is a sequence of words that occur together unusually often thus red wine is a collocation whereas the wine is not a characteristic of collocations is that they are resistant to substitution with words that have similar senses for emple maroon wine sounds definitely odd to get a handle on collocations we start off by extracting from a text a list of word pairs also known as bigrams this is easily accomplished with the function bigrams note if you omitted list above and just typed bigrams more you would have seen output of the form generator object bigrams at x fb b a this is python way of saying that it is ready to compute a sequence of items in this case bigrams for now you just need to know to tell python to convert it into a list using list here we see that the pair of words than done is a bigram and we write it in python as than done now collocations are essentially just frequent bigrams except that we want to pay more attention to the cases that involve rare words in particular we want to find bigrams that occur more often than we would expect based on the frequency of the individual words the collocations function does this for us we will see how it works later the collocations that emerge are very specific to the genre of the texts in order to find red wine as a collocation we would need to process a much larger body of text counting other things counting words is useful but we can count other things too for emple we can look at the distribution of word lengths in a text by creating a freqdist out of a long list of numbers where each number is the length of the corresponding word in the text we start by deriving a list of the lengths of words in text and the freqdist then counts the number of times each of these occurs the result is a distribution containing a quarter of a million items each of which is a number corresponding to a word token in the text but there are at most only distinct items being counted the numbers through because there are only different word lengths i e there are words consisting of just one character two characters twenty characters but none with twenty one or more characters one might wonder how frequent the different lengths of word are e g how many words of length four appear in the text are there more words of length five than length four etc we can do this as follows from this we see that the most frequent word length is and that words of length account for roughly or of the words making up the book although we will not pursue it here further analysis of word length might help us understand differences between authors genres or languages summarizes the functions defined in frequency distributions table functions defined for nltk frequency distributions our discussion of frequency distributions has introduced some important python concepts and we will look at them systematically in back to python making decisions and taking control so far our little programs have had some interesting qualities the ability to work with language and the potential to save human effort through automation a key feature of programming is the ability of machines to make decisions on our behalf executing instructions when certain conditions are met or repeatedly looping through text data until some condition is satisfied this feature is known as control and is the focus of this section conditionals python supports a wide range of operators such as and for testing the relationship between values the full set of these relational operators is shown in table numerical comparison operators we can use these to select different words from a sentence of news text here are some emples only the operator is changed from one line to the next they all use sent the first sentence from text wall street journal as before if you get an error saying that sent is undefined you need to first type from nltk book import there is a common pattern to all of these emples w for w in text if condition where condition is a python test that yields either true or false in the cases shown in the previous code emple the condition is always a numerical comparison however we can also test various properties of words using the functions listed in table some word comparison operators here are some emples of these operators being used to select words from our texts words ending with ableness words containing gnt words having an initial capital and words consisting entirely of digits we can also create more complex conditions if c is a condition then not c is also a condition if we have two conditions c and c then we can combine them to form a new condition using conjunction and disjunction c and c c or c note your turn run the following emples and try to explain what is going on in each one next try to make up some conditions of your own operating on every element in we saw some emples of counting items other than words let take a closer look at the notation we used these expressions have the form f w for or w f for where f is a function that operates on a word to compute its length or to convert it to uppercase for now you do not need to understand the difference between the notations f w and w f instead simply learn this python idiom which performs the same operation on every element of a list in the preceding emples it goes through each word in text assigning each one in turn to the variable w and performing the specified operation on the variable note the notation just described is called a list comprehension this is our first emple of a python idiom a fixed notation that we use habitually without bothering to analyze each time mastering such idioms is an important part of becoming a fluent python programmer let return to the question of vocabulary size and apply the same idiom here now that we are not double counting words like this and this which differ only in capitalization we have wiped off the vocabulary count we can go a step further and eliminate numbers and punctuation from the vocabulary count by filtering out any non alphabetic items this emple is slightly complicated it lowercases all the purely alphabetic items perhaps it would have been simpler just to count the lowercase only items but this gives the wrong answer why do not worry if you do not feel confident with list comprehensions yet since you will see many more emples along with explanations in the following chapters nested code blocks most programming languages permit us to execute a block of code when a conditional expression or if statement is satisfied we already saw emples of conditional tests in code like w for w in sent if len w in the following program we have created a variable called word containing the string value cat the if statement checks whether the test len word is true it is so the body of the if statement is invoked and the print statement is executed displaying a message to the user remember to indent the print statement by typing four spaces when we use the python interpreter we have to add an extra blank line in order for it to detect that the nested block is complete note if you are using python or you need to include the following line in order for the above print function to be recognized if we change the conditional test to len word to check that the length of word is greater than or equal to then the test will no longer be true this time the body of the if statement will not be executed and no message is shown to the user an if statement is known as a control structure because it controls whether the code in the indented block will be run another control structure is the for loop try the following and remember to include the colon and the four spaces this is called a loop because python executes the code in circular fashion it starts by performing the assignment word call effectively using the word variable to name the first item of the list then it displays the value of word to the user next it goes back to the for statement and performs the assignment word me before displaying this new value to the user and so on it continues in this fashion until every item of the list has been processed looping with conditions now we can combine the if and for statements we will loop over every item of the list and print the item only if it ends with the letter l we will pick another name for the variable to demonstrate that python does not try to make sense of variable names you will notice that if and for statements have a colon at the end of the line before the indentation begins in fact all python control structures end with a colon the colon indicates that the current statement relates to the indented block that follows we can also specify an action to be taken if the condition of the if statement is not met here we see the elif else if statement and the else statement notice that these also have colons before the indented code as you can see even with this small amount of python knowledge you can start to build multiline python programs it is important to develop such programs in pieces testing that each piece does what you expect before combining them into a program this is why the python interactive interpreter is so invaluable and why you should get comfortable using it finally let combine the idioms we have been exploring first we create a list of cie and cei words then we loop over each item and print it notice the extra information given in the print statement end this tells python to print a space not the default newline after each word automatic natural language understanding we have been exploring language bottom up with the help of texts and the python programming language however we are also interested in exploiting our knowledge of language and computation by building useful language technologies we will take the opportunity now to step back from the nitty gritty of code in order to paint a bigger picture of natural language processing at a purely practical level we all need help to navigate the universe of information locked up in text on the web search engines have been crucial to the growth and popularity of the web but have some shortcomings it takes skill knowledge and some luck to extract answers to such questions as what tourist sites can i visit between philadelphia and pittsburgh on a limited budget what do experts say about digital slr cameras what predictions about the steel market were made by credible commentators in the past week getting a computer to answer them automatically involves a range of language processing tasks including information extraction inference and summarization and would need to be carried out on a scale and with a level of robustness that is still beyond our current capabilities on a more philosophical level a long standing challenge within artificial intelligence has been to build intelligent machines and a major part of intelligent behaviour is understanding language for many years this goal has been seen as too difficult however as nlp technologies become more mature and robust methods for analyzing unrestricted text become more widespread the prospect of natural language understanding has re emerged as a plausible goal in this section we describe some language understanding technologies to give you a sense of the interesting challenges that are waiting for you word sense disambiguation in word sense disambiguation we want to work out which sense of a word was intended in a given context consider the ambiguous words serve and dish in a sentence containing the phrase he served the dish you can detect that both serve and dish are being used with their food meanings it is unlikely that the topic of discussion shifted from sports to crockery in the space of three words this would force you to invent bizarre images like a tennis pro taking out his or her frustrations on a china tea set laid out beside the court in other words we automatically disambiguate words using context exploiting the simple fact that nearby words have closely related meanings as another emple of this contextual effect consider the word by which has several meanings e g the book by chesterton agentive chesterton was the author of the book the cup by the stove locative the stove is where the cup is and submit by friday temporal friday is the time of the submitting observe in c that the meaning of the italicized word helps us interpret the meaning of by pronoun resolution a deeper kind of language understanding is to work out who did what to whom i e to detect the subjects and objects of verbs you learnt to do this in elementary school but it s harder than you might think in the sentence the thieves stole the paintings it is easy to tell who performed the stealing action consider three possible following sentences in c and try to determine what was sold caught and found one case is ambiguous answering this question involves finding the antecedentof the pronoun they either thieves or paintings computational techniques for tackling this problem include anaphora resolution identifying what a pronoun or noun phrase refers to and semantic role labeling identifying how a noun phrase relates to the verb as agent patient instrument and so on generating language output if we can automatically solve such problems of language understanding we will be able to move on to tasks that involve generating language output such as question answering and machine translation in the first case a machine should be able to answer a user questions relating to collection of texts the machine answer demonstrates that it has correctly worked out that they refers to paintings and not to thieves in the second case the machine should be able to translate the text into another language accurately conveying the meaning of the original text in translating the emple text into french we are forced to choose the gender of the pronoun in the second sentence ils masculine if the thieves are found and elles feminine if the paintings are found correct translation actually depends on correct understanding of the pronoun in all of these emples working out the sense of a word the subject of a verb and the antecedent of a pronoun are steps in establishing the meaning of a sentence things we would expect a language understanding system to be able to do machine translation for a long time now machine translation mt has been the holy grail of language understanding ultimately seeking to provide high quality idiomatic translation between any pair of languages its roots go back to the early days of the cold war when the promise of automatic translation led to substantial government sponsorship and with it the genesis of nlp itself today practical translation systems exist for particular pairs of languages and some are integrated into web search engines however these systems have some serious shortcomings which are starkly revealed by translating a sentence back and forth between a pair of languages until equilibrium is reached e g how long before the next flight to alice springs wie lang vor dem folgenden flug zu alice springs how long before the following flight to alice jump wie lang vor dem folgenden flug zu alice springen sie how long before the following flight to alice do you jump wie lang bevor der folgende flug zu alice tun sie springen how long before the following flight to alice does do you jump wie lang bevor der folgende flug zu alice tut tun sie springen how long before the following flight to alice does do you jump wie lang bevor der folgende flug zu alice tut tun sie springen how long before the following flight does to alice do do you jump wie lang bevor der folgende flug zu alice tut sie tun sprung how long before the following flight does leap to alice does you observe that the system correctly translates alice springs from english to german in the line starting but on the way back to english this ends up as alice jump line the preposition before is initially translated into the corresponding german preposition vor but later into the conjunction bevor line after line the sentences become nonsensical but notice the various phrasings indicated by the commas and the change from jump to leap the translation system did not recognize when a word was part of a proper name and it misinterpreted the grammatical structure note your turn try this yourself using http translationparty com machine translation is difficult because a given word could have several possible translations depending on its meaning and because word order must be changed in keeping with the grammatical structure of the target language today these difficulties are being faced by collecting massive quantities of parallel texts from news and government websites that publish documents in two or more languages given a document in german and english and possibly a bilingual dictionary we can automatically pair up the sentences a process called text alignment once we have a million or more sentence pairs we can detect corresponding words and phrases and build a model that can be used for translating new text spoken dialog systems in the history of artificial intelligence the chief measure of intelligence has been a linguistic one namely the turing test can a dialogue system responding to a user text input perform so naturally that we cannot distinguish it from a human generated response in contrast today commercial dialogue systems are very limited but still perform useful functions in narrowly defined domains as we see here s how may i help you u when is saving private ryan playing s for what theater u the paramount theater s saving private ryan is not playing at the paramount theater but it is playing at the madison theater at and you could not ask this system to provide driving instructions or details of nearby restaurants unless the required information had already been stored and suitable question answer pairs had been incorporated into the language processing system observe that this system seems to understand the user goals the user asks when a movie is showing and the system correctly determines from this that the user wants to see the movie this inference seems so obvious that you probably did not notice it was made yet a natural language system needs to be endowed with this capability in order to interact naturally without it when asked do you know when saving private ryan is playing a system might unhelpfully respond with a cold yes however the developers of commercial dialogue systems use contextual assumptions and business logic to ensure that the different ways in which a user might express requests or provide information are handled in a way that makes sense for the particular application so if you type when is or i want to know when or can you tell me when simple rules will always yield screening times this is enough for the system to provide a useful service figure simple pipeline architecture for a spoken dialogue system spoken input top left is analyzed words are recognized sentences are parsed and interpreted in context application specific actions take place top right a response is planned realized as a syntactic structure then to suitably inflected words and finally to spoken output different types of linguistic knowledge inform each stage of the process dialogue systems give us an opportunity to mention the commonly assumed pipeline for nlp shows the architecture of a simple dialogue system along the top of the diagram moving from left to right is a pipeline of some language understanding components these map from speech input via syntactic parsing to some kind of meaning representation along the middle moving from right to left is the reverse pipeline of components for converting concepts to speech these components make up the dynamic aspects of the system at the bottom of the diagram are some representative bodies of static information the repositories of language related data that the processing components draw on to do their work note your turn for an emple of a primitive dialogue system try having a conversation with an nltk chatbot to see the available chatbots run nltk chat chatbots remember to import nltk first textual entailment the challenge of language understanding has been brought into focus in recent years by a public shared task called recognizing textual entailment rte the basic scenario is simple suppose you want to find evidence to support the hypothesis sandra goudie was defeated by max purnell and that you have another short text that seems to be relevant for emple sandra goudie was first elected to parliament in the elections narrowly winning the seat of coromandel by defeating labour candidate max purnell and pushing incumbent green mp jeanette fitzsimons into third place does the text provide enough evidence for you to accept the hypothesis in this particular case the answer will be no you can draw this conclusion easily but it is very hard to come up with automated methods for making the right decision the rte challenges provide data that allow competitors to develop their systems but not enough data for brute force machine learning techniques a topic we will cover in chap data intensive consequently some linguistic analysis is crucial in the previous emple it is important for the system to note that sandra goudie names the person being defeated in the hypothesis not the person doing the defeating in the text as another illustration of the difficulty of the task consider the following text hypothesis pair in order to determine whether the hypothesis is supported by the text the system needs the following background knowledge i if someone is an author of a book then he she has written that book ii if someone is an editor of a book then he she has not written all of that book iii if someone is editor or author of eighteen books then one cannot conclude that he she is author of eighteen books limitations of nlp despite the research led advances in tasks like rte natural language systems that have been deployed for real world applications still cannot perform common sense reasoning or draw on world knowledge in a general and robust manner we can wait for these difficult artificial intelligence problems to be solved but in the meantime it is necessary to live with some severe limitations on the reasoning and knowledge capabilities of natural language systems accordingly right from the beginning an important goal of nlp research has been to make progress on the difficult task of building technologies that understand language using superficial yet powerful techniques instead of unrestricted knowledge and reasoning capabilities indeed this is one of the goals of this book and we hope to equip you with the knowledge and skills to build useful nlp systems and to contribute to the long term aspiration of building intelligent machines summary texts are represented in python using lists monty python we can use indexing slicing and the len function on lists a word token is a particular appearance of a given word in a text a word type is the unique form of the word as a particular sequence of letters we count word tokens using len text and word types using len set text we obtain the vocabulary of a text t using sorted set t we operate on each item of a text using f x for x in text to derive the vocabulary collapsing case distinctions and ignoring punctuation we can write set w lower for w in text if w isalpha we process each word in a text using a for statement such as for w in t or for word in text this must be followed by the colon character and an indented block of code to be executed each time through the loop we test a condition using an if statement if len word this must be followed by the colon character and an indented block of code to be executed only if the condition is true a frequency distribution is a collection of items along with their frequency counts e g the words of a text and their frequency of appearance a function is a block of code that has been assigned a name and can be reused functions are defined using the def keyword as in def mult x y x and y are parameters of the function and act as placeholders for actual data values a function is called by specifying its name followed by zero or more arguments inside parentheses like this texts mult len text further reading this chapter has introduced new concepts in programming natural language processing and linguistics all mixed in together many of them are consolidated in the following chapters however you may also want to consult the online materials provided with this chapter at http nltk org including links to additional background materials and links to online nlp systems you may also like to read up on some linguistics and nlp related concepts in wikipedia e g collocations the turing test the type token distinction you should acquaint yourself with the python documentation available at http docs python org including the many tutorials and comprehensive reference materials linked there a beginner guide to python is available at http wiki python org moin beginnersguide miscellaneous questions about python might be answered in the faq at http python org doc faq general as you delve into nltk you might want to subscribe to the mailing list where new releases of the toolkit are announced there is also an nltk users mailing list where users help each other as they learn how to use python and nltk for language analysis work details of these lists are available at http nltk org for more information on the topics covered in and on nlp more generally you might like to consult one of the following excellent books indurkhya nitin and fred damerau eds handbook of natural language processing second edition chapman hall crc indurkhya damerau dale moisl somers jurafsky daniel and james martin speech and language processing second edition prentice hall jurafsky martin mitkov ruslan ed the oxford handbook of computational linguistics oxford university press second edition expected in mitkov the association for computational linguistics is the international organization that represents the field of nlp the acl website http www aclweb org hosts many useful resources including information about international and regional conferences and workshops the acl wiki with links to hundreds of useful resources and the acl anthology which contains most of the nlp research literature from the past years fully indexed and freely downloadable some excellent introductory linguistics textbooks are finegan o grady et al osu you might like to consult languagelog a popular linguistics blog with occasional posts that use the techniques described in this book exercises try using the python interpreter as a calculator and typing expressions like given an alphabet of letters there are to the power or ten letter strings we can form that works out to how many hundred letter strings are possible the python multiplication operation can be applied to lists what happens when you type monty python or sent review on computing with language how many words are there in text how many distinct words are there compare the lexical diversity scores for humor and romance fiction in which genre is more lexically diverse produce a dispersion plot of the four main protagonists in sense and sensibility elinor marianne edward and willoughby what can you observe about the different roles played by the males and females in this novel can you identify the couples find the collocations in text consider the following python expression len set text state the purpose of this expression describe the two steps involved in performing this computation review on lists and strings define a string and assign it to a variable e g my string my string but put something more interesting in the string print the contents of this variable in two ways first by simply typing the variable name and pressing enter then by using the print statement try adding the string to itself using my string my string or multiplying it by a number e g my string notice that the strings are joined together without any spaces how could you fix this define a variable my sent to be a list of words using the syntax my sent my sent but with your own words or a favorite saying use join my sent to convert this into a string use split to split the string back into the list form you had to start with define several variables containing lists of words e g phrase phrase and so on join them together in various combinations using the plus operator to form whole sentences what is the relationship between len phrase phrase and len phrase len phrase consider the following two expressions which have the same value which one will typically be more relevant in nlp why monty python monty python we have seen how to represent a sentence as a list of words where each word is a sequence of characters what does sent do why experiment with other index values the first sentence of text is provided to you in the variable sent the index of the in sent is because sent gives us the what are the indexes of the two other occurrences of this word in sent review the discussion of conditionals in find all words in the chat corpus text starting with the letter b show them in alphabetical order type the expression list range at the interpreter prompt now try list range list range and list range we will see a variety of uses for this built in function in later chapters use text index to find the index of the word sunset you will need to insert this word as an argument between the parentheses by a process of trial and error find the slice for the complete sentence that contains this word using list addition and the set and sorted operations compute the vocabulary of the sentences sent sent what is the difference between the following two lines which one will give a larger value will this be the case for other texts what is the difference between the following two tests w isupper and not w islower write the slice expression that extracts the last two words of text find all the four letter words in the chat corpus text with the help of a frequency distribution freqdist show these words in decreasing order of frequency review the discussion of looping with conditions in use a combination of for and if statements to loop over the words of the movie script for monty python and the holy grail text and print all the uppercase words one per line write expressions for finding all words in text that meet the conditions listed below the result should be in the form of a list of words word word ending in ise containing the letter z containing the sequence of letters pt having all lowercase letters except for an initial capital i e titlecase define sent to be the list of words she sells sea shells by the sea shore now write code to perform the following tasks print all words beginning with sh print all words longer than four characters what does the following python code do sum len w for w in text can you use it to work out the average word length of a text define a function called vocab size text that has a single parameter for the text and which returns the vocabulary size of the text define a function percent word text that calculates how often a given word occurs in a text and expresses the result as a percentage we have been using sets to store vocabularies try the following python expression set sent set text experiment with this using different arguments to set what does it do can you think of a practical application for this accessing text corpora and lexical resources practical work in natural language processing typically uses large bodies of linguistic data or corpora the goal of this chapter is to answer the following questions what are some useful text corpora and lexical resources and how can we access them with python which python constructs are most helpful for this work how do we avoid repeating ourselves when writing python code this chapter continues to present programming concepts by emple in the context of a linguistic processing task we will wait until later before exploring each python construct systematically do not worry if you see an emple that contains something unfamiliar simply try it out and see what it does and if you are game modify it by substituting some part of the code with a different text or word this way you will associate a task with a programming idiom and learn the hows and whys later accessing text corpora as just mentioned a text corpus is a large body of text many corpora are designed to contain a careful balance of material in one or more genres we emined some small text collections in such as the speeches known as the us presidential inaugural addresses this particular corpus actually contains dozens of individual texts one per address but for convenience we glued them end to end and treated them as a single text also used various pre defined texts that we accessed by typing from nltk book import however since we want to be able to work with other texts this section emines a variety of text corpora we will see how to select individual texts and how to work with them gutenberg corpus nltk includes a small selection of texts from the project gutenberg electronic text archive which contains some free electronic books hosted at http www gutenberg org we begin by getting the python interpreter to load the nltk package then ask to see nltk corpus gutenberg fileids the file identifiers in this corpus let pick out the first of these texts emma by jane austen and give it a short name emma then find out how many words it contains note in we showed how you could carry out concordancing of a text such as text with the command text concordance however this assumes that you are using one of the nine texts obtained as a result of doing from nltk book import now that you have started emining data from nltk corpus as in the previous emple you have to employ the following pair of statements to perform concordancing and other tasks from when we defined emma we invoked the words function of the gutenberg object in nltk corpus package but since it is cumbersome to type such long names all the time python provides another version of the import statement as follows let write a short program to display other information about each text by looping over all the values of fileid corresponding to the gutenberg file identifiers listed earlier and then computing statistics for each text for a compact output display we will round each number to the nearest integer using round this program displays three statistics for each text average word length average sentence length and the number of times each vocabulary item appears in the text on average our lexical diversity score observe that average word length appears to be a general property of english since it has a recurrent value of in fact the average word length is really not since the num chars variable counts space characters by contrast average sentence length and lexical diversity appear to be characteristics of particular authors the previous emple also showed how we can access the raw text of the book not split up into tokens the raw function gives us the contents of the file without any linguistic processing so for emple len gutenberg raw blake poems txt tells us how many letters occur in the text including the spaces between words the sents function divides the text up into its sentences where each sentence is a list of words note most nltk corpus readers include a variety of access methods apart from words raw and sents richer linguistic content is available from some corpora such as part of speech tags dialogue tags syntactic trees and so forth we will see these in later chapters web and chat text although project gutenberg contains thousands of books it represents established literature it is important to consider less formal language as well nltk small collection of web text includes content from a firefox discussion forum conversations overheard in new york the movie script of pirates of the carribean personal advertisements and wine reviews there is also a corpus of instant messaging chat sessions originally collected by the naval postgraduate school for research on automatic detection of internet predators the corpus contains over posts anonymized by replacing usernames with generic names of the form usernnn and manually edited to remove any other identifying information the corpus is organized into files where each file contains several hundred posts collected on a given date for an age specific chatroom teens s s s plus a generic adults chatroom the filename contains the date chatroom and number of posts e g s posts xml contains posts gathered from the s chat room on brown corpus the brown corpus was the first million word electronic corpus of english created in at brown university this corpus contains text from sources and the sources have been categorized by genre such as news editorial and so on gives an emple of each genre for a complete list see http icame uib no brown bcm los html table emple document for each section of the brown corpus we can access the corpus as a list of words or a list of sentences where each sentence is itself just a list of words we can optionally specify particular categories or files to read the brown corpus is a convenient resource for studying systematic differences between genres a kind of linguistic inquiry known as stylistics let compare genres in their usage of modal verbs the first step is to produce the counts for a particular genre remember to import nltk before doing the following note we need to include end in order for the print function to put its output on a single line note your turn choose a different section of the brown corpus and adapt the previous emple to count a selection of wh words such as what when where who and why next we need to obtain counts for each genre of interest we will use nltk support for conditional frequency distributions these are presented systematically in where we also unpick the following code line by line for the moment you can ignore the details and just concentrate on the output observe that the most frequent modal in the news genre is will while the most frequent modal in the romance genre is could would you have predicted this the idea that word counts might distinguish genres will be taken up again in chap data intensive reuters corpus the reuters corpus contains news documents totaling million words the documents have been classified into topics and grouped into two sets called training and test thus the text with fileid test is a document drawn from the test set this split is for training and testing algorithms that automatically detect the topic of a document as we will see in chap data intensive unlike the brown corpus categories in the reuters corpus overlap with each other simply because a news story often covers multiple topics we can ask for the topics covered by one or more documents or for the documents included in one or more categories for convenience the corpus methods accept a single fileid or a list of fileids similarly we can specify the words or sentences we want in terms of files or categories the first handful of words in each of these texts are the titles which by convention are stored as upper case inaugural address corpus in we looked at the inaugural address corpus but treated it as a single text the graph in fig inaugural used word offset as one of the axes this is the numerical index of the word in the corpus counting from the first word of the first address however the corpus is actually a collection of texts one for each presidential address an interesting property of this collection is its time dimension notice that the year of each text appears in its filename to get the year out of the filename we extracted the first four characters using fileid let s look at how the words america and citizen are used over time the following code converts the words in the inaugural corpus to lowercase using w lower then checks if they start with either of the targets america or citizen using startswith thus it will count words like american s and citizens we ll learn about conditional frequency distributions in for now just consider the output shown in figure plot of a conditional frequency distribution all words in the inaugural address corpus that begin with america or citizen are counted separate counts are kept for each address these are plotted so that trends in usage over time can be observed counts are not normalized for document length annotated text corpora many text corpora contain linguistic annotations representing pos tags named entities syntactic structures semantic roles and so forth nltk provides convenient ways to access several of these corpora and has data packages containing corpora and corpus samples freely downloadable for use in teaching and research lists some of the corpora for information about downloading them see http nltk org data for more emples of how to access nltk corpora please consult the corpus howto at http nltk org howto table some of the corpora and corpus samples distributed with nltk for information about downloading and using them please consult the nltk website corpora in other languages nltk comes with corpora for many languages though in some cases you will need to learn how to manipulate character encodings in python before using these corpora see the last of these corpora udhr contains the universal declaration of human rights in over languages the fileids for this corpus include information about the character encoding used in the file such as utf or latin let use a conditional frequency distribution to emine the differences in word lengths for a selection of languages included in the udhr corpus the output is shown in run the program yourself to see a color plot note that true and false are python built in boolean values figure cumulative word length distributions six translations of the universal declaration of human rights are processed this graph shows that words having or fewer letters account for about of ibibio text of german text and of inuktitut text note your turn pick a language of interest in udhr fileids and define a variable raw text udhr raw language latin now plot a frequency distribution of the letters of the text using nltk freqdist raw text plot unfortunately for many languages substantial corpora are not yet available often there is insufficient government or industrial support for developing language resources and individual efforts are piecemeal and hard to discover or re use some languages have no established writing system or are endangered see for suggestions on how to locate language resources text corpus structure we have seen a variety of corpus structures so far these are summarized in the simplest kind lacks any structure it is just a collection of texts often texts are grouped into categories that might correspond to genre source author language etc sometimes these categories overlap notably in the case of topical categories as a text can be relevant to more than one topic occasionally text collections have temporal structure news collections being the most common emple figure common structures for text corpora the simplest kind of corpus is a collection of isolated texts with no particular organization some corpora are structured into categories like genre brown corpus some categorizations overlap such as topic categories reuters corpus other corpora represent language use over time inaugural address corpus table basic corpus functionality defined in nltk more documentation can be found using help nltk corpus reader and by reading the online corpus howto at http nltk org howto nltk corpus readers support efficient access to a variety of corpora and can be used to work with new corpora lists functionality provided by the corpus readers we illustrate the difference between some of the corpus access methods below loading your own corpus if you have your own collection of text files that you would like to access using the above methods you can easily load them with the help of nltk plaintextcorpusreader check the location of your files on your file system in the following emple we have taken this to be the directory usr share dict whatever the location set this to be the value of corpus root the second parameter of the plaintextcorpusreader initializer can be a list of fileids like a txt test b txt or a pattern that matches all fileids like abc txt see for information about regular expressions as another emple suppose you have your own local copy of penn treebank release in c corpora we can use the bracketparsecorpusreader to access this corpus we specify the corpus root to be the location of the parsed wall street journal component of the corpus and give a file pattern that matches the files contained within its subfolders using forward slashes conditional frequency distributions we introduced frequency distributions in we saw that given some list mylist of words or other items freqdist mylist would compute the number of occurrences of each item in the list here we will generalize this idea when the texts of a corpus are divided into several categories by genre topic author etc we can maintain separate frequency distributions for each category this will allow us to study systematic differences between the categories in the previous section we achieved this using nltk s conditionalfreqdist data type a conditional frequency distribution is a collection of frequency distributions each one for a different condition the condition will often be the category of the text depicts a fragment of a conditional frequency distribution having just two conditions one for news text and one for romance text figure counting words appearing in a text collection a conditional frequency distribution conditions and events a frequency distribution counts observable events such as the appearance of words in a text a conditional frequency distribution needs to pair each event with a condition so instead of processing a sequence of words we have to process a sequence of pairs each pair has the form condition event if we were processing the entire brown corpus by genre there would be conditions one per genre and events one per word counting words by genre in we saw a conditional frequency distribution where the condition was the section of the brown corpus and for each condition we counted words whereas freqdist takes a simple list as input conditionalfreqdist takes a list of pairs let break this down and look at just two genres news and romance for each genre we loop over every word in the genre producing pairs consisting of the genre and the word so as we can see below pairs at the beginning of the list genre word will be of the form news word while those at the end will be of the form romance word we can now use this list of pairs to create a conditionalfreqdist and save it in a variable cfd as usual we can type the name of the variable to inspect it and verify it has two conditions let access the two conditions and satisfy ourselves that each is just a frequency distribution plotting and tabulating distributions apart from combining two or more frequency distributions and being easy to initialize a conditionalfreqdist provides some useful methods for tabulation and plotting the plot in was based on a conditional frequency distribution reproduced in the code below the condition is either of the words america or citizen and the counts being plotted are the number of times the word occured in a particular speech it exploits the fact that the filename for each speech e g lincoln txt contains the year as the first four characters this code generates the pair america for every instance of a word whose lowercased form starts with america such as americans in the file lincoln txt the plot in was also based on a conditional frequency distribution reproduced below this time the condition is the name of the language and the counts being plotted are derived from word lengths it exploits the fact that the filename for each language is the language name followed by latin the character encoding in the plot and tabulate methods we can optionally specify which conditions to display with a conditions parameter when we omit it we get all the conditions similarly we can limit the samples to display with a samples parameter this makes it possible to load a large quantity of data into a conditional frequency distribution and then to explore it by plotting or tabulating selected conditions and samples it also gives us full control over the order of conditions and samples in any displays for emple we can tabulate the cumulative frequency data just for two languages and for words less than characters long as shown below we interpret the last cell on the top row to mean that words of the english text have or fewer letters note your turn working with the news and romance genres from the brown corpus find out which days of the week are most newsworthy and which are most romantic define a variable called days containing a list of days of the week i e monday now tabulate the counts for these words using cfd tabulate samples days now try the same thing using plot in place of tabulate you may control the output order of days with the help of an extra parameter samples monday you may have noticed that the multi line expressions we have been using with conditional frequency distributions look like list comprehensions but without the brackets in general when we use a list comprehension as a parameter to a function like set w lower for w in t we are permitted to omit the square brackets and just write set w lower for w in t see the discussion of generator expressions in for more about this generating random text with bigrams we can use a conditional frequency distribution to create a table of bigrams word pairs we introducted bigrams in the bigrams function takes a list of words and builds a list of consecutive word pairs remember that in order to see the result and not a cryptic generator object we need to use the list function in we treat each word as a condition and for each one we effectively create a frequency distribution over the following words the function generate model contains a simple loop to generate text when we call the function we choose a word such as living as our initial context then once inside the loop we print the current value of the variable word and reset word to be the most likely token in that context using max next time through the loop we use that word as our new context as you can see by inspecting the output this simple approach to text generation tends to get stuck in loops another method would be to randomly choose the next word from among the available words conditional frequency distributions are a useful data structure for many nlp tasks their commonly used methods are summarized in table nltk conditional frequency distributions commonly used methods and idioms for defining accessing and visualizing a conditional frequency distribution of counters more python reusing code by this time you have probably typed and retyped a lot of code in the python interactive interpreter if you mess up when retyping a complex emple you have to enter it again using the arrow keys to access and modify previous commands is helpful but only goes so far in this section we see two important ways to reuse code text editors and python functions creating programs with a text editor the python interactive interpreter performs your instructions as soon as you type them often it is better to compose a multi line program using a text editor then ask python to run the whole program at once using idle you can do this by going to the file menu and opening a new window try this now and enter the following one line program print monty python save this program in a file called monty py then go to the run menu and select the command run module we will learn what modules are shortly the result in the main idle window should look like this you can also type from monty import and it will do the same thing from now on you have a choice of using the interactive interpreter or a text editor to create your programs it is often convenient to test your ideas using the interpreter revising a line of code until it does what you expect once you are ready you can paste the code minus any or prompts into the text editor continue to expand it and finally save the program in a file so that you do not have to type it in again later give the file a short but descriptive name using all lowercase letters and separating words with underscore and using the py filename extension e g monty python py note important our inline code emples include the and prompts as if we are interacting directly with the interpreter as they get more complicated you should instead type them into the editor without the prompts and run them from the editor as shown above when we provide longer programs in this book we will leave out the prompts to remind you to type them into a file rather than using the interpreter you can see this already in above note that it still includes a couple of lines with the python prompt this is the interactive part of the task where you inspect some data and invoke a function remember that all code samples like are downloadable from http nltk org functions suppose that you work on analyzing text that involves different forms of the same word and that part of your program needs to work out the plural form of a given singular noun suppose it needs to do this work in two places once when it is processing some texts and again when it is processing user input rather than repeating the same code several times over it is more efficient and reliable to localize this work inside a function a function is just a named block of code that performs some well defined task as we saw in a function is usually defined to take some inputs using special variables known as parameters and it may produce a result also known as a return value we define a function using the keyword def followed by the function name and any input parameters followed by the body of the function here is the function we saw in including the import statement that is needed for python in order to make division behave as expected we use the keyword return to indicate the value that is produced as output by the function in the above emple all the work of the function is done in the return statement here is an equivalent definition which does the same work using multiple lines of code we will change the parameter name from text to my text data to remind you that this is an arbitrary choice notice that we ve created some new variables inside the body of the function these are local variables and are not accessible outside the function so now we have defined a function with the name lexical diversity but just defining it won t produce any output functions do nothing until they are called or invoked let return to our earlier scenario and actually define a simple function to work out english plurals the function plural in takes a singular noun and generates a plural form though it is not always correct we will discuss functions at greater length in the endswith function is always associated with a string object e g word in to call such functions we give the name of the object a period and then the name of the function these functions are usually known as methods modules over time you will find that you create a variety of useful little text processing functions and you end up copying them from old programs to new ones which file contains the latest version of the function you want to use it makes life a lot easier if you can collect your work into a single place and access previously defined functions without making copies to do this save your function s in a file called say text proc py now you can access your work simply by importing it from the file our plural function obviously has an error since the plural of fan is fans instead of typing in a new version of the function we can simply edit the existing one thus at every stage there is only one version of our plural function and no confusion about which one is being used a collection of variable and function definitions in a file is called a python module a collection of related modules is called a package nltk code for processing the brown corpus is an emple of a module and its collection of code for processing all the different corpora is an emple of a package nltk itself is a set of packages sometimes called a library caution if you are creating a file to contain some of your python code do not name your file nltk py it may get imported in place of the real nltk package when it imports modules python first looks in the current directory folder lexical resources a lexicon or lexical resource is a collection of words and or phrases along with associated information such as part of speech and sense definitions lexical resources are secondary to texts and are usually created and enriched with the help of texts for emple if we have defined a text my text then vocab sorted set my text builds the vocabulary of my text while word freq freqdist my text counts the frequency of each word in the text both of vocab and word freq are simple lexical resources similarly a concordance like the one we saw in gives us information about word usage that might help in the preparation of a dictionary standard terminology for lexicons is illustrated in a lexical entry consists of a headword also known as a lemma along with additional information such as the part of speech and the sense definition two distinct words having the same spelling are called homonyms figure lexicon terminology lexical entries for two lemmas having the same spelling homonyms providing part of speech and gloss information the simplest kind of lexicon is nothing more than a sorted list of words sophisticated lexicons include complex structure within and across the individual entries in this section we will look at some lexical resources included with nltk wordlist corpora nltk includes some corpora that are nothing more than wordlists the words corpus is the usr share dict words file from unix used by some spell checkers we can use it to find unusual or mis spelt words in a text corpus as shown in there is also a corpus of stopwords that is high frequency words like the to and also that we sometimes want to filter out of a document before further processing stopwords usually have little lexical content and their presence in a text fails to distinguish it from other texts let define a function to compute what fraction of words in a text are not in the stopwords list thus with the help of stopwords we filter out over a quarter of the words of the text notice that we have combined two different kinds of corpus here using a lexical resource to filter the content of a text corpus figure a word puzzle a grid of randomly chosen letters with rules for creating words out of the letters this puzzle is known as target a wordlist is useful for solving word puzzles such as the one in our program iterates through every word and for each one checks whether it meets the conditions it is easy to check obligatory letter and length constraints and we will only look for words with six or more letters here it is trickier to check that candidate solutions only use combinations of the supplied letters especially since some of the supplied letters appear twice here the letter v the freqdist comparison method permits us to check that the frequency of each letter in the candidate word is less than or equal to the frequency of the corresponding letter in the puzzle one more wordlist corpus is the names corpus containing first names categorized by gender the male and female names are stored in separate files let find names which appear in both files i e names that are ambiguous for gender it is well known that names ending in the letter a are almost always female we can see this and some other patterns in the graph in produced by the following code remember that name is the last letter of name figure conditional frequency distribution this plot shows the number of female and male names ending with each letter of the alphabet most names ending with a e or i are female names ending in h and l are equally likely to be male or female names ending in k o r s and t are likely to be male a pronouncing dictionary a slightly richer kind of lexical resource is a table or spreadsheet containing a word plus some properties in each row nltk includes the cmu pronouncing dictionary for us english which was designed for use by speech synthesizers for each word this lexicon provides a list of phonetic codes distinct labels for each contrastive sound known as phones observe that fire has two pronunciations in us english the one syllable f ay r and the two syllable f ay er the symbols in the cmu pronouncing dictionary are from the arpabet described in more detail at http en wikipedia org wiki arpabet each entry consists of two parts and we can process these individually using a more complex version of the for statement instead of writing for entry in entries we replace entry with two variable names word pron now each time through the loop word is assigned the first part of the entry and pron is assigned the second part of the entry the above program scans the lexicon looking for entries whose pronunciation consists of three phones if the condition is true it assigns the contents of pron to three new variables ph ph and ph notice the unusual form of the statement which does that work here is another emple of the same for statement this time used inside a list comprehension this program finds all words whose pronunciation ends with a syllable sounding like nicks you could use this method to find rhyming words notice that the one pronunciation is spelt in several ways nics niks nix even ntic with a silent t for the word atlantic let look for some other mismatches between pronunciation and writing can you summarize the purpose of the following emples and explain how they work the phones contain digits to represent primary stress secondary stress and no stress as our final emple we define a function to extract the stress digits and then scan our lexicon to find words having a particular stress pattern note a subtlety of the above program is that our user defined function stress is invoked inside the condition of a list comprehension there is also a doubly nested for loop there is a lot going on here and you might want to return to this once you have had more experience using list comprehensions we can use a conditional frequency distribution to help us find minimally contrasting sets of words here we find all the p words consisting of three sounds and group them according to their first and last sounds rather than iterating over the whole dictionary we can also access it by looking up particular words we will use python dictionary data structure which we will study systematically in we look up a dictionary by giving its name followed by a key such as the word fire inside square brackets if we try to look up a non existent key we get a keyerror this is similar to what happens when we index a list with an integer that is too large producing an indexerror the word blog is missing from the pronouncing dictionary so we tweak our version by assigning a value for this key this has no effect on the nltk corpus next time we access it blog will still be absent we can use any lexical resource to process a text e g to filter out words having some lexical property like nouns or mapping every word of the text for emple the following text to speech function looks up each word of the text in the pronunciation dictionary comparative wordlists another emple of a tabular lexicon is the comparative wordlist nltk includes so called swadesh wordlists lists of about common words in several languages the languages are identified using an iso two letter code we can access cognate words from multiple languages using the entries method specifying a list of languages with one further step we can convert this into a simple dictionary we will learn about dict in we can make our simple translator more useful by adding other source languages let get the german english and spanish english pairs convert each to a dictionary using dict then update our original translate dictionary with these additional mappings we can compare words in various germanic and romance languages shoebox and toolbox lexicons perhaps the single most popular tool used by linguists for managing data is toolbox previously known as shoebox since it replaces the field linguist traditional shoebox full of file cards toolbox is freely downloadable from http www sil org computing toolbox a toolbox file consists of a collection of entries where each entry is made up of one or more fields most fields are optional or repeatable which means that this kind of lexical resource cannot be treated as a table or spreadsheet here is a dictionary for the rotokas language we see just the first entry for the word kaa meaning to gag entries consist of a series of attribute value pairs like ps v to indicate that the part of speech is v verb and ge gag to indicate that the gloss into english is gag the last three pairs contain an emple sentence in rotokas and its translations into tok pisin and english the loose structure of toolbox files makes it hard for us to do much more with them at this stage xml provides a powerful way to process this kind of corpus and we will return to this topic in note the rotokas language is spoken on the island of bougainville papua new guinea this lexicon was contributed to nltk by stuart robinson rotokas is notable for having an inventory of just phonemes contrastive sounds http en wikipedia org wiki rotokas language wordnet wordnet is a semantically oriented dictionary of english similar to a traditional thesaurus but with a richer structure nltk includes the english wordnet with words and synonym sets we will begin by looking at synonyms and how they are accessed in wordnet senses and synonyms consider the sentence in a if we replace the word motorcar in a by automobile to get b the meaning of the sentence stays pretty much the same since everything else in the sentence has remained unchanged we can conclude that the words motorcar and automobile have the same meaning i e they are synonyms we can explore these words with the help of wordnet thus motorcar has just one possible meaning and it is identified as car n the first noun sense of car the entity car n is called a synset or synonym set a collection of synonymous words or lemmas each word of a synset can have several meanings e g car can also signify a train carriage a gondola or an elevator car however we are only interested in the single meaning that is common to all words of the above synset synsets also come with a prose definition and some emple sentences although definitions help humans to understand the intended meaning of a synset the words of the synset are often more useful for our programs to eliminate ambiguity we will identify these words as car n automobile car n motorcar and so on this pairing of a synset with a word is called a lemma we can get all the lemmas for a given synset look up a particular lemma get the synset corresponding to a lemma and get the name of a lemma unlike the word motorcar which is unambiguous and has one synset the word car is ambiguous having five synsets for convenience we can access all the lemmas involving the word car as follows note your turn write down all the senses of the word dish that you can think of now explore this word with the help of wordnet using the same operations we used above the wordnet hierarchy wordnet synsets correspond to abstract concepts and they do not always have corresponding words in english these concepts are linked together in a hierarchy some concepts are very general such as entity state event these are called unique beginners or root synsets others such as gas guzzler and hatchback are much more specific a small portion of a concept hierarchy is illustrated in figure fragment of wordnet concept hierarchy nodes correspond to synsets edges indicate the hypernym hyponym relation i e the relation between superordinate and subordinate concepts wordnet makes it easy to navigate between concepts for emple given a concept like motorcar we can look at the concepts that are more specific the immediate hyponyms we can also navigate up the hierarchy by visiting hypernyms some words have multiple paths because they can be classified in more than one way there are two paths between car n and entity n because wheeled vehicle n can be classified as both a vehicle and a container we can get the most general hypernyms or root hypernyms of a synset as follows note your turn try out nltk convenient graphical wordnet browser nltk app wordnet explore the wordnet hierarchy by following the hypernym and hyponym links more lexical relations hypernyms and hyponyms are called lexical relations because they relate one synset to another these two relations navigate up and down the is a hierarchy another important way to navigate the wordnet network is from items to their components meronyms or to the things they are contained in holonyms for emple the parts of a tree are its trunk crown and so on the part meronyms the substance a tree is made of includes heartwood and sapwood the substance meronyms a collection of trees forms a forest the member holonyms to see just how intricate things can get consider the word mint which has several closely related senses we can see that mint n is part of mint n and the substance from which mint n is made there are also relationships between verbs for emple the act of walking involves the act of stepping so walking entails stepping some verbs have multiple entailments some lexical relationships hold between lemmas e g antonymy you can see the lexical relations and the other methods defined on a synset using dir for emple dir wn synset harmony n semantic similarity we have seen that synsets are linked by a complex network of lexical relations given a particular synset we can traverse the wordnet network to find synsets with related meanings knowing which words are semantically related is useful for indexing a collection of texts so that a search for a general term like vehicle will match documents containing specific terms like limousine recall that each synset has one or more hypernym paths that link it to a root hypernym such as entity n two synsets linked to the same root may have several hypernyms in common cf if two synsets share a very specific hypernym one that is low down in the hypernym hierarchy they must be closely related of course we know that whale is very specific and baleen whale even more so while vertebrate is more general and entity is completely general we can quantify this concept of generality by looking up the depth of each synset similarity measures have been defined over the collection of wordnet synsets which incorporate the above insight for emple path similarity assigns a score in the range based on the shortest path that connects the concepts in the hypernym hierarchy is returned in those cases where a path cannot be found comparing a synset with itself will return consider the following similarity scores relating right whale to minke whale orca tortoise and novel although the numbers wo not mean much they decrease as we move away from the semantic space of sea creatures to inanimate objects note several other similarity measures are available you can type help wn for more information nltk also includes verbnet a hierarhical verb lexicon linked to wordnet it can be accessed with nltk corpus verbnet summary a text corpus is a large structured collection of texts nltk comes with many corpora e g the brown corpus nltk corpus brown some text corpora are categorized e g by genre or topic sometimes the categories of a corpus overlap each other a conditional frequency distribution is a collection of frequency distributions each one for a different condition they can be used for counting word frequencies given a context or a genre python programs more than a few lines long should be entered using a text editor saved to a file with a py extension and accessed using an import statement python functions permit you to associate a name with a particular block of code and re use that code as often as necessary some functions known as methods are associated with an object and we give the object name followed by a period followed by the function like this x funct y e g word isalpha to find out about some variable v type help v in the python interactive interpreter to read the help entry for this kind of object wordnet is a semantically oriented dictionary of english consisting of synonym sets or synsets and organized into a network some functions are not available by default but must be accessed using python import statement further reading extra materials for this chapter are posted at http nltk org including links to freely available resources on the web the corpus methods are summarized in the corpus howto at http nltk org howto and documented extensively in the online api documentation significant sources of published corpora are the linguistic data consortium ldc and the european language resources agency elra hundreds of annotated text and speech corpora are available in dozens of languages non commercial licences permit the data to be used in teaching and research for some corpora commercial licenses are also available but for a higher fee a good tool for creating annotated text corpora is called brat and available from http brat nlplab org these and many other language resources have been documented using olac metadata and can be searched via the olac homepage at http www language archives org corpora list is a mailing list for discussions about corpora and you can find resources by searching the list archives or posting to the list the most complete inventory of the world languages is ethnologue http www ethnologue com of languages only a few dozen have substantial digital resources suitable for use in nlp this chapter has touched on the field of corpus linguistics other useful books in this area include biber conrad reppen mcenery meyer sampson mccarthy scott tribble further readings in quantitative data analysis in linguistics are baayen gries woods fletcher hughes the original description of wordnet is fellbaum although wordnet was originally developed for research in psycholinguistics it is now widely used in nlp and information retrieval wordnets are being developed for many other languages as documented at http www globalwordnet org for a study of wordnet similarity measures see budanitsky hirst other topics touched on in this chapter were phonetics and lexical semantics and we refer readers to chapters and of jurafsky martin exercises create a variable phrase containing a list of words review the operations described in the previous chapter including addition multiplication indexing slicing and sorting use the corpus module to explore austen persuasion txt how many word tokens does this book have how many word types use the brown corpus reader nltk corpus brown words or the web text corpus reader nltk corpus webtext words to access some sample text in two different genres read in the texts of the state of the union addresses using the state union corpus reader count occurrences of men women and people in each document what has happened to the usage of these words over time investigate the holonym meronym relations for some nouns remember that there are three kinds of holonym meronym relation so you need to use member meronyms part meronyms substance meronyms member holonyms part holonyms and substance holonyms in the discussion of comparative wordlists we created an object called translate which you could look up using words in both german and spanish in order to get corresponding words in english what problem might arise with this approach can you suggest a way to avoid this problem according to strunk and white s elements of style the word however used at the start of a sentence means in whatever way or to whatever extent and not nevertheless they give this emple of correct usage however you advise him he will probably do as he thinks best http www bartleby com strunk html use the concordance tool to study actual usage of this word in the various texts we have been considering see also the languagelog posting fossilized prejudices about however at http itre cis upenn edu myl languagelog archives html define a conditional frequency distribution over the names corpus that allows you to see which initial letters are more frequent for males vs females cf pick a pair of texts and study the differences between them in terms of vocabulary vocabulary richness genre etc can you find pairs of words which have quite different meanings across the two texts such as monstrous in moby dick and in sense and sensibility read the bbc news article uk s vicky pollards left behind http news bbc co uk hi education stm the article gives the following statistic about teen language the top words used including yeah no but and like account for around a third of all words how many word types account for a third of all word tokens for a variety of text sources what do you conclude about this statistic read more about this on languagelog at http itre cis upenn edu myl languagelog archives html investigate the table of modal distributions and look for other patterns try to explain them in terms of your own impressionistic understanding of the different genres can you find other closed classes of words that exhibit significant differences across different genres the cmu pronouncing dictionary contains multiple pronunciations for certain words how many distinct words does it contain what fraction of words in this dictionary have more than one possible pronunciation what percentage of noun synsets have no hyponyms you can get all noun synsets using wn all synsets n define a function supergloss s that takes a synset s as its argument and returns a string consisting of the concatenation of the definition of s and the definitions of all the hypernyms and hyponyms of s write a program to find all words that occur at least three times in the brown corpus write a program to generate a table of lexical diversity scores i e token type ratios as we saw in include the full set of brown corpus genres nltk corpus brown categories which genre has the lowest diversity greatest number of tokens per type is this what you would have expected write a function that finds the most frequently occurring words of a text that are not stopwords write a program to print the most frequent bigrams pairs of adjacent words of a text omitting bigrams that contain stopwords write a program to create a table of word frequencies by genre like the one given in for modals choose your own words and try to find words whose presence or absence is typical of a genre discuss your findings write a function word freq that takes a word and the name of a section of the brown corpus as arguments and computes the frequency of the word in that section of the corpus write a program to guess the number of syllables contained in a text making use of the cmu pronouncing dictionary define a function hedge text which processes a text and produces a new version with the word like between every third word zipf law let f w be the frequency of a word w in free text suppose that all the words of a text are ranked according to their frequency with the most frequent word first zipf law states that the frequency of a word type is inversely proportional to its rank i e f r k for some constant k for emple the th most common word type should occur three times as frequently as the th most common word type write a function to process a large text and plot word frequency against word rank using pylab plot do you confirm zipf law hint it helps to use a logarithmic scale what is going on at the extreme ends of the plotted line generate random text e g using random choice abcdefg taking care to include the space character you will need to import random first use the string concatenation operator to accumulate characters into a very long string then tokenize this string and generate the zipf plot as before and compare the two plots what do you make of zipf s law in the light of this modify the text generation program in further to do the following tasks store the n most likely words in a list words then randomly choose a word from the list using random choice you will need to import random first select a particular genre such as a section of the brown corpus or a genesis translation one of the gutenberg texts or one of the web texts train the model on this corpus and get it to generate random text you may have to experiment with different start words how intelligible is the text discuss the strengths and weaknesses of this method of generating random text now train your system using two distinct genres and experiment with generating text in the hybrid genre discuss your observations define a function find language that takes a string as its argument and returns a list of languages that have that string as a word use the udhr corpus and limit your searches to files in the latin encoding what is the branching factor of the noun hypernym hierarchy i e for every noun synset that has hyponyms or children in the hypernym hierarchy how many do they have on average you can get all noun synsets using wn all synsets n the polysemy of a word is the number of senses it has using wordnet we can determine that the noun dog has senses with len wn synsets dog n compute the average polysemy of nouns verbs adjectives and adverbs according to wordnet use one of the predefined similarity measures to score the similarity of each of the following pairs of words rank the pairs in order of decreasing similarity how close is your ranking to the order given here an order that was established experimentally by miller charles car automobile gem jewel journey voyage boy lad coast shore asylum madhouse magician wizard midday noon furnace stove food fruit bird cock bird crane tool implement brother monk lad brother crane implement journey car monk oracle cemetery woodland food rooster coast hill forest graveyard shore woodland monk slave coast forest lad wizard chord smile glass magician rooster voyage noon string processing raw text the most important source of texts is undoubtedly the web it is convenient to have existing text collections to explore such as the corpora we saw in the previous chapters however you probably have your own text sources in mind and need to learn how to access them the goal of this chapter is to answer the following questions how can we write programs to access text from local files and from the web in order to get hold of an unlimited range of language material how can we split documents up into individual words and punctuation symbols so we can carry out the same kinds of analysis we did with text corpora in earlier chapters how can we write programs to produce formatted output and save it in a file in order to address these questions we will be covering key concepts in nlp including tokenization and stemming along the way you will consolidate your python knowledge and learn about strings files and regular expressions since so much text on the web is in html format we will also see how to dispense with markup note important from this chapter onwards our program samples will assume you begin your interactive session or your program with the following import statements accessing text from the web and from disk electronic books a small sample of texts from project gutenberg appears in the nltk corpus collection however you may be interested in analyzing other texts from project gutenberg you can browse the catalog of free online books at http www gutenberg org catalog and obtain a url to an ascii text file although of the texts in project gutenberg are in english it includes material in over other languages including catalan chinese dutch finnish french german italian portuguese and spanish with more than texts each text number is an english translation of crime and punishment and we can access it as follows note the read process will take a few seconds as it downloads this large book if you are using an internet proxy which is not correctly detected by python you may need to specify the proxy manually before using urlopen as follows the variable raw contains a string with characters we can see that it is a string using type raw this is the raw content of the book including many details we are not interested in such as whitespace line breaks and blank lines notice the r and n in the opening line of the file which is how python displays the special carriage return and line feed characters the file must have been created on a windows machine for our language processing we want to break up the string into words and punctuation as we saw in this step is called tokenization and it produces our familiar structure a list of words and punctuation notice that nltk was needed for tokenization but not for any of the earlier tasks of opening a url and reading it into a string if we now take the further step of creating an nltk text from this list we can carry out all of the other linguistic processing we saw in along with the regular list operations like slicing notice that project gutenberg appears as a collocation this is because each text downloaded from project gutenberg contains a header with the name of the text the author the names of people who scanned and corrected the text a license and so on sometimes this information appears in a footer at the end of the file we cannot reliably detect where the content begins and ends and so have to resort to manual inspection of the file to discover unique strings that mark the beginning and the end before trimming raw to be just the content and nothing else the find and rfind reverse find methods help us get the right index values to use for slicing the string we overwrite raw with this slice so now it begins with part i and goes up to but not including the phrase that marks the end of the content this was our first brush with the reality of the web texts found on the web may contain unwanted material and there may not be an automatic way to remove it but with a small amount of extra work we can extract the material we need dealing with html much of the text on the web is in the form of html documents you can use a web browser to save a page as text to a local file then access this as described in the section on files below however if you are going to do this often it is easiest to get python to do the work directly the first step is the same as before using urlopen for fun we will pick a bbc news story called blondes to die out in years an urban legend passed along by the bbc as established scientific fact you can type print html to see the html content in all its glory including meta tags an image map javascript forms and tables to get text out of html we will use a python library called beautifulsoup available from http www crummy com software beautifulsoup this still contains unwanted material concerning site navigation and related stories with some trial and error you can find the start and end indexes of the content and select the tokens of interest and initialize a text as before processing search engine results the web can be thought of as a huge corpus of unannotated text web search engines provide an efficient means of searching this large quantity of text for relevant linguistic emples the main advantage of search engines is size since you are searching such a large set of documents you are more likely to find any linguistic pattern you are interested in furthermore you can make use of very specific patterns which would only match one or two emples on a smaller emple but which might match tens of thousands of emples when run on the web a second advantage of web search engines is that they are very easy to use thus they provide a very convenient tool for quickly checking a theory to see if it is reasonable table google hits for collocations the number of hits for collocations involving the words absolutely or definitely followed by one of adore love like or prefer liberman in languagelog unfortunately search engines have some significant shortcomings first the allowable range of search patterns is severely restricted unlike local corpora where you write programs to search for arbitrarily complex patterns search engines generally only allow you to search for individual words or strings of words sometimes with wildcards second search engines give inconsistent results and can give widely different figures when used at different times or in different geographical regions when content has been duplicated across multiple sites search results may be boosted finally the markup in the result returned by a search engine may change unpredictably breaking any pattern based method of locating particular content a problem which is ameliorated by the use of search engine apis note your turn search the web for the of inside quotes based on the large count can we conclude that the of is a frequent collocation in english processing rss feeds the blogosphere is an important source of text in both formal and informal registers with the help of a python library called the universal feed parser available from https pypi python org pypi feedparser we can access the content of a blog as shown below with some further work we can write programs to create a small corpus of blog posts and use this as the basis for our nlp work reading local files we can also read a file one line at a time using a for loop here we use the strip method to remove the newline character at the end of the input line nltk corpus files can also be accessed using these methods we simply have to use nltk data find to get the filename for any corpus item then we can open and read it in the way we just demonstrated above extracting text from pdf msword and other binary formats ascii text and html text are human readable formats text often comes in binary formats like pdf and msword that can only be opened using specialized software third party libraries such as pypdf and pywin provide access to these formats extracting text from multi column documents is particularly challenging for once off conversion of a few documents it is simpler to open the document with a suitable application then save it as text to your local drive and access it as described below if the document is already on the web you can enter its url in google search box the search result often includes a link to an html version of the document which you can save as text capturing user input sometimes we want to capture the text that a user inputs when she is interacting with our program to prompt the user to type a line of input call the python function input after saving the input to a variable we can manipulate it just as we have done for other strings the nlp pipeline summarizes what we have covered in this section including the process of building a vocabulary that we saw in one step normalization will be discussed in figure the processing pipeline we open a url and read its html content remove the markup and select a slice of characters this is then tokenized and optionally converted into an nltk text object we can also lowercase all the words and extract the vocabulary there is a lot going on in this pipeline to understand it properly it helps to be clear about the type of each variable that it mentions we find out the type of any python object x using type x e g type is int since is an integer when we load the contents of a url or file and when we strip out html markup we are dealing with strings python str data type we will learn more about strings in when we tokenize a string we produce a list of words and this is python list type normalizing and sorting lists produces other lists the type of an object determines what operations you can perform on it so for emple we can append to a list but not to a string similarly we can concatenate strings with strings and lists with lists but we cannot concatenate strings with lists strings text processing at the lowest level it is time to emine a fundamental data type that we have been studiously avoiding so far in earlier chapters we focused on a text as a list of words we did not look too closely at words and how they are handled in the programming language by using nltk corpus interface we were able to ignore the files that these texts had come from the contents of a word and of a file are represented by programming languages as a fundamental data type known as a string in this section we explore strings in detail and show the connection between strings words texts and files basic operations with strings strings are specified using single quotes or double quotes as shown below if a string contains a single quote we must backslash escape the quote so python knows a literal quote character is intended or else put the string in double quotes otherwise the quote inside the string will be interpreted as a close quote and the python interpreter will report a syntax error sometimes strings go over several lines python provides us with various ways of entering them in the next emple a sequence of two strings is joined into a single string we need to use backslash or parentheses so that the interpreter knows that the statement is not complete after the first line unfortunately the above methods do not give us a newline between the two lines of the sonnet instead we can use a triple quoted string as follows now that we can define strings we can try some simple operations on them first let look at the operation known as concatenation it produces a new string that is a copy of the two original strings pasted together end to end notice that concatenation does not do anything clever like insert a space between the words we can even multiply strings note your turn try running the following code then try to use your understanding of the string and operations to figure out how it works be careful to distinguish between the string which is a single whitespace character and which is the empty string we have seen that the addition and multiplication operations apply to strings not just numbers however note that we cannot use subtraction or division with strings these error messages are another emple of python telling us that we have got our data types in a muddle in the first case we are told that the operation of subtraction i e cannot apply to objects of type str strings while in the second we are told that division cannot take str and int as its two operands printing strings so far when we have wanted to look at the contents of a variable or see the result of a calculation we have just typed the variable name into the interpreter we can also see the contents of a variable using the print statement notice that there are no quotation marks this time when we inspect a variable by typing its name in the interpreter the interpreter prints the python representation of its value since it is a string the result is quoted however when we tell the interpreter to print the contents of the variable we do not see quotation characters since there are none inside the string the print statement allows us to display more than one item on a line in various ways as shown below accessing individual characters as we saw in for lists strings are indexed starting from zero when we index a string we get one of its characters or letters a single character is nothing special it is just a string of length as with lists if we try to access an index that is outside of the string we get an error again as with lists we can use negative indexes for strings where is the index of the last character positive and negative indexes give us two ways to refer to any position in a string in this case when the string had a length of indexes and both refer to the same character a space notice that len monty we can write for loops to iterate over the characters in strings this print function includes the optional end parameter which is how we tell python to print a space instead of a newline at the end we can count individual characters as well we should ignore the case distinction by normalizing everything to lowercase and filter out non alphabetic characters this gives us the letters of the alphabet with the most frequently occurring letters listed first this is quite complicated and we will explain it more carefully below you might like to visualize the distribution using fdist plot the relative character frequencies of a text can be used in automatically identifying the language of the text accessing substrings figure string slicing the string monty python is shown along with its positive and negative indexes two substrings are selected using slice notation the slice m n contains the characters from position m through n a substring is any continuous section of a string that we want to pull out for further processing we can easily access substrings using the same slice notation we used for lists see for emple the following code accesses the substring starting at index up to but not including index here we see the characters are p y t and h which correspond to monty monty but not monty this is because a slice starts at the first index but finishes one before the end index we can also slice with negative indexes the same basic rule of starting from the start index and stopping one before the end index applies here we stop before the space character as with list slices if we omit the first value the substring begins at the start of the string if we omit the second value the substring continues to the end of the string we test if a string contains a particular substring using the in operator as follows we can also find the position of a substring within a string using find note your turn make up a sentence and assign it to a variable e g sent my sentence now write slice expressions to pull out individual words this is obviously not a convenient way to process the words of a text more operations on strings python has comprehensive support for processing strings a summary including some operations we have not seen yet is shown in for more information on strings type help str at the python prompt table useful string methods operations on strings in addition to the string tests shown in all methods produce a new string or list the difference between lists and strings strings and lists are both kinds of sequence we can pull them apart by indexing and slicing them and we can join them together by concatenating them however we cannot join strings and lists when we open a file for reading into a python program we get a string corresponding to the contents of the whole file if we use a for loop to process the elements of this string all we can pick out are the individual characters we do not get to choose the granularity by contrast the elements of a list can be as big or small as we like for emple they could be paragraphs sentences phrases words characters so lists have the advantage that we can be flexible about the elements they contain and correspondingly flexible about any downstream processing consequently one of the first things we are likely to do in a piece of nlp code is tokenize a string into a list of strings conversely when we want to write our results to a file or to a terminal we will usually format them as a string lists and strings do not have ectly the same functionality lists have the added power that you can change their elements on the other hand if we try to do that with a string changing the th character in query to f we get this is because strings are immutable you ca not change a string once you have created it however lists are mutable and their contents can be modified at any time as a result lists support operations that modify the original value rather than producing a new value note your turn consolidate your knowledge of strings by trying some of the exercises on strings at the end of this chapter text processing with unicode our programs will often need to deal with different languages and different character sets the concept of plain text is a fiction if you live in the english speaking world you probably use ascii possibly without realizing it if you live in europe you might use one of the extended latin character sets containing such characters as for danish and norwegian for hungarian for spanish and breton and for czech and slovak in this section we will give an overview of how to use unicode for processing texts that use non ascii character sets what is unicode unicode supports over a million characters each character is assigned a number called a code point in python code points are written in the form uxxxx where xxxx is the number in digit hedecimal form within a program we can manipulate unicode strings just like normal strings however when unicode characters are stored in files or displayed on a terminal they must be encoded as a stream of bytes some encodings such as ascii and latin use a single byte per code point so they can only support a small subset of unicode enough for a single language other encodings such as utf use multiple bytes and can represent the full range of unicode characters text in files will be in a particular encoding so we need some mechanism for translating it into unicode translation into unicode is called decoding conversely to write out unicode to a file or a terminal we first need to translate it into a suitable encoding this translation out of unicode is called encoding and is illustrated in figure unicode decoding and encoding from a unicode perspective characters are abstract entities which can be realized as one or more glyphs only glyphs can appear on a screen or be printed on paper a font is a mapping from characters to glyphs extracting encoded text from files let assume that we have a small text file and that we know how it is encoded for emple polish lat txt as the name suggests is a snippet of polish text from the polish wikipedia see http pl wikipedia org wiki biblioteka pruska this file is encoded as latin also known as iso the function nltk data find locates the file for us the python open function can read encoded data into unicode strings and write out unicode strings in encoded form it takes a parameter to specify the encoding of the file being read or written so let open our polish file with the encoding latin and inspect the contents of the file if this does not display correctly on your terminal or if we want to see the underlying numerical values or codepoints of the characters then we can convert all non ascii characters into their two digit xxx and four digit uxxxx representations the first line above illustrates a unicode escape string preceded by the u escape string namely u the relevant unicode character will be dislayed on the screen as the glyph in the third line of the preceding emple we see xf which corresponds to the glyph and is within the range in python source code is encoded using utf by default and you can include unicode characters in strings if you are using idle or another program editor that supports unicode arbitrary unicode characters can be included using the uxxxx escape sequence we find the integer ordinal of a character using ord for emple the hedecimal digit notation for is type hex to discover this and we can define a string with the appropriate escape sequence note there are many factors determining what glyphs are rendered on your screen if you are sure that you have the correct encoding but your python code is still failing to produce the glyphs you expected you should also check that you have the necessary fonts installed on your system it may be necessary to configure your locale to render utf encoded characters then use print nacute encode utf in order to see the displayed in your terminal we can also see how this character is represented as a sequence of bytes inside a text file the module unicodedata lets us inspect the properties of unicode characters in the following emple we select all characters in the third line of our polish text outside the ascii range and print their utf byte sequence followed by their code point integer using the standard unicode convention i e prefixing the hex digits with u followed by their unicode name if you replace c encode utf in with c and if your system supports utf you should see an output like the following u f latin small letter o with acute u b latin small letter s with acute u a latin capital letter s with acute u latin small letter a with ogonek u latin small letter l with stroke alternatively you may need to replace the encoding utf in the emple by latin again depending on the details of your system the next emples illustrate how python string methods and the re module can work with unicode characters we will take a close look at the re module in the following section the w matches a word character cf nltk tokenizers allow unicode strings as input and correspondingly yield unicode strings as output using your local encoding in python if you are used to working with characters in a particular local encoding you probably want to be able to use your standard methods for inputting and editing strings in a python file in order to do this you need to include the string coding coding as the first or second line of your file note that coding has to be a string like latin big or utf see figure unicode and idle utf encoded string literals in the idle editor this requires that an appropriate font is set in idle preferences here we have chosen courier ce regular expressions for detecting word patterns many linguistic processing tasks involve pattern matching for emple we can find words ending with ed using endswith ed we saw a variety of such word tests in regular expressions give us a more powerful and flexible method for describing the character patterns we are interested in note there are many other published introductions to regular expressions organized around the syntax of regular expressions and applied to searching text files instead of doing this again we focus on the use of regular expressions at different stages of linguistic processing as usual we will adopt a problem based approach and present new features only as they are needed to solve practical problems in our discussion we will mark regular expressions using chevrons like this patt to use regular expressions in python we need to import the re library using import re we also need a list of words to search we will use the words corpus again we will preprocess it to remove any proper names using basic meta characters let find words ending with ed using the regular expression ed we will use the re search p s function to check whether the pattern p can be found somewhere inside the string s we need to specify the characters of interest and use the dollar sign which has a special behavior in the context of regular expressions in that it matches the end of the word the wildcard symbol matches any single character suppose we have room in a crossword puzzle for an letter word with j as its third letter and t as its sixth letter in place of each blank cell we use a period note your turn the caret symbol matches the start of a string just like the matches the end what results do we get with the above emple if we leave out both of these and search for j t finally the symbol specifies that the previous character is optional thus e mail will match both email and e mail we could count the total number of occurrences of this word in either spelling in a text using sum for w in text if re search e mail w ranges and closures figure t text on keys the t system is used for entering text on mobile phones see two or more words that are entered with the same sequence of keystrokes are known as textonyms for emple both hole and golf are entered by pressing the sequence what other words could be produced with the same sequence here we use the regular expression ghi mno jlk def the first part of the expression ghi matches the start of a word followed by g h or i the next part of the expression mno constrains the second character to be m n or o the third and fourth characters are also constrained only four words satisfy all these constraints note that the order of characters inside the square brackets is not significant so we could have written hig nom ljk fed and matched the same words note your turn look for some finger twisters by searching for words that only use part of the number pad for emple ghijklmno or more concisely g o will match words that only use keys in the center row and a fj o will match words that use keys in the top right corner what do and mean let explore the symbol a bit further notice that it can be applied to individual letters or to bracketed sets of letters it should be clear that simply means one or more instances of the preceding item which could be an individual character like m a set like fed or a range like d f now let s replace with which means zero or more instances of the preceding item the regular expression m i n e will match everything that we found using m i n e but also words where some of the letters don t appear at all e g me min and mmmmm note that the and symbols are sometimes referred to as kleene closures or simply closures the operator has another function when it appears as the first character inside square brackets for emple aeiouaeiou matches any character other than a vowel we can search the nps chat corpus for words that are made up entirely of non vowel characters using aeiouaeiou to find items like these grrr cyb r and zzzzzzzz notice this includes non alphabetic characters here are some more emples of regular expressions being used to find tokens that match a particular pattern illustrating the use of some new symbols and note your turn study the above emples and try to work out what the and notations mean before you read on you probably worked out that a backslash means that the following character is deprived of its special powers and must literally match a specific character in the word thus while is special only matches a period the braced expressions like specify the number of repeats of the previous item the pipe character indicates a choice between the material on its left or its right parentheses indicate the scope of an operator they can be used together with the pipe or disjunction symbol like this w i e ai oo t matching wit wet wait and woot it is instructive to see what happens when you omit the parentheses from the last expression above and search for ed ing the meta characters we have seen are summarized in table basic regular expression meta characters including wildcards ranges and closures to the python interpreter a regular expression is just like any other string if the string contains a backslash followed by particular characters it will interpret these specially for emple b would be interpreted as the backspace character in general when using regular expressions containing backslash we should instruct the interpreter not to look inside the string at all but simply to pass it directly to the re library for processing we do this by prefixing the string with the letter r to indicate that it is a raw string for emple the raw string r band b contains two b symbols that are interpreted by the re library as matching word boundaries instead of backspace characters if you get into the habit of using r for regular expressions as we will do from now on you will avoid having to think about these complications useful applications of regular expressions the above emples all involved searching for words w that match some regular expression regexp using re search regexp w apart from checking if a regular expression matches a word we can use regular expressions to extract material from words or to modify words in specific ways extracting word pieces the re findall find all method finds all non overlapping matches of the given regular expression let s find all the vowels in a word then count them let look for all sequences of two or more vowels in some text and determine their relative frequency note your turn in the w c date time format dates are represented like this replace the in the following python code with a regular expression in order to convert the string to a list of integers int n for n in re findall doing more with word pieces once we can use re findall to extract material from words there is interesting things to do with the pieces like glue them back together or plot them it is sometimes noted that english text is highly redundant and it is still easy to read when word internal vowels are left out for emple declaration becomes dclrtn and inalienable becomes inlnble retaining any initial or final vowel sequences the regular expression in our next emple matches initial vowel sequences final vowel sequences and all consonants everything else is ignored this three way disjunction is processed left to right if one of the three parts matches the word any later parts of the regular expression are ignored we use re findall to extract all the matching pieces and join to join them together see for more about the join operation next let combine regular expressions with conditional frequency distributions here we will extract all consonant vowel sequences from the words of rotokas such as ka and si since each of these is a pair it can be used to initialize a conditional frequency distribution we then tabulate the frequency of each pair emining the rows for s and t we see they are in partial complementary distribution which is evidence that they are not distinct phonemes in the language thus we could conceivably drop s from the rotokas alphabet and simply have a pronunciation rule that the letter t is pronounced s when followed by i note that the single entry having su namely kasuari cassowary is borrowed from english if we want to be able to inspect the words behind the numbers in the above table it would be helpful to have an index allowing us to quickly find the list of words that contains a given consonant vowel pair e g cv index su should give us all words containing su here is how we can do this this program processes each word w in turn and for each one finds every substring that matches the regular expression ptksvr aeiou in the case of the word kasuari it finds ka su and ri therefore the cv word pairs list will contain ka kasuari su kasuari and ri kasuari one further step using nltk index converts this into a useful index finding word stems when we use a web search engine we usually do not mind or even notice if the words in the document differ from our search terms in having different endings a query for laptops finds documents containing laptop and vice versa indeed laptop and laptops are just two forms of the same dictionary word or lemma for some language processing tasks we want to ignore word endings and just deal with word stems there are various ways we can pull out the stem of a word here is a simple minded approach which just strips off anything that looks like a suffix although we will ultimately use nltk built in stemmers it is interesting to see how we can use regular expressions for this task our first step is to build up a disjunction of all the suffixes we need to enclose it in parentheses in order to limit the scope of the disjunction here re findall just gave us the suffix even though the regular expression matched the entire word this is because the parentheses have a second function to select substrings to be extracted if we want to use the parentheses to specify the scope of the disjunction but not to select the material to be output we have to add which is just one of many arcane subtleties of regular expressions here is the revised version however we would actually like to split the word into stem and suffix so we should just parenthesize both parts of the regular expression this looks promising but still has a problem let look at a different word processes the regular expression incorrectly found an s suffix instead of an es suffix this demonstrates another subtlety the star operator is greedy and the part of the expression tries to consume as much of the input as possible if we use the non greedy version of the star operator written we get what we want this works even when we allow an empty suffix by making the content of the second parentheses optional this approach still has many problems can you spot them but we will move on to define a function to perform stemming and apply it to a whole text notice that our regular expression removed the s from ponds but also from is and basis it produced some non words like distribut and deriv but these are acceptable stems in some applications searching tokenized text you can use a special kind of regular expression for searching across multiple words in a text where a text is a list of tokens for emple a man finds all instances of a man in the text the angle brackets are used to mark token boundaries and any whitespace between the angle brackets is ignored behaviors that are unique to nltk s findall method for texts in the following emple we include which will match any single token and enclose it in parentheses so only the matched word e g monied and not the matched phrase e g a monied man is produced the second emple finds three word phrases ending with the word bro the last emple finds sequences of three or more words starting with the letter l note your turn consolidate your understanding of regular expression patterns and substitutions using nltk re show p s which annotates the string s to show every place where pattern p was matched and nltk app nemo which provides a graphical interface for exploring regular expressions for more practice try some of the exercises on regular expressions at the end of this chapter it is easy to build search patterns when the linguistic phenomenon we are studying is tied to particular words in some cases a little creativity will go a long way for instance searching a large text corpus for expressions of the form x and other ys allows us to discover hypernyms cf with enough text this approach would give us a useful store of information about the taxonomy of objects without the need for any manual labor however our search results will usually contain false positives i e cases that we would want to exclude for emple the result demands and other factors suggests that demand is an instance of the type factor but this sentence is actually about wage demands nevertheless we could construct our own ontology of english concepts by manually correcting the output of such searches note this combination of automatic and manual processing is the most common way for new corpora to be constructed we will return to this in searching corpora also suffers from the problem of false negatives i e omitting cases that we would want to include it is risky to conclude that some linguistic phenomenon does not exist in a corpus just because we could not find any instances of a search pattern perhaps we just did not think carefully enough about suitable patterns note your turn look for instances of the pattern as x as y to discover information about entities and their properties normalizing text in earlier program emples we have often converted text to lowercase before doing anything with its words e g set w lower for w in text by using lower we have normalized the text to lowercase so that the distinction between the and the is ignored often we want to go further than this and strip off any affixes a task known as stemming a further step is to make sure that the resulting form is a known word in a dictionary a task known as lemmatization we discuss each of these in turn first we need to define the data we will use in this section stemmers nltk includes several off the shelf stemmers and if you ever need a stemmer you should use one of these in preference to crafting your own using regular expressions since these handle a wide range of irregular cases the porter and lancaster stemmers follow their own rules for stripping affixes observe that the porter stemmer correctly handles the word lying mapping it to lie while the lancaster stemmer does not stemming is not a well defined process and we typically pick the stemmer that best suits the application we have in mind the porter stemmer is a good choice if you are indexing some texts and want to support search using alternative forms of words illustrated in which uses object oriented programming techniques that are outside the scope of this book string formatting techniques to be covered in and the enumerate function to be explained in lemmatization the wordnet lemmatizer only removes affixes if the resulting word is in its dictionary this additional checking process makes the lemmatizer slower than the above stemmers notice that it does not handle lying but it converts women to woman the wordnet lemmatizer is a good choice if you want to compile the vocabulary of some texts and want a list of valid lemmas or lexicon headwords note another normalization task involves identifying non standard words including numbers abbreviations and dates and mapping any such tokens to a special vocabulary for emple every decimal number could be mapped to a single token and every acronym could be mapped to aaa this keeps the vocabulary small and improves the accuracy of many language modeling tasks so far our format strings generated output of arbitrary width on the page or screen we can add padding to obtain output of a given width by inserting into the brackets a colon followed by an integer so specifies that we want a string that is padded to width it is right justified by default for numbers but we can precede the width specifier with a alignment option to make numbers left justified strings are left justified by default but can be right justified with the alignment option system message error ch rst line unexpected indentation other control characters can be used to specify the sign and precision of floating point numbers for emple f indicates that four digits should be displayed after the decimal point for a floating point number the string formatting is smart enough to know that if you include a in your format specification then you want to represent the value as a percentage there is no need to multiply by an important use of formatting strings is for tabulating data recall that in we saw data being tabulated from a conditional frequency distribution let perform the tabulation ourselves exercising full control of headings and column widths as shown in note the clear separation between the language processing work and the tabulation of results recall from the listing in that we used a format string width and bound a value to the width parameter in format this allows us to specify the width of a field using a variable we could use this to automatically customize the column to be just wide enough to accommodate all the words using width max len w for w in words writing results to a file we have seen how to read text from files it is often useful to write output to files as well the following code opens a file output txt for writing and saves the program output to the file when we write non text data to a file we must convert it to a string first we can do this conversion using formatting strings as we saw above let write the total number of words to our file caution you should avoid filenames that contain space characters like output file txt or that are identical except for case distinctions e g output txt and output txt text wrapping when the output of our program is text like instead of tabular it will usually be necessary to wrap it so that it can be displayed conveniently consider the following output which overflows its line and which uses a complicated print statement we can take care of line wrapping with the help of python textwrap module for maximum clarity we will separate each step onto its own line notice that there is a linebreak between more and its following number if we wanted to avoid this we could redefine the formatting string so that it contained no spaces e g s d then instead of printing the value of wrapped we could print wrapped replace summary in this book we view a text as a list of words a raw text is a potentially long string containing words and whitespace formatting and is how we typically store and visualize a text a string is specified in python using single or double quotes monty python monty python the characters of a string are accessed using indexes counting from zero monty python gives the value m the length of a string is found using len substrings are accessed using slice notation monty python gives the value onty if the start index is omitted the substring begins at the start of the string if the end index is omitted the slice continues to the end of the string strings can be split into lists monty python split gives monty python lists can be joined into strings join monty python gives monty python we can read text from a file input txt using text open input txt read we can read text from url using text request urlopen url read decode utf we can iterate over the lines of a text file using for line in open f we can write text to a file by opening the file for writing output file open output txt w then adding content to the file print monty python file output file texts found on the web may contain unwanted material such as headers footers markup that need to be removed before we do any linguistic processing tokenization is the segmentation of a text into basic units or tokens such as words and punctuation tokenization based on whitespace is inadequate for many applications because it bundles punctuation together with words nltk provides an off the shelf tokenizer nltk word tokenize lemmatization is a process that maps the various forms of a word such as appeared appears to the canonical or citation form of the word also known as the lexeme or lemma e g appear regular expressions are a powerful and flexible method of specifying patterns once we have imported the re module we can use re findall to find all substrings in a string that match a pattern if a regular expression string includes a backslash you should tell python not to preprocess the string by using a raw string with an r prefix r aregexp when backslash is used before certain characters e g n this takes on a special meaning newline character however when backslash is used before regular expression wildcards and operators e g these characters lose their special meaning and are matched literally a string formatting expression template arg tuple consists of a format string template that contains conversion specifiers like s and d further reading extra materials for this chapter are posted at http nltk org including links to freely available resources on the web remember to consult the python reference materials at http docs python org for emple this documentation covers universal newline support explaining how to work with the different newline conventions used by various operating systems for more emples of processing words with nltk see the tokenization stemming and corpus howtos at http nltk org howto chapters and of jurafsky martin contain more advanced material on regular expressions and morphology for more extensive discussion of text processing with python see mertz for information about normalizing non standard words see sproat et al there are many references for regular expressions both practical and theoretical for an introductory tutorial to using regular expressions in python see kuchling regular expression howto http www amk ca python howto regex for a comprehensive and detailed manual in using regular expressions covering their syntax in most major programming languages including python see friedl other presentations include section of jurafsky martin and chapter of mertz there are many online resources for unicode useful discussions of python facilities for handling unicode are ned batchelder pragmatic unicode http nedbatchelder com text unipain html unicode howto python documentation http docs python org howto unicode html david beazley mastering python i o http pyvideo org video pycon mastering python i o joel spolsky the absolute minimum every software developer absolutely positively must know about unicode and character sets no excuses http www joelonsoftware com articles unicode html the problem of tokenizing chinese text is a major focus of sighan the acl special interest group on chinese language processing http sighan org our method for segmenting english text follows brent this work falls in the area of language acquisition niyogi collocations are a special case of multiword expressions a multiword expression is a small phrase whose meaning and other properties cannot be predicted from its words alone e g part of speech baldwin kim simulated annealing is a heuristic for finding a good approximation to the optimum value of a function in a large discrete search space based on an analogy with annealing in metallurgy the technique is described in many artificial intelligence texts the approach to discovering hyponyms in text using search patterns like x and other ys is described by hearst exercises define a string s colorless write a python statement that changes this to colourless using only the slice and concatenation operations we can use the slice notation to remove morphological endings on words for emple dogs removes the last character of dogs leaving dog use slice notation to remove the affixes from these words we have inserted a hyphen to indicate the affix boundary but omit this from your strings dish es run ning nation ality un do pre heat we saw how we can generate an indexerror by indexing beyond the end of a string is it possible to construct an index that goes too far to the left before the start of the string we can specify a step size for the slice the following returns every second character within the slice monty it also works in the reverse direction monty try these for yourself then experiment with different step values what happens if you ask the interpreter to evaluate monty explain why this is a reasonable result describe the class of strings matched by the following regular expressions a za z a z a z p aeiou t d d aeiou aeiou aeiou w w s test your answers using nltk re show write regular expressions to match the following classes of strings a single determiner assume that a an and the are the only determiners an arithmetic expression using integers addition and multiplication such as write a utility function that takes a url as its argument and returns the contents of the url with all html markup removed use from urllib import request and then request urlopen http nltk org read decode utf to access the contents of the url save some text into a file corpus txt define a function load f that reads from the file named in its sole argument and returns a string containing the text of the file use nltk regexp tokenize to create a tokenizer that tokenizes the various kinds of punctuation in this text use one multi line regular expression with inline comments using the verbose flag x use nltk regexp tokenize to create a tokenizer that tokenizes the following kinds of expression monetary amounts dates names of people and organizations rewrite the following loop as a list comprehension define a string raw containing a sentence of your own choosing now split raw on some character other than space such as s write a for loop to print out the characters of a string one per line what is the difference between calling split on a string with no argument or with as the argument e g sent split versus sent split what happens when the string being split contains tab characters consecutive space characters or a sequence of tabs and spaces in idle you will need to use t to enter a tab character create a variable words containing a list of words experiment with words sort and sorted words what is the difference explore the difference between strings and integers by typing the following at a python prompt and try converting between strings and integers using int and str use a text editor to create a file called prog py containing the single line monty monty python next start up a new session with the python interpreter and enter the expression monty at the prompt you will get an error from the interpreter now try the following note that you have to leave off the py part of the filename this time python should return with a value you can also try import prog in which case python should be able to evaluate the expression prog monty at the prompt what happens when the formatting strings s and s are used to display strings that are longer than six characters read in some text from a corpus tokenize it and print the list of all wh word types that occur wh words in english are used in questions relative clauses and exclamations who which what and so on print them in order are any words duplicated in this list because of the presence of case distinctions or punctuation create a file consisting of words and made up frequencies where each line consists of a word the space character and a positive integer e g fuzzy read the file into a python list using open filename readlines next break each line into its two fields using split and convert the number into an integer using int the result should be a list of the form fuzzy write code to access a favorite webpage and extract some text from it for emple access a weather site and extract the forecast top temperature for your town or city today write a function unknown that takes a url as its argument and returns a list of unknown words that occur on that webpage in order to do this extract all substrings consisting of lowercase letters using re findall and remove any items from this set that occur in the words corpus nltk corpus words try to categorize these words manually and discuss your findings emine the results of processing the url http news bbc co uk using the regular expressions suggested above you will see that there is still a fair amount of non textual data there particularly javascript commands you may also find that sentence breaks have not been properly preserved define further regular expressions that improve the extraction of text from this web page are you able to write a regular expression to tokenize text in such a way that the word do not is tokenized into do and n t explain why this regular expression wo not work n t w try to write code to convert text into hack r using regular expressions and substitution where e i o l s w t ate normalize the text to lowercase before converting it add more substitutions of your own now try to map s to two different values for word initial s and for word internal s pig latin is a simple transformation of english text each word of the text is converted as follows move any consonant or consonant cluster that appears at the start of the word to the end then append ay e g string ingstray idle idleay http en wikipedia org wiki pig latin write a function to convert a word to pig latin write code that converts text instead of individual words extend it further to preserve capitalization to keep qu together i e so that quiet becomes ietquay and to detect when y is used as a consonant e g yellow vs a vowel e g style download some text from a language that has vowel harmony e g hungarian extract the vowel sequences of words and create a vowel bigram table python s random module includes a function choice which randomly chooses an item from a sequence e g choice aehh will produce one of four possible characters with the letter h being twice as frequent as the others write a generator expression that produces a sequence of randomly chosen letters drawn from the string aehh and put this expression inside a call to the join function to concatenate them into one long string you should get a result that looks like uncontrolled sneezing or maniacal laughter he haha ee heheeh eha use split and join again to normalize the whitespace in this string consider the numeric expressions in the following sentence from the medline corpus the corresponding free cortisol fractions in these sera were and respectively should we say that the numeric expression is three words or should we say that it s a single compound word or should we say that it is actually nine words since it s read four point five three plus or minus zero point fifteen percent or should we say that it s not a real word at all since it wouldn t appear in any dictionary discuss these different possibilities can you think of application domains that motivate at least two of these answers readability measures are used to score the reading difficulty of a text for the purposes of selecting texts of appropriate difficulty for language learners let us define w to be the average number of letters per word and s to be the average number of words per sentence in a given text the automated readability index ari of the text is defined to be w s compute the ari score for various sections of the brown corpus including section f lore and j learned make use of the fact that nltk corpus brown words produces a sequence of words while nltk corpus brown sents produces a sequence of sentences use the porter stemmer to normalize some tokenized text calling the stemmer on each word do the same thing with the lancaster stemmer and see if you observe any differences define the variable saying to contain the list after all is said and done more is said than done process this list using a for loop and store the length of each word in a new list lengths hint begin by assigning the empty list to lengths using lengths then each time through the loop use append to add another length value to the list now do the same thing using a list comprehension define a variable silly to contain the string newly formed bland ideas are inexpressible in an infuriating way this happens to be the legitimate interpretation that bilingual english spanish speakers can assign to chomsky famous nonsense phrase colorless green ideas sleep furiously according to wikipedia now write code to perform the following tasks split silly into a list of strings one per word using python split operation and save this to a variable called bland extract the second letter of each word in silly and join them into a string to get eoldrnnnna combine the words in bland back into a single string using join make sure the words in the resulting string are separated with whitespace print the words of silly in alphabetical order one per line the index function can be used to look up items in sequences for emple inexpressible index e tells us the index of the first position of the letter e what happens when you look up a substring e g inexpressible index re define a variable words containing a list of words now use words index to look up the position of an individual word define a variable silly as in the exercise above use the index function in combination with list slicing to build a list phrase consisting of all the words up to but not including in in silly write code to convert nationality adjectives like canadian and australian to their corresponding nouns canada and australia see http en wikipedia org wiki list of adjectival forms of place names read the languagelog post on phrases of the form as best as p can and as best p can where p is a pronoun investigate this phenomenon with the help of a corpus and the findall method for searching tokenized text described in http itre cis upenn edu myl languagelog archives html study the lolcat version of the book of genesis accessible as nltk corpus genesis words lolcat txt and the rules for converting text into lolspeak at http www lolcatbible com index php title how to speak lolcat define regular expressions to convert english words into corresponding lolspeak words read about the re sub function for string substitution using regular expressions using help re sub and by consulting the further readings for this chapter use re sub in writing code to remove html tags from an html file and to normalize whitespace an interesting challenge for tokenization is words that have been split across a line break e g if long term is split then we have the string long nterm write a regular expression that identifies words that are hyphenated at a line break the expression will need to include the n character use re sub to remove the n character from these words how might you identify words that should not remain hyphenated once the newline is removed e g encyclo npedia x read the wikipedia entry on soundex implement this algorithm in python obtain raw texts from two or more genres and compute their respective reading difficulty scores as in the earlier exercise on reading difficulty e g compare abc rural news and abc science news nltk corpus abc use punkt to perform sentence segmentation rewrite the following nested loop as a nested list comprehension use wordnet to create a semantic index for a text collection extend the concordance search program in indexing each word using the offset of its first synset e g wn synsets dog offset and optionally the offset of some of its ancestors in the hypernym hierarchy with the help of a multilingual corpus such as the universal declaration of human rights corpus nltk corpus udhr and nltk frequency distribution and rank correlation functionality nltk freqdist nltk spearman correlation develop a system that guesses the language of a previously unseen text for simplicity work with a single character encoding and just a few languages write a program that processes a text and discovers cases where a word has been used with a novel sense for each word compute the wordnet similarity between all synsets of the word and all synsets of the words in its context note that this is a crude approach doing it well is a difficult open research problem read the article on normalization of non standard words sproat et al and implement a similar system for text normalization writing structured programs by now you will have a sense of the capabilities of the python programming language for processing natural language however if you are new to python or to programming you may still be wrestling with python and not feel like you are in full control yet in this chapter we will address the following questions how can you write well structured readable programs that you and others will be able to re use easily how do the fundamental building blocks work such as loops functions and assignment what are some of the pitfalls with python programming and how can you avoid them along the way you will consolidate your knowledge of fundamental programming constructs learn more about using features of the python language in a natural and concise way and learn some useful techniques in visualizing natural language data as before this chapter contains many emples and exercises and as before some exercises introduce new material readers new to programming should work through them carefully and consult other introductions to programming if necessary experienced programmers can quickly skim this chapter in the other chapters of this book we have organized the programming concepts as dictated by the needs of nlp here we revert to a more conventional approach where the material is more closely tied to the structure of the programming language there is not room for a complete presentation of the language so we will just focus on the language constructs and idioms that are most important for nlp back to the basics assignment assignment would seem to be the most elementary programming concept not deserving a separate discussion however there are some surprising subtleties here consider the following code fragment this behaves ectly as expected when we write bar foo in the above code the value of foo the string monty is assigned to bar that is bar is a copy of foo so when we overwrite foo with a new string python on line the value of bar is not affected however assignment statements do not always involve making copies in this way assignment always copies the value of an expression but a value is not always what you might expect it to be in particular the value of a structured object such as a list is actually just a reference to the object in the following emple assigns the reference of foo to the new variable bar now when we modify something inside foo on line we can see that the contents of bar have also been changed figure list assignment and computer memory two list objects foo and bar reference the same location in the computer memory updating foo will also modify bar and vice versa the line bar foo does not copy the contents of the variable only its object reference to understand what is going on here we need to know how lists are stored in the computer s memory in we see that a list foo is a reference to an object stored at location which is itself a series of pointers to other locations holding strings when we assign bar foo it is just the object reference that gets copied this behavior extends to other aspects of the language such as parameter passing let experiment some more by creating a variable empty holding the empty list then using it three times on the next line observe that changing one of the items inside our nested list of lists changed them all this is because each of the three elements is actually just a reference to one and the same list in memory note your turn use multiplication to create a list of lists nested now modify one of the elements of the list and observe that all the elements are changed use python id function to find out the numerical identifier for any object and verify that id nested id nested and id nested are all the same now notice that when we assign a new value to one of the elements of the list it does not propagate to the others we began with a list containing three references to a single empty list object then we modified that object by appending python to it resulting in a list containing three references to a single list object python next we overwrote one of those references with a reference to a new object monty this last step modified one of the three object references inside the nested list however the python object was not changed and is still referenced from two places in our nested list of lists it is crucial to appreciate this difference between modifying an object via an object reference and overwriting an object reference note important to copy the items from a list foo to a new list bar you can write bar foo this copies the object references inside the list to copy a structure without copying any object references use copy deepcopy equality python provides two ways to check that a pair of items are the same the is operator tests for object identity we can use it to verify our earlier observations about objects first we create a list containing several copies of the same object and demonstrate that they are not only identical according to but also that they are one and the same object now let put a new python in this nest we can easily show that the objects are not all identical you can do several pairwise tests to discover which position contains the interloper but the id function makes detection easier this reveals that the second item of the list has a distinct identifier if you try running this code snippet yourself expect to see different numbers in the resulting list and also the interloper may be in a different position having two kinds of equality might seem strange however it is really just the type token distinction familiar from natural language here showing up in a programming language conditionals in the condition part of an if statement a nonempty string or list is evaluated as true while an empty string or list evaluates as false that is we do not need to say if len element in the condition what the difference between using if elif as opposed to using a couple of if statements in a row well consider the following situation since the if clause of the statement is satisfied python never tries to evaluate the elif clause so we never get to print out by contrast if we replaced the elif by an if then we would print out both and so an elif clause potentially gives us more information than a bare if clause when it evaluates to true it tells us not only that the condition is satisfied but also that the condition of the main if clause was not satisfied so far we have seen two kinds of sequence object strings and lists another kind of sequence is called a tuple tuples are formed with the comma operator and typically enclosed using parentheses we ve actually seen them in the previous chapters and sometimes referred to them as pairs since there were always two members however tuples can have any number of members like lists and strings tuples can be indexed and sliced and have a length caution tuples are constructed using the comma operator parentheses are a more general feature of python syntax designed for grouping a tuple containing the single element snark is defined by adding a trailing comma like this snark the empty tuple is a special case and is defined using empty parentheses let compare strings lists and tuples directly and do the indexing slice and length operation on each type notice in this code sample that we computed multiple values on a single line separated by commas these comma separated expressions are actually just tuples python allows us to omit the parentheses around tuples if there is no ambiguity when we print a tuple the parentheses are always displayed by using tuples in this way we are implicitly aggregating items together operating on sequence types we can iterate over the items in a sequence s in a variety of useful ways as shown in table various ways to iterate over sequences the sequence functions illustrated in can be combined in various ways for emple to get unique elements of s sorted in reverse use reversed sorted set s we can randomize the contents of a list s before iterating over them using random shuffle s we can convert between these sequence types for emple tuple s converts any kind of sequence into a tuple and list s converts any kind of sequence into a list we can convert a list of strings to a single string using the join function e g join words some other objects such as a freqdist can be converted into a sequence using list or sorted and support iteration e g in the next emple we use tuples to re arrange the contents of our list we can omit the parentheses because the comma has higher precedence than assignment this is an idiomatic and readable way to move items inside a list it is equivalent to the following traditional way of doing such tasks that does not use tuples notice that this method needs a temporary variable tmp as we have seen python has sequence functions such as sorted and reversed that rearrange the items of a sequence there are also functions that modify the structure of a sequence and which can be handy for language processing thus zip takes the items of two or more sequences and zips them together into a single list of tuples given a sequence s enumerate s returns pairs consisting of an index and the item at that index note it is a widespread feature of python and nltk to only perform computation when required a feature known as lazy evaluation if you ever see a result like zip object at x d when you expect to see a sequence you can force the object to be evaluated just by putting it in a context that expects a sequence like list x or for item in x for some nlp tasks it is necessary to cut up a sequence into two or more parts for instance we might want to train a system on of the data and test it on the remaining to do this we decide the location where we want to cut the data then cut the sequence at that location we can verify that none of the original data is lost during this process nor is it duplicated we can also verify that the ratio of the sizes of the two pieces is what we intended combining different sequence types let combine our knowledge of these three sequence types together with list comprehensions to perform the task of sorting the words in a string by their length each of the above lines of code contains a significant feature a simple string is actually an object with methods defined on it such as split we use a list comprehension to build a list of tuples where each tuple consists of a number the word length and the word e g the we use the sort method to sort the list in place finally we discard the length information and join the words back into a single string the underscore is just a regular python variable but we can use underscore by convention to indicate that we will not use its value we began by talking about the commonalities in these sequence types but the above code illustrates important differences in their roles first strings appear at the beginning and the end this is typical in the context where our program is reading in some text and producing output for us to read lists and tuples are used in the middle but for different purposes a list is typically a sequence of objects all having the same type of arbitrary length we often use lists to hold sequences of words in contrast a tuple is typically a collection of objects of different types of fixed length we often use a tuple to hold a record a collection of different fields relating to some entity this distinction between the use of lists and tuples takes some getting used to so here is another emple here a lexicon is represented as a list because it is a collection of objects of a single type lexical entries of no predetermined length an individual entry is represented as a tuple because it is a collection of objects with different interpretations such as the orthographic form the part of speech and the pronunciations represented in the sampa computer readable phonetic alphabet http www phon ucl ac uk home sampa note that these pronunciations are stored using a list why note a good way to decide when to use tuples vs lists is to ask whether the interpretation of an item depends on its position for emple a tagged token combines two strings having different interpretation and we choose to interpret the first item as the token and the second item as the tag thus we use tuples like this grail noun a tuple of the form noun grail would be nonsensical since it would be a word noun tagged grail in contrast the elements of a text are all tokens and position is not significant thus we use lists like this venetian blind a list of the form blind venetian would be equally valid the linguistic meaning of the words might be different but the interpretation of list items as tokens is unchanged the distinction between lists and tuples has been described in terms of usage however there is a more fundamental difference in python lists are mutable while tuples are immutable in other words lists can be modified while tuples cannot here are some of the operations on lists that do in place modification of the list note your turn convert lexicon to a tuple using lexicon tuple lexicon then try each of the above operations to confirm that none of them is permitted on tuples generator expressions we have been making heavy use of list comprehensions for compact and readable processing of texts here is an emple where we tokenize and normalize a text suppose we now want to process these words further we can do this by inserting the above expression inside a call to some other function but python allows us to omit the brackets the second line uses a generator expression this is more than a notational convenience in many language processing situations generator expressions will be more efficient in storage for the list object must be allocated before the value of max is computed if the text is very large this could be slow in the data is streamed to the calling function since the calling function simply has to find the maximum value the word which comes latest in lexicographic sort order it can process the stream of data without having to store anything more than the maximum value seen so far questions of style programming is as much an art as a science the undisputed bible of programming a page multi volume work by donald knuth is called the art of computer programming many books have been written on literate programming recognizing that humans not just computers must read and understand programs here we pick up on some issues of programming style that have important ramifications for the readability of your code including code layout procedural vs declarative style and the use of loop variables python coding style when writing programs you make many subtle choices about names spacing comments and so on when you look at code written by other people needless differences in style make it harder to interpret the code therefore the designers of the python language have published a style guide for python code available at http www python org dev peps pep the underlying value presented in the style guide is consistency for the purpose of maximizing the readability of code we briefly review some of its key recommendations here and refer readers to the full guide for detailed discussion with emples code layout should use four spaces per indentation level you should make sure that when you write python code in a file you avoid tabs for indentation since these can be misinterpreted by different text editors and the indentation can be messed up lines should be less than characters long if necessary you can break a line inside parentheses brackets or braces because python is able to detect that the line continues over to the next line if you need to break a line outside parentheses brackets or braces you can often add extra parentheses and you can always add a backslash at the end of the line that is broken note typing spaces instead of tabs soon becomes a chore many programming editors have built in support for python and can automatically indent code and highlight any syntax errors including indentation errors for a list of python aware editors please see http wiki python org moin pythoneditors procedural vs declarative style we have just seen how the same task can be performed in different ways with implications for efficiency another factor influencing program development is programming style consider the following program to compute the average length of words in the brown corpus in this program we use the variable count to keep track of the number of tokens seen and total to store the combined length of all words this is a low level style not far removed from machine code the primitive operations performed by the computer cpu the two variables are just like a cpu registers accumulating values at many intermediate stages values that are meaningless until the end we say that this program is written in a procedural style dictating the machine operations step by step now consider the following program that computes the same thing the first line uses a generator expression to sum the token lengths while the second line computes the average as before each line of code performs a complete meaningful task which can be understood in terms of high level properties like total is the sum of the lengths of the tokens implementation details are left to the python interpreter the second program uses a built in function and constitutes programming at a more abstract level the resulting code is more declarative let s look at an extreme emple the equivalent declarative version uses familiar built in functions and its purpose is instantly recognizable another case where a loop variable seems to be necessary is for printing a counter with each line of output instead we can use enumerate which processes a sequence s and produces a tuple of the form i s i for each item in s starting with s here we enumerate the key value pairs of the frequency distribution resulting in nested tuples rank word count we print rank so that the counting appears to start from as required when producing a list of ranked items it is sometimes tempting to use loop variables to store a maximum or minimum value seen so far let use this method to find the longest word in a text however a more transparent solution uses two list comprehensions both having forms that should be familiar by now note that our first solution found the first word having the longest length while the second solution found all of the longest words which is usually what we would want although there is a theoretical efficiency difference between the two solutions the main overhead is reading the data into main memory once it is there a second pass through the data is effectively instantaneous we also need to balance our concerns about program efficiency with programmer efficiency a fast but cryptic solution will be harder to understand and maintain some legitimate uses for counters there are cases where we still want to use loop variables in a list comprehension for emple we need to use a loop variable to extract successive overlapping n grams from a list it is quite tricky to get the range of the loop variable right since this is a common operation in nlp nltk supports it with functions bigrams text and trigrams text and a general purpose ngrams text n here is an emple of how we can use loop variables in building multidimensional structures for emple to build an array with m rows and n columns where each cell is a set we could use a nested list comprehension observe that the loop variables i and j are not used anywhere in the resulting object they are just needed for a syntactically correct for statement as another emple of this usage observe that the expression very for i in range produces a list containing three instances of very with no integers in sight note that it would be incorrect to do this work using multiplication for reasons concerning object copying that were discussed earlier in this section iteration is an important programming device it is tempting to adopt idioms from other languages however python offers some elegant and highly readable alternatives as we have seen functions the foundation of structured programming functions provide an effective way to package and re use program code as already explained in for emple suppose we find that we often want to read text from an html file this involves several steps opening the file reading it in normalizing whitespace and stripping html markup we can collect these steps into a function and give it a name such as get text as shown in now any time we want to get cleaned up text from an html file we can just call get text with the name of the file as its only argument it will return a string and we can assign this to a variable e g contents get text test html each time we want to use this series of steps we only have to call the function using functions has the benefit of saving space in our program more importantly our choice of name for the function helps make the program readable in the case of the above emple whenever our program needs to read cleaned up text from a file we don t have to clutter the program with four lines of code we simply need to call get text this naming helps to provide some semantic interpretation it helps a reader of our program to see what the program means notice that the above function definition contains a string the first string inside a function definition is called a docstring not only does it document the purpose of the function to someone reading the code it is accessible to a programmer who has loaded the code from a file help get text help on function get text in module main get text file read text from a file normalizing whitespace and stripping html markup we have seen that functions help to make our work reusable and readable they also help make it reliable when we re use code that has already been developed and tested we can be more confident that it handles a variety of cases correctly we also remove the risk that we forget some important step or introduce a bug the program that calls our function also has increased reliability the author of that program is dealing with a shorter program and its components behave transparently to summarize as its name suggests a function captures functionality it is a segment of code that can be given a meaningful name and which performs a well defined task functions allow us to abstract away from the details to see a bigger picture and to program more effectively the rest of this section takes a closer look at functions exploring the mechanics and discussing ways to make your programs easier to read function inputs and outputs we pass information to functions using a function parameters the parenthesized list of variables and constants following the function name in the function definition here is a complete emple we first define the function to take two parameters msg and num then we call the function and pass it two arguments monty and these arguments fill the placeholders provided by the parameters and provide values for the occurrences of msg and num in the function body it is not necessary to have any parameters as we see in the following emple a function usually communicates its results back to the calling program via the return statement as we have just seen to the calling program it looks as if the function call had been replaced with the function result e g a python function is not required to have a return statement some functions do their work as a side effect printing a result modifying a file or updating the contents of a parameter to the function such functions are called procedures in some other programming languages consider the following three sort functions the third one is dangerous because a programmer could use it without realizing that it had modified its input in general functions should modify the contents of a parameter my sort or return a value my sort not both my sort parameter passing back in you saw that assignment works on values but that the value of a structured object is a reference to that object the same is true for functions python interprets function parameters as values this is known as call by value in the following code set up has two parameters both of which are modified inside the function we begin by assigning an empty string to w and an empty list to p after calling the function w is unchanged while p is changed notice that w was not changed by the function when we called set up w p the value of w an empty string was assigned to a new variable word inside the function the value of word was modified however that change did not propagate to w this parameter passing is identical to the following sequence of assignments let look at what happened with the list p when we called set up w p the value of p a reference to an empty list was assigned to a new local variable properties so both variables now reference the same memory location the function modifies properties and this change is also reflected in the value of p as we saw the function also assigned a new value to properties the number this did not modify the contents at that memory location but created a new local variable this behavior is just as if we had done the following sequence of assignments thus to understand python call by value parameter passing it is enough to understand how assignment works remember that you can use the id function and is operator to check your understanding of object identity after each statement variable scope function definitions create a new local scope for variables when you assign to a new variable inside the body of a function the name is only defined within that function the name is not visible outside the function or in other functions this behavior means you can choose variable names without being concerned about collisions with names used in your other function definitions when you refer to an existing name from within the body of a function the python interpreter first tries to resolve the name with respect to the names that are local to the function if nothing is found the interpreter checks if it is a global name within the module finally if that does not succeed the interpreter checks if the name is a python built in this is the so called lgb rule of name resolution local then global then built in caution a function can enable access to a global variable using the global declaration however this practice should be avoided as much as possible defining global variables inside a function introduces dependencies on context and limits the portability or reusability of the function in general you should use parameters for function inputs and return values for function outputs checking parameter types python does not allow us to declare the type of a variable when we write a program and this permits us to define functions that are flexible about the type of their arguments for emple a tagger might expect a sequence of words but it would not care whether this sequence is expressed as a list or a tuple or an iterator another sequence type that is outside the scope of the current discussion however often we want to write programs for later use by others and want to program in a defensive style providing useful warnings when functions have not been invoked correctly the author of the following tag function assumed that its argument would always be a string the function returns sensible values for the arguments the and knight but look what happens when it is passed a list it fails to complain even though the result which it returns is clearly incorrect the author of this function could take some extra steps to ensure that the word parameter of the tag function is a string a naive approach would be to check the type of the argument using if not type word is str and if word is not a string to simply return python s special empty value none this is a slight improvement because the function is checking the type of the argument and trying to return a special diagnostic value for the wrong input however it is also dangerous because the calling program may not detect that none is intended as a special value and this diagnostic return value may then be propagated to other parts of the program with unpredictable consequences this approach also fails if the word is a unicode string which has type unicode not str here s a better solution using an assert statement together with python s basestring type that generalizes over both unicode and str if the assert statement fails it will produce an error that cannot be ignored since it halts program execution additionally the error message is easy to interpret adding assertions to a program helps you find logical errors and is a kind of defensive programming a more fundamental approach is to document the parameters to each function using docstrings as described later in this section functional decomposition well structured programs usually make extensive use of functions when a block of program code grows longer than lines it is a great help to readability if the code is broken up into one or more functions each one having a clear purpose this is analogous to the way a good essay is divided into paragraphs each expressing one main idea functions provide an important kind of abstraction they allow us to group multiple actions into a single complex action and associate a name with it compare this with the way we combine the actions of go and bring back into a single more complex action fetch when we use functions the main program can be written at a higher level of abstraction making its structure transparent e g appropriate use of functions makes programs more readable and maintainable additionally it becomes possible to reimplement a function replacing the function body with more efficient code without having to be concerned with the rest of the program consider the freq words function in it updates the contents of a frequency distribution that is passed in as a parameter and it also prints a list of the n most frequent words this function has a number of problems the function has two side effects it modifies the contents of its second parameter and it prints a selection of the results it has computed the function would be easier to understand and to reuse elsewhere if we initialize the freqdist object inside the function in the same place it is populated and if we moved the selection and display of results to the calling program given that its task is to identify frequent words it should probably just return a list not the whole frequency distribution in we refactor this function and simplify its interface by dropping the freqdist parameter the readability and usability of the freq words function is improved note we have used as a variable name this is no different to any other variable except it signals to the reader that we do not have a use for the information it holds documenting functions if we have done a good job at decomposing our program into functions then it should be easy to describe the purpose of each function in plain language and provide this in the docstring at the top of the function definition this statement should not explain how the functionality is implemented in fact it should be possible to re implement the function using a different method without changing this statement for the simplest functions a one line docstring is usually adequate see you should provide a triple quoted string containing a complete sentence on a single line for non trivial functions you should still provide a one sentence summary on the first line since many docstring processing tools index this string this should be followed by a blank line then a more detailed description of the functionality see http www python org dev peps pep for more information in docstring conventions docstrings can include a doctest block illustrating the use of the function and the expected output these can be tested automatically using python s docutils module docstrings should document the type of each parameter to the function and the return type at a minimum that can be done in plain text however note that nltk uses the sphinx markup language to document parameters this format can be automatically converted into richly structured api documentation see http nltk org and includes special handling of certain fields such as param which allow the inputs and outputs of functions to be clearly documented illustrates a complete docstring doing more with functions this section discusses more advanced features which you may prefer to skip on the first time through this chapter functions as arguments so far the arguments we have passed into functions have been simple objects like strings or structured objects like lists python also lets us pass a function as an argument to another function now we can abstract out the operation and apply a different operation on the same data as the following emples show we can pass the built in function len or a user defined function last letter as arguments to another function the objects len and last letter can be passed around like lists and dictionaries notice that parentheses are only used after a function name if we are invoking the function when we are simply treating the function as an object these are omitted python provides us with one more way to define functions as arguments to other functions so called lambda expressions supposing there was no need to use the above last letter function in multiple places and thus no need to give it a name we can equivalently write the following our next emple illustrates passing a function to the sorted function when we call the latter with a single argument the list to be sorted it uses the built in comparison function cmp however we can supply our own sort function e g to sort by decreasing length accumulative functions these functions start by initializing some storage and iterate over input to build it up before returning some final object a large structure or aggregated result a standard way to do this is to initialize an empty list accumulate the material then return the list as shown in function search in the function search is a generator the first time this function is called it gets as far as the yield statement and pauses the calling program gets the first word and does any necessary processing once the calling program is ready for another word execution of the function is continued from where it stopped until the next time it encounters a yield statement this approach is typically more efficient as the function only generates the data as it is required by the calling program and does not need to allocate additional memory to store the output cf our discussion of generator expressions above here is a more sophisticated emple of a generator which produces all permutations of a list of words in order to force the permutations function to generate all its output we wrap it with a call to list note the permutations function uses a technique called recursion discussed below in the ability to generate permutations of a set of words is useful for creating data to test a grammar higher order functions python provides some higher order functions that are standard features of functional programming languages such as haskell we illustrate them here alongside the equivalent expression using list comprehensions let start by defining a function is content word which checks whether a word is from the open class of content words we use this function as the first parameter of filter which applies the function to each item in the sequence contained in its second parameter and only retains the items for which the function returns true another higher order function is map which applies a function to every item in a sequence it is a general version of the extract property function we saw in here is a simple way to find the average length of a sentence in the news section of the brown corpus followed by an equivalent version with list comprehension calculation in the above emples we specified a user defined function is content word and a built in function len we can also provide a lambda expression here is a pair of equivalent emples which count the number of vowels in each word the solutions based on list comprehensions are usually more readable than the solutions based on higher order functions and we have favored the former approach throughout this book named arguments when there are a lot of parameters it is easy to get confused about the correct order instead we can refer to parameters by name and even assign them a default value just in case one was not provided by the calling program now the parameters can be specified in any order and can be omitted these are called keyword arguments if we mix these two kinds of parameters then we must ensure that the unnamed parameters precede the named ones it has to be this way since unnamed parameters are defined by position we can define a function that takes an arbitrary number of unnamed and named parameters and access them via an in place list of arguments args and an in place dictionary of keyword arguments kwargs dictionaries will be presented in when args appears as a function parameter it actually corresponds to all the unnamed parameters of the function here is another illustration of this aspect of python syntax for the zip function which operates on a variable number of arguments we will use the variable name song to demonstrate that there is nothing special about the name args it should be clear from the above emple that typing song is just a convenient shorthand and equivalent to typing out song song song here is another emple of the use of keyword arguments in a function definition along with three equivalent ways to call the function a side effect of having named arguments is that they permit optionality thus we can leave out any arguments where we are happy with the default value freq words ch rst min freq words ch rst another common use of optional arguments is to permit a flag here is a revised version of the same function that reports its progress if a verbose flag is set caution take care not to use a mutable object as the default value of a parameter a series of calls to the function will use the same object sometimes with bizarre results as we will see in the discussion of debugging below caution if your program will work with a lot of files it is a good idea to close any open files once they are no longer required python will close open files automatically if you use the with statement program development programming is a skill that is acquired over several years of experience with a variety of programming languages and tasks key high level abilities are algorithm design and its manifestation in structured programming key low level abilities include familiarity with the syntactic constructs of the language and knowledge of a variety of diagnostic methods for trouble shooting a program which does not exhibit the expected behavior this section describes the internal structure of a program module and how to organize a multi module program then it describes various kinds of error that arise during program development what you can do to fix them and better still to avoid them in the first place structure of a python module the purpose of a program module is to bring logically related definitions and functions together in order to facilitate re use and abstraction python modules are nothing more than individual py files for emple if you were working with a particular corpus format the functions to read and write the format could be kept together constants used by both formats such as field separators or a extn inf filename extension could be shared if the format was updated you would know that only one file needed to be changed similarly a module could contain code for creating and manipulating a particular data structure such as syntax trees or code for performing a particular processing task such as plotting corpus statistics when you start writing python modules it helps to have some emples to emulate you can locate the code for any nltk module on your system using the file variable e g this returns the location of the compiled pyc file for the module and you will probably see a different location on your machine the file that you will need to open is the corresponding py source file and this will be in the same directory as the pyc file alternatively you can view the latest version of this module on the web at http code google com p nltk source browse trunk nltk nltk metrics distance py like every other nltk module distance py begins with a group of comment lines giving a one line title of the module and identifying the authors since the code is distributed it also includes the url where the code is available a copyright statement and license information next is the module level docstring a triple quoted multiline string containing information about the module that will be printed when someone types help nltk metrics distance natural language toolkit distance metrics copyright c nltk project author edward loper edloper gmail com steven bird stevenbird gmail com tom lippincott tom cs columbia edu url http nltk org for license information see license txt n distance metrics compute the distance between two items usually strings n as metrics they must satisfy the following three requirements d a a d a b d a c d a b d b c n n after this comes all the import statements required for the module then any global variables followed by a series of function definitions that make up most of the module other modules define classes the main building block of object oriented programming which falls outside the scope of this book most nltk modules also include a demo function which can be used to see emples of the module in use note some module variables and functions are only used within the module these should have names beginning with an underscore e g helper since this will hide the name if another module imports this one using the idiom from module import these names will not be imported you can optionally list the externally accessible names of a module using a special built in variable like this all edit distance jaccard distance multi module programs some programs bring together a diverse range of tasks such as loading data from a corpus performing some analysis tasks on the data then visualizing it we may already have stable modules that take care of loading data and producing visualizations our work might involve coding up the analysis task and just invoking functions from the existing modules this scenario is depicted in figure structure of a multi module program the main program my program py imports functions from two other modules unique analysis tasks are localized to the main program while common loading and visualization tasks are kept apart to facilitate re use and abstraction by dividing our work into several modules and using import statements to access functions defined elsewhere we can keep the individual modules simple and easy to maintain this approach will also result in a growing collection of modules and make it possible for us to build sophisticated systems involving a hierarchy of modules designing such systems well is a complex software engineering task and beyond the scope of this book sources of error mastery of programming depends on having a variety of problem solving skills to draw upon when the program doesn t work as expected something as trivial as a mis placed symbol might cause the program to behave very differently we call these bugs because they are tiny in comparison to the damage they can cause they creep into our code unnoticed and it s only much later when we re running the program on some new data that their presence is detected sometimes fixing one bug only reveals another and we get the distinct impression that the bug is on the move the only reassurance we have is that bugs are spontaneous and not the fault of the programmer flippancy aside debugging code is hard because there are so many ways for it to be faulty our understanding of the input data the algorithm or even the programming language may be at fault let look at emples of each of these first the input data may contain some unexpected characters for emple wordnet synset names have the form tree n with three components separated using periods the nltk wordnet module initially decomposed these names using split however this method broke when someone tried to look up the word phd which has the synset name ph d n containing four periods instead of the expected two the solution was to use rsplit to do at most two splits using the rightmost instances of the period and leaving the ph d string intact although several people had tested the module before it was released it was some weeks before someone detected the problem see http code google com p nltk issues detail id second a supplied function might not behave as expected for emple while testing nltk s interface to wordnet one of the authors noticed that no synsets had any antonyms defined even though the underlying database provided a large quantity of antonym information what looked like a bug in the wordnet interface turned out to be a misunderstanding about wordnet itself antonyms are defined for lemmas not for synsets the only bug was a misunderstanding of the interface see http code google com p nltk issues detail id third our understanding of python s semantics may be at fault it is easy to make the wrong assumption about the relative scope of two operators for emple s s d ph d n produces a run time error typeerror not enough arguments for format string this is because the percent operator has higher precedence than the comma operator the fix is to add parentheses in order to force the required scope as another emple suppose we are defining a function to collect all tokens of a text having a given length the function has parameters for the text and the word length and an extra parameter that allows the initial value of the result to be given as a parameter the first time we call find words we get all three letter words as expected the second time we specify an initial value for the result a one element list ur and as expected the result has this word along with the other two letter word in our text now the next time we call find words we use the same parameters as in but we get a different result each time we call find words with no third parameter the result will simply extend the result of the previous call rather than start with the empty result list as specified in the function definition the program behavior is not as expected because we incorrectly assumed that the default value was created at the time the function was invoked however it is created just once at the time the python interpreter loads the function this one list object is used whenever no explicit value is provided to the function debugging techniques since most code errors result from the programmer making incorrect assumptions the first thing to do when you detect a bug is to check your assumptions localize the problem by adding print statements to the program showing the value of important variables and showing how far the program has progressed if the program produced an exception a run time error the interpreter will print a stack trace pinpointing the location of program execution at the time of the error if the program depends on input data try to reduce this to the smallest size while still producing the error once you have localized the problem to a particular function or to a line of code you need to work out what is going wrong it is often helpful to recreate the situation using the interactive command line define some variables then copy paste the offending line of code into the session and see what happens check your understanding of the code by reading some documentation and emining other code samples that purport to do the same thing that you are trying to do try explaining your code to someone else in case they can see where things are going wrong python provides a debugger which allows you to monitor the execution of your program specify line numbers where execution will stop i e breakpoints and step through sections of code and inspect the value of variables you can invoke the debugger on your code as follows it will present you with a prompt pdb where you can type instructions to the debugger type help to see the full list of commands typing step or just s will execute the current line and stop if the current line calls a function it will enter the function and stop at the first line typing next or just n is similar but it stops execution at the next line in the current function the break or b command can be used to create or list breakpoints type continue or c to continue execution as far as the next breakpoint type the name of any variable to inspect its value we can use the python debugger to locate the problem in our find words function remember that the problem arose the second time the function was called we will start by calling the function without using the debugger using the smallest possible input the second time we will call it with the debugger here we typed just two commands into the debugger step took us inside the function and args showed the values of its arguments or parameters we see immediately that result has an initial value of cat and not the empty list as expected the debugger has helped us to localize the problem prompting us to check our understanding of python functions defensive programming in order to avoid some of the pain of debugging it helps to adopt some defensive programming habits instead of writing a line program then testing it build the program bottom up out of small pieces that are known to work each time you combine these pieces to make a larger unit test it carefully to see that it works as expected consider adding assert statements to your code specifying properties of a variable e g assert isinstance text list if the value of the text variable later becomes a string when your code is used in some larger context this will raise an assertionerror and you will get immediate notification of the problem once you think you have found the bug view your solution as a hypothesis try to predict the effect of your bugfix before re running the program if the bug is not fixed do not fall into the trap of blindly changing the code in the hope that it will magically start working again instead for each change try to articulate a hypothesis about what is wrong and why the change will fix the problem then undo the change if the problem was not resolved as you develop your program extend its functionality and fix any bugs it helps to maintain a suite of test cases this is called regression testing since it is meant to detect situations where the code regresses where a change to the code has an unintended side effect of breaking something that used to work python provides a simple regression testing framework in the form of the doctest module this module searches a file of code or documentation for blocks of text that look like an interactive python session of the form you have already seen many times in this book it executes the python commands it finds and tests that their output matches the output supplied in the original file whenever there is a mismatch it reports the expected and actual values for details please consult the doctest documentation at http docs python org library doctest html apart from its value for regression testing the doctest module is useful for ensuring that your software documentation stays in sync with your code perhaps the most important defensive programming strategy is to set out your code clearly choose meaningful variable and function names and simplify the code wherever possible by decomposing it into functions and modules with well documented interfaces algorithm design this section discusses more advanced concepts which you may prefer to skip on the first time through this chapter a major part of algorithmic problem solving is selecting or adapting an appropriate algorithm for the problem at hand sometimes there are several alternatives and choosing the best one depends on knowledge about how each alternative performs as the size of the data grows whole books are written on this topic and we only have space to introduce some key concepts and elaborate on the approaches that are most prevalent in natural language processing the best known strategy is known as divide and conquer we attack a problem of size n by dividing it into two problems of size n solve these problems and combine their results into a solution of the original problem for emple suppose that we had a pile of cards with a single word written on each card we could sort this pile by splitting it in half and giving it to two other people to sort they could do the same in turn then when two sorted piles come back it is an easy task to merge them into a single sorted pile see for an illustration of this process figure sorting by divide and conquer to sort an array we split it in half and sort each half recursively we merge each sorted half back into a whole list again recursively this algorithm is known as merge sort another emple is the process of looking up a word in a dictionary we open the book somewhere around the middle and compare our word with the current page if it is earlier in the dictionary we repeat the process on the first half if its later we use the second half this search method is called binary search since it splits the problem in half at every step in another approach to algorithm design we attack a problem by transforming it into an instance of a problem we already know how to solve for emple in order to detect duplicate entries in a list we can pre sort the list then scan through it once to check if any adjacent pairs of elements are identical recursion the above emples of sorting and searching have a striking property to solve a problem of size n we have to break it in half and then work on one or more problems of size n a common way to implement such methods uses recursion we define a function f which simplifies the problem and calls itself to solve one or more easier instances of the same problem it then combines the results into a solution for the original problem for emple suppose we have a set of n words and want to calculate how many different ways they can be combined to make a sequence of words if we have only one word n there is just one way to make it into a sequence if we have a set of two words there are two ways to put them into a sequence for three words there are six possibilities in general for n words there are n n ways i e the factorial of n we can code this up as follows however there is also a recursive algorithm for solving this problem based on the following observation suppose we have a way to construct all orderings for n distinct words then for each such ordering there are n places where we can insert a new word at the start the end or any of the n boundaries between the words thus we simply multiply the number of solutions found for n by the value of n we also need the base case to say that if we have a single word there is just one ordering we can code this up as follows these two algorithms solve the same problem one uses iteration while the other uses recursion we can use recursion to navigate a deeply nested object such as the wordnet hypernym hierarchy let count the size of the hypernym hierarchy rooted at a given synset s we will do this by finding the size of each hyponym of s then adding these together we will also add for the synset itself the following function size does this work notice that the body of the function includes a recursive call to size we can also design an iterative solution to this problem which processes the hierarchy in layers the first layer is the synset itself then all the hyponyms of the synset then all the hyponyms of the hyponyms each time through the loop it computes the next layer by finding the hyponyms of everything in the last layer it also maintains a total of the number of synsets encountered so far not only is the iterative solution much longer it is harder to interpret it forces us to think procedurally and keep track of what is happening with the layer and total variables through time let satisfy ourselves that both solutions give the same result we will use another form of the import statement allowing us to abbreviate the name wordnet to wn as a final emple of recursion let use it to construct a deeply nested object a letter trie is a data structure that can be used for indexing a lexicon one letter at a time the name is based on the word retrieval for emple if trie contained a letter trie then trie c would be a smaller trie which held all words starting with c demonstrates the recursive process of building a trie using python dictionaries to insert the word chien french for dog we split off the c and recursively insert hien into the sub trie trie c the recursion continues until there are no letters remaining in the word when we store the intended value in this case the word dog caution despite the simplicity of recursive programming it comes with a cost each time a function is called some state information needs to be pushed on a stack so that once the function has completed execution can continue from where it left off for this reason iterative solutions are often more efficient than recursive solutions space time tradeoffs we can sometimes significantly speed up the execution of a program by building an auxiliary data structure such as an index the listing in implements a simple text retrieval system for the movie reviews corpus by indexing the document collection it provides much faster lookup a more subtle emple of a space time tradeoff involves replacing the tokens of a corpus with integer identifiers we create a vocabulary for the corpus a list in which each word is stored once then invert this list so that we can look up any word to find its identifier each document is preprocessed so that a list of words becomes a list of integers any language models can now work with integers see the listing in for an emple of how to do this for a tagged corpus another emple of a space time tradeoff is maintaining a vocabulary list if you need to process an input text to check that all words are in an existing vocabulary the vocabulary should be stored as a set not a list the elements of a set are automatically indexed so testing membership of a large set will be much faster than testing membership of the corresponding list we can test this claim using the timeit module the timer class has two parameters a statement which is executed multiple times and setup code that is executed once at the beginning we will simulate a vocabulary of items using a list or set of integers the test statement will generate a random item which has a chance of being in the vocabulary performing list membership tests takes a total of seconds while the equivalent tests on a set take a mere seconds or three orders of magnitude faster dynamic programming dynamic programming is a general technique for designing algorithms which is widely used in natural language processing the term programming is used in a different sense to what you might expect to mean planning or scheduling dynamic programming is used when a problem contains overlapping sub problems instead of computing solutions to these sub problems repeatedly we simply store them in a lookup table in the remainder of this section we will introduce dynamic programming but in a rather different context to syntactic parsing pingala was an indian author who lived around the th century b c and wrote a treatise on sanskrit prosody called the chandas shastra virahanka extended this work around the th century a d studying the number of ways of combining short and long syllables to create a meter of length n short syllables marked s take up one unit of length while long syllables marked l take two pingala found for emple that there are five ways to construct a meter of length v ll ssl sls lss ssss observe that we can split v into two subsets those starting with l and those starting with s as shown in with this observation we can write a little recursive function called virahanka to compute these meters shown in notice that in order to compute v we first compute v and v but to compute v we need to first compute v and v this call structure is depicted in as you can see v is computed twice this might not seem like a significant problem but it turns out to be rather wasteful as n gets large to compute v using this recursive technique we would compute v times and for v we would compute v times a much better alternative is to store the value of v in a table and look it up whenever we need it the same goes for other values such as v and so on function virahanka implements a dynamic programming approach to the problem it works by filling up a table called lookup with solutions to all smaller instances of the problem stopping as soon as we reach the value we are interested in at this point we read off the value and return it crucially each sub problem is only ever solved once notice that the approach taken in virahanka is to solve smaller problems on the way to solving larger problems accordingly this is known as the bottom up approach to dynamic programming unfortunately it turns out to be quite wasteful for some applications since it may compute solutions to sub problems that are never required for solving the main problem this wasted computation can be avoided using the top down approach to dynamic programming which is illustrated in the function virahanka in unlike the bottom up approach this approach is recursive it avoids the huge wastage of virahanka by checking whether it has previously stored the result if not it computes the result recursively and stores it in the table the last step is to return the stored result the final method in virahanka is to use a python decorator called memoize which takes care of the housekeeping work done by virahanka without cluttering up the program this memoization process stores the result of each previous call to the function along with the parameters that were used if the function is subsequently called with the same parameters it returns the stored result instead of recalculating it this aspect of python syntax is beyond the scope of this book this concludes our brief introduction to dynamic programming we will encounter it again in a sample of python libraries python has hundreds of third party libraries specialized software packages that extend the functionality of python nltk is one such library to realize the full power of python programming you should become familiar with several other libraries most of these will need to be manually installed on your computer matplotlib python has some libraries that are useful for visualizing language data the matplotlib package supports sophisticated plotting functions with a matlab style interface and is available from http matplotlib sourceforge net so far we have focused on textual presentation and the use of formatted print statements to get output lined up in columns it is often very useful to display numerical data in graphical form since this often makes it easier to detect patterns for emple in we saw a table of numbers showing the frequency of particular modal verbs in the brown corpus classified by genre the program in presents the same information in graphical format the output is shown in a color figure in the graphical display figure bar chart showing frequency of modals in different sections of brown corpus this visualization was produced by the program in from the bar chart it is immediately obvious that may and must have almost identical relative frequencies the same goes for could and might it is also possible to generate such data visualizations on the fly for emple a web page with form input could permit visitors to specify search parameters submit the form and see a dynamically generated visualization to do this we have to specify the agg backend for matplotlib which is a library for producing raster pixel images next we use all the same matplotlib methods as before but instead of displaying the result on a graphical terminal using pyplot show we save it to a file using pyplot savefig we specify the filename then print html markup that directs the web browser to load the file networkx the networkx package is for defining and manipulating structures consisting of nodes and edges known as graphs it is available from https networkx lanl gov networkx can be used in conjunction with matplotlib to visualize networks such as wordnet the semantic network we introduced in the program in initializes an empty graph then traverses the wordnet hypernym hierarchy adding edges to the graph notice that the traversal is recursive applying the programming technique discussed in the resulting display is shown in figure visualization with networkx and matplotlib part of the wordnet hypernym hierarchy is displayed starting with dog n the darkest node in the middle node size is based on the number of children of the node and color is based on the distance of the node from dog n this visualization was produced by the program in csv language analysis work often involves data tabulations containing information about lexical items or the participants in an empirical study or the linguistic features extracted from a corpus here is a fragment of a simple lexicon in csv format sleep sli p v i a condition of body and mind walk wo k v intr progress by lifting and setting down each foot wake weik intrans cease to sleep we can use python csv library to read and write files stored in this format for emple we can open a csv file called lexicon csv and iterate over its rows each row is just a list of strings if any fields contain numerical data they will appear as strings and will have to be converted using int or float numpy the numpy package provides substantial support for numerical processing in python numpy has a multi dimensional array object which is easy to initialize and access numpy includes linear algebra functions here we perform singular value decomposition on a matrix an operation used in latent semantic analysis to help identify implicit concepts in a document collection nltk clustering package nltk cluster makes extensive use of numpy arrays and includes support for k means clustering gaussian em clustering group average agglomerative clustering and dendrogram plots for details type help nltk cluster other python libraries there are many other python libraries and you can search for them with the help of the python package index http pypi python org many libraries provide an interface to external software such as relational databases e g mysql python and large document collections e g pylucene many other libraries give access to file formats such as pdf msword and xml pypdf pywin xml etree rss feeds e g feedparser and electronic mail e g imaplib email summary python assignment and parameter passing use object references e g if a is a list and we assign b a then any operation on a will modify b and vice versa the is operation tests if two objects are identical internal objects while tests if two objects are equivalent this distinction parallels the type token distinction strings lists and tuples are different kinds of sequence object supporting common operations such as indexing slicing len sorted and membership testing using in a declarative programming style usually produces more compact readable code manually incremented loop variables are usually unnecessary when a sequence must be enumerated use enumerate functions are an essential programming abstraction key concepts to understand are parameter passing variable scope and docstrings a function serves as a namespace names defined inside a function are not visible outside that function unless those names are declared to be global modules permit logically related material to be localized in a file a module serves as a namespace names defined in a module such as variables and functions are not visible to other modules unless those names are imported dynamic programming is an algorithm design technique used widely in nlp that stores the results of previous computations in order to avoid unnecessary recomputation further reading this chapter has touched on many topics in programming some specific to python and some quite general we have just scratched the surface and you may want to read more about these topics starting with the further materials for this chapter available at http nltk org the python website provides extensive documentation it is important to understand the built in functions and standard types described at http docs python org library functions html and http docs python org library stdtypes html we have learnt about generators and their importance for efficiency for information about iterators a closely related topic see http docs python org library itertools html consult your favorite python book for more information on such topics an excellent resource for using python for multimedia processing including working with sound files is guzdial when using the online python documentation be aware that your installed version might be different from the version of the documentation you are reading you can easily check what version you have with import sys sys version version specific documentation is available at http www python org doc versions algorithm design is a rich field within computer science some good starting points are harel levitin knuth useful guidance on the practice of software development is provided in hunt thomas and mcconnell exercises find out more about sequence objects using python help facility in the interpreter type help str help list and help tuple this will give you a full list of the functions supported by each type some functions have special names flanked with underscore as the help documentation shows each such function corresponds to something more familiar for emple x getitem y is just a long winded way of saying x y identify three operations that can be performed on both tuples and lists identify three list operations that cannot be performed on tuples name a context where using a list instead of a tuple generates a python error find out how to create a tuple consisting of a single item there are at least two ways to do this create a list words is nlp fun use a series of assignment statements e g words words and a temporary variable tmp to transform this list into the list nlp is fun now do the same transformation using tuple assignment read about the built in comparison function cmp by typing help cmp how does it differ in behavior from the comparison operators does the method for creating a sliding window of n grams behave correctly for the two limiting cases n and n len sent we pointed out that when empty strings and empty lists occur in the condition part of an if clause they evaluate to false in this case they are said to be occurring in a boolean context experiment with different kind of non boolean expressions in boolean contexts and see whether they evaluate as true or false use the inequality operators to compare strings e g monty python what happens when you do z a try pairs of strings which have a common prefix e g monty montague read up on lexicographical sort in order to understand what is going on here try comparing structured objects e g monty monty does this behave as expected write code that removes whitespace at the beginning and end of a string and normalizes whitespace between words to be a single space character do this task using split and join do this task using regular expression substitutions write a program to sort words by length define a helper function cmp len which uses the cmp comparison function on word lengths create a list of words and store it in a variable sent now assign sent sent modify one of the items in sent and verify that sent has changed now try the same exercise but instead assign sent sent modify sent again and see what happens to sent explain now define text to be a list of lists of strings e g to represent a text consisting of multiple sentences now assign text text assign a new value to one of the words e g text monty check what this did to text explain load python deepcopy function i e from copy import deepcopy consult its documentation and test that it makes a fresh copy of any object initialize an n by m list of lists of empty strings using list multiplication e g word table n m what happens when you set one of its values e g word table hello explain why this happens now write an expression using range to construct a list of lists and show that it does not have this problem write code to initialize a two dimensional array of sets called word vowels and process a list of words adding each word to word vowels l v where l is the length of the word and v is the number of vowels it contains write a function novel text that prints any word that appeared in the last of a text that had not been encountered earlier write a program that takes a sentence expressed as a single string splits it and counts up the words get it to print out each word and the word frequency one per line in alphabetical order read up on gematria a method for assigning numbers to words and for mapping between words having the same number to discover the hidden meaning of texts http en wikipedia org wiki gematria http essenes net gemcal htm write a function gematria that sums the numerical values of the letters of a word according to the letter values in letter vals process a corpus e g nltk corpus state union and for each document count how many of its words have the number write a function decode to process a text randomly replacing words with their gematria equivalents in order to discover the hidden meaning of the text write a function shorten text n to process a text omitting the n most frequently occurring words of the text how readable is it write code to print out an index for a lexicon allowing someone to look up words according to their meanings or pronunciations whatever properties are contained in lexical entries write a list comprehension that sorts a list of wordnet synsets for proximity to a given synset for emple given the synsets minke whale n orca n novel n and tortoise n sort them according to their shortest path distance from right whale n write a function that takes a list of words containing duplicates and returns a list of words with no duplicates sorted by decreasing frequency e g if the input list contained instances of the word table and instances of the word chair then table would appear before chair in the output list write a function that takes a text and a vocabulary as its arguments and returns the set of words that appear in the text but not in the vocabulary both arguments can be represented as lists of strings can you do this in a single line using set difference import the itemgetter function from the operator module in python standard library i e from operator import itemgetter create a list words containing several words now try calling sorted words key itemgetter and sorted words key itemgetter explain what itemgetter is doing write a recursive function lookup trie key that looks up a key in a trie and returns the value it finds extend the function to return a word when it is uniquely determined by its prefix e g vanguard is the only word that starts with vang so lookup trie vang should return the same thing as lookup trie vanguard read up on keyword linkage chapter of scott tribble extract keywords from nltk s shakespeare corpus and using the networkx package plot keyword linkage networks read about string edit distance and the levenshtein algorithm try the implementation provided in nltk edit distance in what way is this using dynamic programming does it use the bottom up or top down approach see also http norvig com spell correct html the catalan numbers arise in many applications of combinatorial mathematics including the counting of parse trees the series can be defined as follows c and cn n cicn i write a recursive function to compute nth catalan number cn now write another function that does this computation using dynamic programming use the timeit module to compare the performance of these functions as n increases reproduce some of the results of zhao zobel concerning authorship identification study gender specific lexical choice and see if you can reproduce some of the results of http www clintoneast com articles words php write a recursive function that pretty prints a trie in alphabetically sorted order e g chair flesh t cat ic stylish en dog with the help of the trie data structure write a recursive function that processes text locating the uniqueness point in each word and discarding the remainder of each word how much compression does this give how readable is the resulting text obtain some raw text in the form of a single long string use python textwrap module to break it up into multiple lines now write code to add extra spaces between words in order to justify the output each line must have the same width and spaces must be approximately evenly distributed across each line no line can begin or end with a space develop a simple extractive summarization tool that prints the sentences of a document which contain the highest total word frequency use freqdist to count word frequencies and use sum to sum the frequencies of the words in each sentence rank the sentences according to their score finally print the n highest scoring sentences in document order carefully review the design of your program especially your approach to this double sorting make sure the program is written as clearly as possible read the following article on semantic orientation of adjectives use the networkx package to visualize a network of adjectives with edges to indicate same vs different semantic orientation http www aclweb org anthology p design an algorithm to find the statistically improbable phrases of a document collection http www amazon com gp search inside sipshelp html write a program to implement a brute force algorithm for discovering word squares a kind of n n crossword in which the entry in the nth row is the same as the entry in the nth column for discussion see http itre cis upenn edu myl languagelog archives html categorizing and tagging words back in elementary school you learnt the difference between nouns verbs adjectives and adverbs these word classes are not just the idle invention of grammarians but are useful categories for many language processing tasks as we will see they arise from simple analysis of the distribution of words in text the goal of this chapter is to answer the following questions what are lexical categories and how are they used in natural language processing what is a good python data structure for storing words and their categories how can we automatically tag each word of a text with its word class along the way we will cover some fundamental techniques in nlp including sequence labeling n gram models backoff and evaluation these techniques are useful in many areas and tagging gives us a simple context in which to present them we will also see how tagging is the second step in the typical nlp pipeline following tokenization the process of classifying words into their parts of speech and labeling them accordingly is known as part of speech tagging pos tagging or simply tagging parts of speech are also known as word classes or lexical categories the collection of tags used for a particular task is known as a tagset our emphasis in this chapter is on exploiting tags and tagging text automatically using a tagger a part of speech tagger or pos tagger processes a sequence of words and attaches a part of speech tag to each word do not forget to import nltk here we see that and is cc a coordinating conjunction now and completely are rb or adverbs for is in a preposition something is nn a noun and different is jj an adjective note nltk provides documentation for each tag which can be queried using the tag e g nltk help upenn tagset rb or a regular expression e g nltk help upenn tagset nn some corpora have readme files with tagset documentation see nltk corpus readme substituting in the name of the corpus let look at another emple this time including some homonyms notice that refuse and permit both appear as a present tense verb vbp and a noun nn e g refuse is a verb meaning deny while refuse is a noun meaning trash i e they are not homophones thus we need to know which word is being used in order to pronounce the text correctly for this reason text to speech systems usually perform pos tagging note your turn many words like ski and race can be used as nouns or verbs with no difference in pronunciation can you think of others hint think of a commonplace object and try to put the word to before it to see if it can also be a verb or think of an action and try to put the before it to see if it can also be a noun now make up a sentence with both uses of this word and run the pos tagger on this sentence lexical categories like noun and part of speech tags like nn seem to have their uses but the details will be obscure to many readers you might wonder what justification there is for introducing this extra level of information many of these categories arise from superficial analysis the distribution of words in text consider the following analysis involving woman a noun bought a verb over a preposition and the a determiner the text similar method takes a word w finds all contexts w w w then finds all words w that appear in the same context i e w w w observe that searching for woman finds nouns searching for bought mostly finds verbs searching for over generally finds prepositions searching for the finds several determiners a tagger can correctly identify the tags on these words in the context of a sentence e g the woman bought over worth of clothes a tagger can also model our knowledge of unknown words e g we can guess that scrobbling is probably a verb with the root scrobble and likely to occur in contexts like he was scrobbling tagged corpora representing tagged tokens by convention in nltk a tagged token is represented using a tuple consisting of the token and the tag we can create one of these special tuples from the standard string representation of a tagged token using the function str tuple we can construct a list of tagged tokens directly from a string the first step is to tokenize the string to access the individual word tag strings and then to convert each of these into a tuple using str tuple reading tagged corpora several of the corpora included with nltk have been tagged for their part of speech here is an emple of what you might see if you opened a file from the brown corpus with a text editor the at fulton np tl county nn tl grand jj tl jury nn tl said vbd friday nr an at investigation nn of in atlanta np recent jj primary nn election nn produced vbd no at evidence nn that cs any dti irregularities nns took vbd place nn other corpora use a variety of formats for storing part of speech tags nltk corpus readers provide a uniform interface so that you do not have to be concerned with the different file formats in contrast with the file fragment shown above the corpus reader for the brown corpus represents the data as shown below note that part of speech tags have been converted to uppercase since this has become standard practice since the brown corpus was published whenever a corpus contains tagged text the nltk corpus interface will have a tagged words method here are some more emples again using the output format illustrated for the brown corpus not all corpora employ the same set of tags see the tagset help functionality and the readme methods mentioned above for documentation initially we want to avoid the complications of these tagsets so we use a built in mapping to the universal tagset tagged corpora for several other languages are distributed with nltk including chinese hindi portuguese spanish dutch and catalan these usually contain non ascii text and python always displays this in hedecimal when printing a larger structure such as a list if your environment is set up correctly with appropriate editors and fonts you should be able to display individual strings in a human readable way for emple shows data accessed using nltk corpus indian figure pos tagged data from four indian languages bangla hindi marathi and telugu if the corpus is also segmented into sentences it will have a tagged sents method that divides up the tagged words into sentences rather than presenting them as one big list this will be useful when we come to developing automatic taggers as they are trained and tested on lists of sentences not words a universal part of speech tagset tagged corpora use many different conventions for tagging words to help us get started we will be looking at a simplified tagset shown in table universal part of speech tagset let see which of these tags are the most common in the news category of the brown corpus note your turn plot the above frequency distribution using tag fd plot cumulative true what percentage of words are tagged using the first five tags of the above list we can use these tags to do powerful searches using a graphical pos concordance tool nltk app concordance use it to search for any combination of words and pos tags e g n n n n hit vd hit vn or the adj man nouns nouns generally refer to people places things or concepts e g woman scotland book intelligence nouns can appear after determiners and adjectives and can be the subject or object of the verb as shown in table syntactic patterns involving some nouns the simplified noun tags are n for common nouns like book and np for proper nouns like scotland let inspect some tagged text to see what parts of speech occur before a noun with the most frequent ones first to begin with we construct a list of bigrams whose members are themselves word tag pairs such as the det fulton np and fulton np county n then we construct a freqdist from the tag parts of the bigrams this confirms our assertion that nouns occur after determiners and adjectives including numeral adjectives tagged as num verbs verbs are words that describe events and actions e g fall eat in in the context of a sentence verbs typically express a relation involving the referents of one or more noun phrases table syntactic patterns involving some verbs what are the most common verbs in news text let sort all the verbs by frequency note that the items being counted in the frequency distribution are word tag pairs since words and tags are paired we can treat the word as a condition and the tag as an event and initialize a conditional frequency distribution with a list of condition event pairs this lets us see a frequency ordered list of tags given a word we can reverse the order of the pairs so that the tags are the conditions and the words are the events now we can see likely words for a given tag we will do this for the wsj tagset rather than the universal tagset to clarify the distinction between vbd past tense and vbn past participle let find words which can be both vbd and vbn and see some surrounding text in this case we see that the past participle of kicked is preceded by a form of the auxiliary verb have is this generally true note your turn given the list of past participles produced by list cfd vn try to collect a list of all the word tag pairs that immediately precede items in that list adjectives and adverbs two other important word classes are adjectives and adverbs adjectives describe nouns and can be used as modifiers e g large in the large pizza or in predicates e g the pizza is large english adjectives can have internal structure e g fall ing in the falling stocks adverbs modify verbs to specify the time manner place or direction of the event described by the verb e g quickly in the stocks fell quickly adverbs may also modify adjectives e g really in mary teacher was really nice english has several categories of closed class words in addition to prepositions such as articles also often called determiners e g the a modals e g should may and personal pronouns e g she they each dictionary and grammar classifies these words differently note your turn if you are uncertain about some of these parts of speech study them using nltk app concordance or watch some of the schoolhouse rock grammar videos available at youtube or consult the further reading section at the end of this chapter unsimplified tags let find the most frequent nouns of each noun part of speech type the program in finds all tags starting with nn and provides a few emple words for each one you will see that there are many variants of nn the most important contain for possessive nouns s for plural nouns since plural nouns typically end in s and p for proper nouns in addition most of the tags have suffix modifiers nc for citations hl for words in headlines and tl for titles a feature of brown tags when we come to constructing part of speech taggers later in this chapter we will use the unsimplified tags exploring tagged corpora let briefly return to the kinds of exploration of corpora we saw in previous chapters this time exploiting pos tags suppose we are studying the word often and want to see how it is used in text we could ask to see the words that follow often however it is probably more instructive to use the tagged words method to look at the part of speech tag of the following words notice that the most high frequency parts of speech following often are verbs nouns never appear in this position in this particular corpus next let s look at some larger context and find words involving particular sequences of tags and words in this case verb to verb in code three word phrase we consider each three word window in the sentence and check if they meet our criterion if the tags match we print the corresponding words finally let look for words that are highly ambiguous as to their part of speech tag understanding why such words are tagged as they are in each context can help us clarify the distinctions between the tags note your turn open the pos concordance tool nltk app concordance and load the complete brown corpus simplified tagset now pick some of the above words and see how the tag of the word correlates with the context of the word e g search for near to see all forms mixed together near adj to see it used as an adjective near n to see just those cases where a noun follows and so forth for a larger set of emples modify the supplied code so that it lists words having three distinct tags mapping words to properties using python dictionaries as we have seen a tagged word of the form word tag is an association between a word and a part of speech tag once we start doing part of speech tagging we will be creating programs that assign a tag to a word the tag which is most likely in a given context we can think of this process as mapping from words to tags the most natural way to store mappings in python uses the so called dictionary data type also known as an associative array or hash array in other programming languages in this section we look at dictionaries and see how they can represent a variety of language information including parts of speech indexing lists vs dictionaries a text as we have seen is treated in python as a list of words an important property of lists is that we can look up a particular item by giving its index e g text notice how we specify a number and get back a word we can think of a list as a simple kind of table as shown in figure list look up we access the contents of a python list with the help of an integer index contrast this situation with frequency distributions where we specify a word and get back a number e g fdist monstrous which tells us the number of times a given word has occurred in a text look up using words is familiar to anyone who has used a dictionary some more emples are shown in figure dictionary look up we access the entry of a dictionary using a key such as someone name a web domain or an english word other names for dictionary are map hashmap hash and associative array in the case of a phonebook we look up an entry using a name and get back a number when we type a domain name in a web browser the computer looks this up to get back an ip address a word frequency table allows us to look up a word and find its frequency in a text collection in all these cases we are mapping from names to numbers rather than the other way around as with a list in general we would like to be able to map between arbitrary types of information lists a variety of linguistic objects along with what they map table linguistic objects as mappings from keys to values most often we are mapping from a word to some structured object for emple a document index maps from a word which we can represent as a string to a list of pages represented as a list of integers in this section we will see how to represent such mappings in python dictionaries in python python provides a dictionary data type that can be used for mapping between arbitrary types it is like a conventional dictionary in that it gives you an efficient way to look things up however as we see from it has a much wider range of uses to illustrate we define pos to be an empty dictionary and then add four entries to it specifying the part of speech of some words we add entries to a dictionary using the familiar square bracket notation so for emple says that the part of speech of colorless is adjective or more specifically that the key colorless is assigned the value adj in dictionary pos when we inspect the value of pos we see a set of key value pairs once we have populated the dictionary in this way we can employ the keys to retrieve values of course we might accidentally use a key that has not been assigned a value this raises an important question unlike lists and strings where we can use len to work out which integers will be legal indexes how do we work out the legal keys for a dictionary if the dictionary is not too big we can simply inspect its contents by evaluating the variable pos as we saw above line this gives us the key value pairs notice that they are not in the same order they were originally entered this is because dictionaries are not sequences but mappings cf and the keys are not inherently ordered alternatively to just find the keys we can convert the dictionary to a list or use the dictionary in a context where a list is expected as the parameter of sorted or in a for loop note when you type list pos you might see a different order to the one shown above if you want to see the keys in order just sort them as well as iterating over all keys in the dictionary with a for loop we can use the for loop as we did for printing lists finally the dictionary methods keys values and items allow us to access the keys values and key value pairs as separate lists we can even sort tuples which orders them according to their first element and if the first elements are the same it uses their second elements we want to be sure that when we look something up in a dictionary we only get one value for each key now suppose we try to use a dictionary to store the fact that the word sleep can be used as both a verb and a noun initially pos sleep is given the value v but this is immediately overwritten with the new value n in other words there can only be one entry in the dictionary for sleep however there is a way of storing multiple values in that entry we use a list value e g pos sleep n v in fact this is what we saw in for the cmu pronouncing dictionary which stores multiple pronunciations for a single word defining dictionaries we can use the same key value pair format to create a dictionary there is a couple of ways to do this and we will normally use the first note that dictionary keys must be immutable types such as strings and tuples if we try to define a dictionary using a mutable key we get a typeerror default dictionaries if we try to access a key that is not in a dictionary we get an error however its often useful if a dictionary can automatically create an entry for this new key and give it a default value such as zero or the empty list for this reason a special kind of dictionary called a defaultdict is available in order to use it we have to supply a parameter which can be used to create the default value e g int float str list dict tuple note these default values are actually functions that convert other objects to the specified type e g int list when they are called with no parameter int list they return and respectively the above emples specified the default value of a dictionary entry to be the default value of a particular data type however we can specify any default value we like simply by providing the name of a function that can be called with no arguments to create the required value let return to our part of speech emple and create a dictionary whose default value for any entry is n when we access a non existent entry it is automatically added to the dictionary note the above emple used a lambda expression introduced in this lambda expression specifies no parameters so we call it using parentheses with no arguments thus the definitions of f and g below are equivalent let s see how default dictionaries could be used in a more substantial language processing task many language processing tasks including tagging struggle to correctly process the hapaxes of a text they can perform better with a fixed vocabulary and a guarantee that no new words will appear we can preprocess a text to replace low frequency words with a special out of vocabulary token unk with the help of a default dictionary can you work out how to do this without reading on we need to create a default dictionary that maps each word to its replacement the most frequent n words will be mapped to themselves everything else will be mapped to unk incrementally updating a dictionary we can employ dictionaries to count occurrences emulating the method for tallying words shown in fig tally we begin by initializing an empty defaultdict then process each part of speech tag in the text if the tag has not been seen before it will have a zero count by default each time we encounter a tag we increment its count using the operator the listing in illustrates an important idiom for sorting a dictionary by its values to show words in decreasing order of frequency the first parameter of sorted is the items to sort a list of tuples consisting of a pos tag and a frequency the second parameter specifies the sort key using a function itemgetter in general itemgetter n returns a function that can be called on some other sequence object to obtain the nth element e g the last parameter of sorted specifies that the items should be returned in reverse order i e decreasing values of frequency there is a second useful programming idiom at the beginning of where we initialize a defaultdict and then use a for loop to update its values here is a schematic version my dictionary defaultdict function to create default value for item in sequence my dictionary item key is updated with information about item here is another instance of this pattern where we index words according to their last two letters the following emple uses the same pattern to create an anagram dictionary you might experiment with the third line to get an idea of why this program works since accumulating words like this is such a common task nltk provides a more convenient way of creating a defaultdict list in the form of nltk index note nltk index is a defaultdict list with extra support for initialization similarly nltk freqdist is essentially a defaultdict int with extra support for initialization along with sorting and plotting methods complex keys and values we can use default dictionaries with complex keys and values let study the range of possible tags for a word given the word itself and the tag of the previous word we will see how this information can be used by a pos tagger this emple uses a dictionary whose default value for an entry is a dictionary whose default value is int i e zero notice how we iterated over the bigrams of the tagged corpus processing a pair of word tag pairs for each iteration each time through the loop we updated our pos dictionary entry for t w a tag and its following word when we look up an item in pos we must specify a compound key and we get back a dictionary object a pos tagger could use such information to decide that the word right when preceded by a determiner should be tagged as adj inverting a dictionary dictionaries support efficient lookup so long as you want to get the value for any key if d is a dictionary and k is a key we type d k and immediately obtain the value finding a key given a value is slower and more cumbersome if we expect to do this kind of reverse lookup often it helps to construct a dictionary that maps values to keys in the case that no two keys have the same value this is an easy thing to do we just get all the key value pairs in the dictionary and create a new dictionary of value key pairs the next emple also illustrates another way of initializing a dictionary pos with key value pairs let first make our part of speech dictionary a bit more realistic and add some more words to pos using the dictionary update method to create the situation where multiple keys have the same value then the technique just shown for reverse lookup will no longer work why not instead we have to use append to accumulate the words for each part of speech as follows now we have inverted the pos dictionary and can look up any part of speech and find all words having that part of speech we can do the same thing even more simply using nltk support for indexing as follows a summary of python dictionary methods is given in table python dictionary methods a summary of commonly used methods and idioms involving dictionaries automatic tagging in the rest of this chapter we will explore various ways to automatically add part of speech tags to text we will see that the tag of a word depends on the word and its context within a sentence for this reason we will be working with data at the level of tagged sentences rather than words we will begin by loading the data we will be using the default tagger the simplest possible tagger assigns the same tag to each token this may seem to be a rather banal step but it establishes an important baseline for tagger performance in order to get the best result we tag each word with the most likely tag let find out which tag is most likely now using the unsimplified tagset now we can create a tagger that tags everything as nn unsurprisingly this method performs rather poorly on a typical corpus it will tag only about an eighth of the tokens correctly as we see below default taggers assign their tag to every single word even words that have never been encountered before as it happens once we have processed several thousand words of english text most new words will be nouns as we will see this means that default taggers can help to improve the robustness of a language processing system we will return to them shortly the regular expression tagger the regular expression tagger assigns tags to tokens on the basis of matching patterns for instance we might guess that any word ending in ed is the past participle of a verb and any word ending with s is a possessive noun we can express these as a list of regular expressions note that these are processed in order and the first one that matches is applied now we can set up a tagger and use it to tag a sentence now its right about a fifth of the time the final regular expression is a catch all that tags everything as a noun this is equivalent to the default tagger only much less efficient instead of re specifying this as part of the regular expression tagger is there a way to combine this tagger with the default tagger we will see how to do this shortly note your turn see if you can come up with patterns to improve the performance of the above regular expression tagger note that describes a way to partially automate such work the lookup tagger a lot of high frequency words do not have the nn tag let s find the hundred most frequent words and store their most likely tag we can then use this information as the model for a lookup tagger an nltk unigramtagger it should come as no surprise by now that simply knowing the tags for the most frequent words enables us to tag a large fraction of tokens correctly nearly half in fact let see what it does on some untagged input text many words have been assigned a tag of none because they were not among the most frequent words in these cases we would like to assign the default tag of nn in other words we want to use the lookup table first and if it is unable to assign a tag then use the default tagger a process known as backoff we do this by specifying one tagger as a parameter to the other as shown below now the lookup tagger will only store word tag pairs for words other than nouns and whenever it cannot assign a tag to a word it will invoke the default tagger let put all this together and write a program to create and evaluate lookup taggers having a range of sizes in figure lookup tagger observe that performance initially increases rapidly as the model size grows eventually reaching a plateau when large increases in model size yield little improvement in performance this emple used the pylab plotting package discussed in evaluation in the above emples you will have noticed an emphasis on accuracy scores in fact evaluating the performance of such tools is a central theme in nlp recall the processing pipeline in fig sds any errors in the output of one module are greatly multiplied in the downstream modules we evaluate the performance of a tagger relative to the tags a human expert would assign since we do not usually have access to an expert and impartial human judge we make do instead with gold standard test data this is a corpus which has been manually annotated and which is accepted as a standard against which the guesses of an automatic system are assessed the tagger is regarded as being correct if the tag it guesses for a given word is the same as the gold standard tag of course the humans who designed and carried out the original gold standard annotation were only human further analysis might show mistakes in the gold standard or may eventually lead to a revised tagset and more elaborate guidelines nevertheless the gold standard is by definition correct as far as the evaluation of an automatic tagger is concerned note developing an annotated corpus is a major undertaking apart from the data it generates sophisticated tools documentation and practices for ensuring high quality annotation the tagsets and other coding schemes inevitably depend on some theoretical position that is not shared by all however corpus creators often go to great lengths to make their work as theory neutral as possible in order to maximize the usefulness of their work we will discuss the challenges of creating a corpus in n gram tagging unigram tagging unigram taggers are based on a simple statistical algorithm for each token assign the tag that is most likely for that particular token for emple it will assign the tag jj to any occurrence of the word frequent since frequent is used as an adjective e g a frequent word more often than it is used as a verb e g i frequent this cafe a unigram tagger behaves just like a lookup tagger except there is a more convenient technique for setting it up called training in the following code sample we train a unigram tagger use it to tag a sentence then evaluate we train a unigramtagger by specifying tagged sentence data as a parameter when we initialize the tagger the training process involves inspecting the tag of each word and storing the most likely tag for any word in a dictionary stored inside the tagger separating the training and testing data now that we are training a tagger on some data we must be careful not to test it on the same data as we did in the above emple a tagger that simply memorized its training data and made no attempt to construct a general model would get a perfect score but would also be useless for tagging new text instead we should split the data training on and testing on the remaining although the score is worse we now have a better picture of the usefulness of this tagger i e its performance on previously unseen text general n gram tagging when we perform a language processing task based on unigrams we are using one item of context in the case of tagging we only consider the current token in isolation from any larger context given such a model the best we can do is tag each word with its a priori most likely tag this means we would tag a word such as wind with the same tag regardless of whether it appears in the context the wind or to wind an n gram tagger is a generalization of a unigram tagger whose context is the current word together with the part of speech tags of the n preceding tokens as shown in the tag to be chosen tn is circled and the context is shaded in grey in the emple of an n gram tagger shown in we have n that is we consider the tags of the two preceding words in addition to the current word an n gram tagger picks the tag that is most likely in the given context figure tagger context note a gram tagger is another term for a unigram tagger i e the context used to tag a token is just the text of the token itself gram taggers are also called bigram taggers and gram taggers are called trigram taggers the ngramtagger class uses a tagged training corpus to determine which part of speech tag is most likely for each context here we see a special case of an n gram tagger namely a bigram tagger first we train it then use it to tag untagged sentences notice that the bigram tagger manages to tag every word in a sentence it saw during training but does badly on an unseen sentence as soon as it encounters a new word i e it is unable to assign a tag it cannot tag the following word i e million even if it was seen during training simply because it never saw it during training with a none tag on the previous word consequently the tagger fails to tag the rest of the sentence its overall accuracy score is very low as n gets larger the specificity of the contexts increases as does the chance that the data we wish to tag contains contexts that were not present in the training data this is known as the sparse data problem and is quite pervasive in nlp as a consequence there is a trade off between the accuracy and the coverage of our results and this is related to the precision recall trade off in information retrieval caution n gram taggers should not consider context that crosses a sentence boundary accordingly nltk taggers are designed to work with lists of sentences where each sentence is a list of words at the start of a sentence tn and preceding tags are set to none combining taggers one way to address the trade off between accuracy and coverage is to use the more accurate algorithms when we can but to fall back on algorithms with wider coverage when necessary for emple we could combine the results of a bigram tagger a unigram tagger and a default tagger as follows try tagging the token with the bigram tagger if the bigram tagger is unable to find a tag for the token try the unigram tagger if the unigram tagger is also unable to find a tag use a default tagger most nltk taggers permit a backoff tagger to be specified the backoff tagger may itself have a backoff tagger note your turn extend the above emple by defining a trigramtagger called t which backs off to t note that we specify the backoff tagger when the tagger is initialized so that training can take advantage of the backoff tagger thus if the bigram tagger would assign the same tag as its unigram backoff tagger in a certain context the bigram tagger discards the training instance this keeps the bigram tagger model as small as possible we can further specify that a tagger needs to see more than one instance of a context in order to retain it e g nltk bigramtagger sents cutoff backoff t will discard contexts that have only been seen once or twice tagging unknown words our approach to tagging unknown words still uses backoff to a regular expression tagger or a default tagger these are unable to make use of context thus if our tagger encountered the word blog not seen during training it would assign it the same tag regardless of whether this word appeared in the context the blog or to blog how can we do better with these unknown words or out of vocabulary items a useful method to tag unknown words based on context is to limit the vocabulary of a tagger to the most frequent n words and to replace every other word with a special word unk using the method shown in during training a unigram tagger will probably learn that unk is usually a noun however the n gram taggers will detect contexts in which it has some other tag for emple if the preceding word is to tagged to then unk will probably be tagged as a verb storing taggers training a tagger on a large corpus may take a significant time instead of training a tagger every time we need one it is convenient to save a trained tagger in a file for later re use let save our tagger t to a file t pkl now in a separate python process we can load our saved tagger now let check that it can be used for tagging performance limitations what is the upper limit to the performance of an n gram tagger consider the case of a trigram tagger how many cases of part of speech ambiguity does it encounter we can determine the answer to this question empirically thus one out of twenty trigrams is ambiguous emples given the current word and the previous two tags in of cases there is more than one tag that could be legitimately assigned to the current word according to the training data assuming we always pick the most likely tag in such ambiguous contexts we can derive a lower bound on the performance of a trigram tagger another way to investigate the performance of a tagger is to study its mistakes some tags may be harder than others to assign and it might be possible to treat them specially by pre or post processing the data a convenient way to look at tagging errors is the confusion matrix it charts expected tags the gold standard against actual tags generated by a tagger based on such analysis we may decide to modify the tagset perhaps a distinction between tags that is difficult to make can be dropped since it is not important in the context of some larger processing task another way to analyze the performance bound on a tagger comes from the less than agreement between human annotators more in general observe that the tagging process collapses distinctions e g lexical identity is usually lost when all personal pronouns are tagged prp at the same time the tagging process introduces new distinctions and removes ambiguities e g deal tagged as vb or nn this characteristic of collapsing certain distinctions and introducing new distinctions is an important feature of tagging which facilitates classification and prediction when we introduce finer distinctions in a tagset an n gram tagger gets more detailed information about the left context when it is deciding what tag to assign to a particular word however the tagger simultaneously has to do more work to classify the current token simply because there are more tags to choose from conversely with fewer distinctions as with the simplified tagset the tagger has less information about context and it has a smaller range of choices in classifying the current token we have seen that ambiguity in the training data leads to an upper limit in tagger performance sometimes more context will resolve the ambiguity in other cases however as noted by church young bloothooft the ambiguity can only be resolved with reference to syntax or to world knowledge despite these imperfections part of speech tagging has played a central role in the rise of statistical approaches to natural language processing in the early s the surprising accuracy of statistical taggers was a striking demonstration that it was possible to solve one small part of the language understanding problem namely part of speech disambiguation without reference to deeper sources of linguistic knowledge can this idea be pushed further in we shall see that it can transformation based tagging a potential issue with n gram taggers is the size of their n gram table or language model if tagging is to be employed in a variety of language technologies deployed on mobile computing devices it is important to strike a balance between model size and tagger performance an n gram tagger with backoff may store trigram and bigram tables large sparse arrays which may have hundreds of millions of entries a second issue concerns context the only information an n gram tagger considers from prior context is tags even though words themselves might be a useful source of information it is simply impractical for n gram models to be conditioned on the identities of words in the context in this section we emine brill tagging an inductive tagging method which performs very well using models that are only a tiny fraction of the size of n gram taggers brill tagging is a kind of transformation based learning named after its inventor the general idea is very simple guess the tag of each word then go back and fix the mistakes in this way a brill tagger successively transforms a bad tagging of a text into a better one as with n gram tagging this is a supervised learning method since we need annotated training data to figure out whether the tagger guess is a mistake or not however unlike n gram tagging it does not count observations but compiles a list of transformational correction rules the process of brill tagging is usually explained by analogy with painting suppose we were painting a tree with all its details of boughs branches twigs and leaves against a uniform sky blue background instead of painting the tree first then trying to paint blue in the gaps it is simpler to paint the whole canvas blue then correct the tree section by over painting the blue background in the same fashion we might paint the trunk a uniform brown before going back to over paint further details with even finer brushes brill tagging uses the same idea begin with broad brush strokes then fix up the details with successively finer changes let s look at an emple involving the following sentence we will emine the operation of two rules a replace nn with vb when the previous word is to b replace to with in when the next tag is nns illustrates this process first tagging with the unigram tagger then applying the rules to fix the errors table steps in brill tagging in this table we see two rules all such rules are generated from a template of the following form replace t with t in the context c typical contexts are the identity or the tag of the preceding or following word or the appearance of a specific tag within words of the current word during its training phase the tagger guesses values for t t and c to create thousands of candidate rules each rule is scored according to its net benefit the number of incorrect tags that it corrects less the number of correct tags it incorrectly modifies brill taggers have another interesting property the rules are linguistically interpretable compare this with the n gram taggers which employ a potentially massive table of n grams we cannot learn much from direct inspection of such a table in comparison to the rules learned by the brill tagger demonstrates nltk brill tagger how to determine the category of a word now that we have emined word classes in detail we turn to a more basic question how do we decide what category a word belongs to in the first place in general linguists use morphological syntactic and semantic clues to determine the category of a word morphological clues the internal structure of a word may give useful clues as to the word category for emple ness is a suffix that combines with an adjective to produce a noun e g happy happiness ill illness so if we encounter a word that ends in ness this is very likely to be a noun similarly ment is a suffix that combines with some verbs to produce a noun e g govern government and establish establishment english verbs can also be morphologically complex for instance the present participle of a verb ends in ing and expresses the idea of ongoing incomplete action e g falling eating the ing suffix also appears on nouns derived from verbs e g the falling of the leaves this is known as the gerund syntactic clues another source of information is the typical contexts in which a word can occur for emple assume that we have already determined the category of nouns then we might say that a syntactic criterion for an adjective in english is that it can occur immediately before a noun or immediately following the words be or very according to these tests near should be categorized as an adjective semantic clues finally the meaning of a word is a useful clue as to its lexical category for emple the best known definition of a noun is semantic the name of a person place or thing within modern linguistics semantic criteria for word classes are treated with suspicion mainly because they are hard to formalize nevertheless semantic criteria underpin many of our intuitions about word classes and enable us to make a good guess about the categorization of words in languages that we are unfamiliar with for emple if all we know about the dutch word verjaardag is that it means the same as the english word birthday then we can guess that verjaardag is a noun in dutch however some care is needed although we might translate zij is vandaag jarig as it s her birthday today the word jarig is in fact an adjective in dutch and has no ect equivalent in english new words all languages acquire new lexical items a list of words recently added to the oxford dictionary of english includes cyberslacker fatoush blamestorm sars cantopop bupkis noughties muggle and robata notice that all these new words are nouns and this is reflected in calling nouns an open class by contrast prepositions are regarded as a closed class that is there is a limited set of words belonging to the class e g above along at below beside between during for from in near on outside over past through towards under up with and membership of the set only changes very gradually over time morphology in part of speech tagsets common tagsets often capture some morpho syntactic information that is information about the kind of morphological markings that words receive by virtue of their syntactic role consider for emple the selection of distinct grammatical forms of the word go illustrated in the following sentences each of these forms go goes gone and went is morphologically distinct from the others consider the form goes this occurs in a restricted set of grammatical contexts and requires a third person singular subject thus the following sentences are ungrammatical by contrast gone is the past participle form it is required after have and cannot be replaced in this context by goes and cannot occur as the main verb of a clause we can easily imagine a tagset in which the four distinct grammatical forms just discussed were all tagged as vb although this would be adequate for some purposes a more fine grained tagset provides useful information about these forms that can help other processors that try to detect patterns in tag sequences the brown tagset captures these distinctions as summarized in table some morphosyntactic distinctions in the brown tagset in addition to this set of verb tags the various forms of the verb to be have special tags be be being beg am bem are ber is bez been ben were bed and was bedz plus extra tags for negative forms of the verb all told this fine grained tagging of verbs means that an automatic tagger that uses this tagset is effectively carrying out a limited amount of morphological analysis most part of speech tagsets make use of the same basic categories such as noun verb adjective and preposition however tagsets differ both in how finely they divide words into categories and in how they define their categories for emple is might be tagged simply as a verb in one tagset but as a distinct form of the lexeme be in another tagset as in the brown corpus this variation in tagsets is unavoidable since part of speech tags are used in different ways for different tasks in other words there is no one right way to assign tags only more or less useful ways depending on one goals summary words can be grouped into classes such as nouns verbs adjectives and adverbs these classes are known as lexical categories or parts of speech parts of speech are assigned short labels or tags such as nn vb the process of automatically assigning parts of speech to words in text is called part of speech tagging pos tagging or just tagging automatic tagging is an important step in the nlp pipeline and is useful in a variety of situations including predicting the behavior of previously unseen words analyzing word usage in corpora and text to speech systems some linguistic corpora such as the brown corpus have been pos tagged a variety of tagging methods are possible e g default tagger regular expression tagger unigram tagger and n gram taggers these can be combined using a technique known as backoff taggers can be trained and evaluated using tagged corpora backoff is a method for combining models when a more specialized model such as a bigram tagger cannot assign a tag in a given context we backoff to a more general model such as a unigram tagger part of speech tagging is an important early emple of a sequence classification task in nlp a classification decision at any one point in the sequence makes use of words and tags in the local context a dictionary is used to map between arbitrary types of information such as a string and a number freq cat we create dictionaries using the brace notation pos pos furiously adv ideas n colorless adj n gram taggers can be defined for large values of n but once n is larger than we usually encounter the sparse data problem even with a large quantity of training data we only see a tiny fraction of possible contexts transformation based tagging involves learning a series of repair rules of the form change tag s to tag t in context c where each rule fixes mistakes and possibly introduces a smaller number of errors further reading extra materials for this chapter are posted at http nltk org including links to freely available resources on the web for more emples of tagging with nltk please see the tagging howto at http nltk org howto chapters and of jurafsky martin contain more advanced material on n grams and part of speech tagging the universal tagset is described by petrov das mcdonald other approaches to tagging involve machine learning methods chap data intensive in we will see a generalization of tagging called chunking in which a contiguous sequence of words is assigned a single tag for tagset documentation see nltk help upenn tagset and nltk help brown tagset lexical categories are introduced in linguistics textbooks including those listed in there are many other kinds of tagging words can be tagged with directives to a speech synthesizer indicating which words should be emphasized words can be tagged with sense numbers indicating which sense of the word was used words can also be tagged with morphological features emples of each of these kinds of tags are shown below for space reasons we only show the tag for a single word note also that the first two emples use xml style tags where elements in angle brackets enclose the word that is tagged speech synthesis markup language w c ssml that is a emphasis big emphasis car semcor brown corpus tagged with wordnet senses space in any wf pos nn lemma form wnsn form wf is completely measured by the three dimensions wordnet form nn sense shape form configuration contour conformation morphological tagging from the turin university italian treebank e italiano come progetto e realizzazione il primo primo adj ordin m sing porto turistico dell albania note that tagging is also performed at higher levels here is an emple of dialogue act tagging from the nps chat corpus forsyth martell included with nltk each turn of the dialogue is categorized as to its communicative function statement user dude i wanted some of that ynquestion user m i missing something bye user i am gonna go fix food i will be back later system user join system user slaps user around a bit with a large trout statement user m pm me if u tryin to chat exercises search the web for spoof newspaper headlines to find such gems as british left waffles on falkland islands and juvenile court to try shooting defendant manually tag these headlines to see if knowledge of the part of speech tags removes the ambiguity working with someone else take turns to pick a word that can be either a noun or a verb e g contest the opponent has to predict which one is likely to be the most frequent in the brown corpus check the opponent prediction and tally the score over several turns tokenize and tag the following sentence they wind back the clock while we chase after the wind what different pronunciations and parts of speech are involved review the mappings in discuss any other emples of mappings you can think of what type of information do they map from and to using the python interpreter in interactive mode experiment with the dictionary emples in this chapter create a dictionary d and add some entries what happens if you try to access a non existent entry e g d xyz try deleting an element from a dictionary d using the syntax del d abc check that the item was deleted create two dictionaries d and d and add some entries to each now issue the command d update d what did this do what might it be useful for create a dictionary e to represent a single lexical entry for some word of your choice define keys like headword part of speech sense and emple and assign them suitable values satisfy yourself that there are restrictions on the distribution of go and went in the sense that they cannot be freely interchanged in the kinds of contexts illustrated in d in train a unigram tagger and run it on some new text observe that some words are not assigned a tag why not learn about the affix tagger type help nltk affixtagger train an affix tagger and run it on some new text experiment with different settings for the affix length and the minimum word length discuss your findings train a bigram tagger with no backoff tagger and run it on some of the training data next run it on some new data what happens to the performance of the tagger why we can use a dictionary to specify the values to be substituted into a formatting string read python library documentation for formatting strings http docs python org lib typesseq strings html and use this method to display today date in two different formats use sorted and set to get a sorted list of tags used in the brown corpus removing duplicates write programs to process the brown corpus and find answers to the following questions which nouns are more common in their plural form rather than their singular form only consider regular plurals formed with the s suffix which word has the greatest number of distinct tags what are they and what do they represent list tags in order of decreasing frequency what do the most frequent tags represent which tags are nouns most commonly found after what do these tags represent explore the following issues that arise in connection with the lookup tagger what happens to the tagger performance for the various model sizes when a backoff tagger is omitted consider the curve in suggest a good size for a lookup tagger that balances memory and performance can you come up with scenarios where it would be preferable to minimize memory usage or to maximize performance with no regard for memory usage what is the upper limit of performance for a lookup tagger assuming no limit to the size of its table hint write a program to work out what percentage of tokens of a word are assigned the most likely tag for that word on average generate some statistics for tagged data to answer the following questions what proportion of word types are always assigned the same part of speech tag how many words are ambiguous in the sense that they appear with at least two tags what percentage of word tokens in the brown corpus involve these ambiguous words the evaluate method works out how accurately the tagger performs on this text for emple if the supplied tagged text was the dt dog nn and the tagger produced the output the nn dog nn then the score would be let try to figure out how the evaluation method works a tagger t takes a list of words as input and produces a list of tagged words as output however t evaluate is given correctly tagged text as its only parameter what must it do with this input before performing the tagging once the tagger has created newly tagged text how might the evaluate method go about comparing it with the original tagged text and computing the accuracy score now emine the source code to see how the method is implemented inspect nltk tag api file to discover the location of the source code and open this file using an editor be sure to use the api py file and not the compiled api pyc binary file write code to search the brown corpus for particular words and phrases according to tags to answer the following questions produce an alphabetically sorted list of the distinct words tagged as md identify words that can be plural nouns or third person singular verbs e g deals flies identify three word prepositional phrases of the form in det nn eg in the lab what is the ratio of masculine to feminine pronouns in we saw a table involving frequency counts for the verbs adore love like prefer and preceding qualifiers absolutely and definitely investigate the full range of adverbs that appear before these four verbs we defined the regexp tagger that can be used as a fall back tagger for unknown words this tagger only checks for cardinal numbers by testing for particular prefix or suffix strings it should be possible to guess other tags for emple we could tag any word that ends with s as a plural noun define a regular expression tagger using regexptagger that tests for at least five other patterns in the spelling of words use inline documentation to explain the rules consider the regular expression tagger developed in the exercises in the previous section evaluate the tagger using its accuracy method and try to come up with ways to improve its performance discuss your findings how does objective evaluation help in the development process how serious is the sparse data problem investigate the performance of n gram taggers as n increases from to tabulate the accuracy score estimate the training data required for these taggers assuming a vocabulary size of and a tagset size of obtain some tagged data for another language and train and evaluate a variety of taggers on it if the language is morphologically complex or if there are any orthographic clues e g capitalization to word classes consider developing a regular expression tagger for it ordered after the unigram tagger and before the default tagger how does the accuracy of your tagger s compare with the same taggers run on english data discuss any issues you encounter in applying these methods to the language plotted a curve showing change in the performance of a lookup tagger as the model size was increased plot the performance curve for a unigram tagger as the amount of training data is varied inspect the confusion matrix for the bigram tagger t defined in and identify one or more sets of tags to collapse define a dictionary to do the mapping and evaluate the tagger on the simplified data experiment with taggers using the simplified tagset or make one of your own by discarding all but the first character of each tag name such a tagger has fewer distinctions to make but much less information on which to base its work discuss your findings recall the emple of a bigram tagger which encountered a word it had not seen during training and tagged the rest of the sentence as none it is possible for a bigram tagger to fail part way through a sentence even if it contains no unseen words even if the sentence was used during training in what circumstance can this happen can you write a program to find some emples of this preprocess the brown news data by replacing low frequency words with unk but leaving the tags untouched now train and evaluate a bigram tagger on this data how much does this help what is the contribution of the unigram tagger and default tagger now modify the program in to use a logarithmic scale on the x axis by replacing pylab plot with pylab semilogx what do you notice about the shape of the resulting plot does the gradient tell you anything consult the documentation for the brill tagger demo function using help nltk tag brill demo experiment with the tagger by setting different values for the parameters is there any trade off between training time corpus size and performance write code that builds a dictionary of dictionaries of sets use it to store the set of pos tags that can follow a given word having a given pos tag i e wordi tagi tagi there are distinct words in the brown corpus having ectly three possible tags print a table with the integers in one column and the number of distinct words in the corpus having distinct tags in the other column for the word with the greatest number of distinct tags print out sentences from the corpus containing the word one for each possible tag write a program to classify contexts involving the word must according to the tag of the following word can this be used to discriminate between the epistemic and deontic uses of must create a regular expression tagger and various unigram and n gram taggers incorporating backoff and train them on part of the brown corpus create three different combinations of the taggers test the accuracy of each combined tagger which combination works best try varying the size of the training corpus how does it affect your results our approach for tagging an unknown word has been to consider the letters of the word using regexptagger or to ignore the word altogether and tag it as a noun using nltk defaulttagger these methods will not do well for texts having new words that are not nouns consider the sentence i like to blog on kim blog if blog is a new word then looking at the previous tag to versus np would probably be helpful i e we need a default tagger that is sensitive to the preceding tag create a new kind of unigram tagger that looks at the tag of the previous word and ignores the current word the best way to do this is to modify the source code for unigramtagger which presumes knowledge of object oriented programming in python add this tagger to the sequence of backoff taggers including ordinary trigram and bigram taggers that look at words right before the usual default tagger evaluate the contribution of this new unigram tagger consider the code in which determines the upper bound for accuracy of a trigram tagger review abney discussion concerning the impossibility of ect tagging church young bloothooft explain why correct tagging of these emples requires access to other kinds of information than just words and tags how might you estimate the scale of this problem use some of the estimation techniques in nltk probability such as lidstone or laplace estimation to develop a statistical tagger that does a better job than n gram backoff taggers in cases where contexts encountered during testing were not seen during training inspect the diagnostic files created by the brill tagger rules out and errors out obtain the demonstration code by accessing the source code at http www nltk org code and create your own version of the brill tagger delete some of the rule templates based on what you learned from inspecting rules out add some new rule templates which employ contexts that might help to correct the errors you saw in errors out develop an n gram backoff tagger that permits anti n grams such as the the to be specified when a tagger is initialized an anti ngram is assigned a count of zero and is used to prevent backoff for this n gram e g to avoid estimating p the the as just p the investigate three different ways to define the split between training and testing data when developing a tagger using the brown corpus genre category source fileid and sentence compare their relative performance and discuss which method is the most legitimate you might use n fold cross validation discussed in to improve the accuracy of the evaluations develop your own ngramtagger class that inherits from nltk class and which encapsulates the method of collapsing the vocabulary of the tagged training and testing data that was described in this chapter make sure that the unigram and default backoff taggers have access to the full vocabulary learning to classify text detecting patterns is a central part of natural language processing words ending in ed tend to be past tense verbs frequent use of will is indicative of news text these observable patterns word structure and word frequency happen to correlate with particular aspects of meaning such as tense and topic but how did we know where to start looking which aspects of form to associate with which aspects of meaning the goal of this chapter is to answer the following questions how can we identify particular features of language data that are salient for classifying it how can we construct models of language that can be used to perform language processing tasks automatically what can we learn about language from these models along the way we will study some important machine learning techniques including decision trees naive bayes classifiers and maximum entropy classifiers we will gloss over the mathematical and statistical underpinnings of these techniques focusing instead on how and when to use them see the further readings section for more technical background before looking at these methods we first need to appreciate the broad scope of this topic supervised classification classification is the task of choosing the correct class label for a given input in basic classification tasks each input is considered in isolation from all other inputs and the set of labels is defined in advance some emples of classification tasks are deciding whether an email is spam or not deciding what the topic of a news article is from a fixed list of topic areas such as sports technology and politics deciding whether a given occurrence of the word bank is used to refer to a river bank a financial institution the act of tilting to the side or the act of depositing something in a financial institution the basic classification task has a number of interesting variants for emple in multi class classification each instance may be assigned multiple labels in open class classification the set of labels is not defined in advance and in sequence classification a list of inputs are jointly classified a classifier is called supervised if it is built based on training corpora containing the correct label for each input the framework used by supervised classification is shown in figure supervised classification a during training a feature extractor is used to convert each input value to a feature set these feature sets which capture the basic information about each input that should be used to classify it are discussed in the next section pairs of feature sets and labels are fed into the machine learning algorithm to generate a model b during prediction the same feature extractor is used to convert unseen inputs to feature sets these feature sets are then fed into the model which generates predicted labels in the rest of this section we will look at how classifiers can be employed to solve a wide variety of tasks our discussion is not intended to be comprehensive but to give a representative sample of tasks that can be performed with the help of text classifiers gender identification in we saw that male and female names have some distinctive characteristics names ending in a e and i are likely to be female while names ending in k o r s and t are likely to be male let build a classifier to model these differences more precisely the first step in creating a classifier is deciding what features of the input are relevant and how to encode those features for this emple we will start by just looking at the final letter of a given name the following feature extractor function builds a dictionary containing relevant information about a given name the returned dictionary known as a feature set maps from feature names to their values feature names are case sensitive strings that typically provide a short human readable description of the feature as in the emple last letter feature values are values with simple types such as booleans numbers and strings note most classification methods require that features be encoded using simple value types such as booleans numbers and strings but note that just because a feature has a simple type this does not necessarily mean that the feature value is simple to express or compute indeed it is even possible to use very complex and informative values such as the output of a second supervised classifier as features now that we have defined a feature extractor we need to prepare a list of emples and corresponding class labels next we use the feature extractor to process the names data and divide the resulting list of feature sets into a training set and a test set the training set is used to train a new naive bayes classifier we will learn more about the naive bayes classifier later in the chapter for now let just test it out on some names that did not appear in its training data observe that these character names from the matrix are correctly classified although this science fiction movie is set in it still conforms with our expectations about names and genders we can systematically evaluate the classifier on a much larger quantity of unseen data finally we can emine the classifier to determine which features it found most effective for distinguishing the names genders this listing shows that the names in the training set that end in a are female times more often than they are male but names that end in k are male times more often than they are female these ratios are known as likelihood ratios and can be useful for comparing different feature outcome relationships note your turn modify the gender features function to provide the classifier with features encoding the length of the name its first letter and any other features that seem like they might be informative retrain the classifier with these new features and test its accuracy when working with large corpora constructing a single list that contains the features of every instance can use up a large amount of memory in these cases use the function nltk classify apply features which returns an object that acts like a list but does not store all the feature sets in memory choosing the right features selecting relevant features and deciding how to encode them for a learning method can have an enormous impact on the learning method ability to extract a good model much of the interesting work in building a classifier is deciding what features might be relevant and how we can represent them although it is often possible to get decent performance by using a fairly simple and obvious set of features there are usually significant gains to be had by using carefully constructed features based on a thorough understanding of the task at hand typically feature extractors are built through a process of trial and error guided by intuitions about what information is relevant to the problem it s common to start with a kitchen sink approach including all the features that you can think of and then checking to see which features actually are helpful we take this approach for name gender features in however there are usually limits to the number of features that you should use with a given learning algorithm if you provide too many features then the algorithm will have a higher chance of relying on idiosyncrasies of your training data that do not generalize well to new emples this problem is known as overfitting and can be especially problematic when working with small training sets for emple if we train a naive bayes classifier using the feature extractor shown in it will overfit the relatively small training set resulting in a system whose accuracy is about lower than the accuracy of a classifier that only pays attention to the final letter of each name once an initial set of features has been chosen a very productive method for refining the feature set is error analysis first we select a development set containing the corpus data for creating the model this development set is then subdivided into the training set and the dev test set the training set is used to train the model and the dev test set is used to perform error analysis the test set serves in our final evaluation of the system for reasons discussed below it is important that we employ a separate dev test set for error analysis rather than just using the test set the division of the corpus data into different subsets is shown in figure organization of corpus data for training supervised classifiers the corpus data is divided into two sets the development set and the test set the development set is often further subdivided into a training set and a dev test set having divided the corpus into appropriate datasets we train a model using the training set and then run it on the dev test set using the dev test set we can generate a list of the errors that the classifier makes when predicting name genders we can then emine individual error cases where the model predicted the wrong label and try to determine what additional pieces of information would allow it to make the right decision or which existing pieces of information are tricking it into making the wrong decision the feature set can then be adjusted accordingly the names classifier that we have built generates about errors on the dev test corpus looking through this list of errors makes it clear that some suffixes that are more than one letter can be indicative of name genders for emple names ending in yn appear to be predominantly female despite the fact that names ending in n tend to be male and names ending in ch are usually male even though names that end in h tend to be female we therefore adjust our feature extractor to include features for two letter suffixes rebuilding the classifier with the new feature extractor we see that the performance on the dev test dataset improves by almost percentage points from to this error analysis procedure can then be repeated checking for patterns in the errors that are made by the newly improved classifier each time the error analysis procedure is repeated we should select a different dev test training split to ensure that the classifier does not start to reflect idiosyncrasies in the dev test set but once we have used the dev test set to help us develop the model we can no longer trust that it will give us an accurate idea of how well the model would perform on new data it is therefore important to keep the test set separate and unused until our model development is complete at that point we can use the test set to evaluate how well our model will perform on new input values document classification part of speech tagging in we built a regular expression tagger that chooses a part of speech tag for a word by looking at the internal make up of the word however this regular expression tagger had to be hand crafted instead we can train a classifier to work out which suffixes are most informative let begin by finding out what the most common suffixes are next we will define a feature extractor function which checks a given word for these suffixes feature extraction functions behave like tinted glasses highlighting some of the properties colors in our data and making it impossible to see other properties the classifier will rely exclusively on these highlighted properties when determining how to label inputs in this case the classifier will make its decisions based only on information about which of the common suffixes if any a given word has now that we ve defined our feature extractor we can use it to train a new decision tree classifier to be discussed in one nice feature of decision tree models is that they are often fairly easy to interpret we can even instruct nltk to print them out as pseudocode exploiting context by augmenting the feature extraction function we could modify this part of speech tagger to leverage a variety of other word internal features such as the length of the word the number of syllables it contains or its prefix however as long as the feature extractor just looks at the target word we have no way to add features that depend on the context that the word appears in but contextual features often provide powerful clues about the correct tag for emple when tagging the word fly knowing that the previous word is a will allow us to determine that it is functioning as a noun not a verb in order to accommodate features that depend on a word context we must revise the pattern that we used to define our feature extractor instead of just passing in the word to be tagged we will pass in a complete untagged sentence along with the index of the target word this approach is demonstrated in which employs a context dependent feature extractor to define a part of speech tag classifier it is clear that exploiting contextual features improves the performance of our part of speech tagger for emple the classifier learns that a word is likely to be a noun if it comes immediately after the word large or the word gubernatorial however it is unable to learn the generalization that a word is probably a noun if it follows an adjective because it doesn t have access to the previous word s part of speech tag in general simple classifiers always treat each input as independent from all other inputs in many contexts this makes perfect sense for emple decisions about whether names tend to be male or female can be made on a case by case basis however there are often cases such as part of speech tagging where we are interested in solving classification problems that are closely related to one another sequence classification in order to capture the dependencies between related classification tasks we can use joint classifier models which choose an appropriate labeling for a collection of related inputs in the case of part of speech tagging a variety of different sequence classifier models can be used to jointly choose part of speech tags for all the words in a given sentence one sequence classification strategy known as consecutive classification or greedy sequence classification is to find the most likely class label for the first input then to use that answer to help find the best label for the next input the process can then be repeated until all of the inputs have been labeled this is the approach that was taken by the bigram tagger from which began by choosing a part of speech tag for the first word in the sentence and then chose the tag for each subsequent word based on the word itself and the predicted tag for the previous word this strategy is demonstrated in first we must augment our feature extractor function to take a history argument which provides a list of the tags that we have predicted for the sentence so far each tag in history corresponds with a word in sentence but note that history will only contain tags for words we have already classified that is words to the left of the target word thus while it is possible to look at some features of words to the right of the target word it is not possible to look at the tags for those words since we have not generated them yet having defined a feature extractor we can proceed to build our sequence classifier during training we use the annotated tags to provide the appropriate history to the feature extractor but when tagging new sentences we generate the history list based on the output of the tagger itself other methods for sequence classification one shortcoming of this approach is that we commit to every decision that we make for emple if we decide to label a word as a noun but later find evidence that it should have been a verb there is no way to go back and fix our mistake one solution to this problem is to adopt a transformational strategy instead transformational joint classifiers work by creating an initial assignment of labels for the inputs and then iteratively refining that assignment in an attempt to repair inconsistencies between related inputs the brill tagger described in is a good emple of this strategy another solution is to assign scores to all of the possible sequences of part of speech tags and to choose the sequence whose overall score is highest this is the approach taken by hidden markov models hidden markov models are similar to consecutive classifiers in that they look at both the inputs and the history of predicted tags however rather than simply finding the single best tag for a given word they generate a probability distribution over tags these probabilities are then combined to calculate probability scores for tag sequences and the tag sequence with the highest probability is chosen unfortunately the number of possible tag sequences is quite large given a tag set with tags there are about trillion ways to label a word sentence in order to avoid considering all these possible sequences separately hidden markov models require that the feature extractor only look at the most recent tag or the most recent n tags where n is fairly small given that restriction it is possible to use dynamic programming to efficiently find the most likely tag sequence in particular for each consecutive word index i a score is computed for each possible current and previous tag this same basic approach is taken by two more advanced models called maximum entropy markov models and linear chain conditional random field models but different algorithms are used to find scores for tag sequences further emples of supervised classification sentence segmentation sentence segmentation can be viewed as a classification task for punctuation whenever we encounter a symbol that could possibly end a sentence such as a period or a question mark we have to decide whether it terminates the preceding sentence the first step is to obtain some data that has already been segmented into sentences and convert it into a form that is suitable for extracting features here tokens is a merged list of tokens from the individual sentences and boundaries is a set containing the indexes of all sentence boundary tokens next we need to specify the features of the data that will be used in order to decide whether punctuation indicates a sentence boundary based on this feature extractor we can create a list of labeled featuresets by selecting all the punctuation tokens and tagging whether they are boundary tokens or not using these featuresets we can train and evaluate a punctuation classifier to use this classifier to perform sentence segmentation we simply check each punctuation mark to see whether it is labeled as a boundary and divide the list of words at the boundary marks the listing in shows how this can be done identifying dialogue act types when processing dialogue it can be useful to think of utterances as a type of action performed by the speaker this interpretation is most straightforward for performative statements such as i forgive you or i bet you can t climb that hill but greetings questions answers assertions and clarifications can all be thought of as types of speech based actions recognizing the dialogue acts underlying the utterances in a dialogue can be an important first step in understanding the conversation the nps chat corpus which was demonstrated in consists of over posts from instant messaging sessions these posts have all been labeled with one of dialogue act types such as statement emotion ynquestion and continuer we can therefore use this data to build a classifier that can identify the dialogue act types for new instant messaging posts the first step is to extract the basic messaging data we will call xml posts to get a data structure representing the xml annotation for each post next we will define a simple feature extractor that checks what words the post contains finally we construct the training and testing data by applying the feature extractor to each post using post get class to get a post dialogue act type and create a new classifier recognizing textual entailment recognizing textual entailment rte is the task of determining whether a given piece of text t entails another text called the hypothesis as already discussed in to date there have been four rte challenges where shared development and test data is made available to competing teams here are a couple of emples of text hypothesis pairs from the challenge development dataset the label true indicates that the entailment holds and false that it fails to hold challenge pair true t parviz davudi was representing iran at a meeting of the shanghai co operation organisation sco the fledgling association that binds russia china and four former soviet republics of central asia together to fight terrorism h china is a member of sco challenge pair false t according to nc articles of organization the members of llc company are h nelson beavers iii h chester beavers and jennie beavers stewart h jennie beavers stewart is a share holder of carolina analytical laboratory it should be emphasized that the relationship between text and hypothesis is not intended to be logical entailment but rather whether a human would conclude that the text provides reasonable evidence for taking the hypothesis to be true we can treat rte as a classification task in which we try to predict the true false label for each pair although it seems likely that successful approaches to this task will involve a combination of parsing semantics and real world knowledge many early attempts at rte achieved reasonably good results with shallow analysis based on similarity between the text and hypothesis at the word level in the ideal case we would expect that if there is an entailment then all the information expressed by the hypothesis should also be present in the text conversely if there is information found in the hypothesis that is absent from the text then there will be no entailment in our rte feature detector we let words i e word types serve as proxies for information and our features count the degree of word overlap and the degree to which there are words in the hypothesis but not in the text captured by the method hyp extra not all words are equally important named entity mentions such as the names of people organizations and places are likely to be more significant which motivates us to extract distinct information for words and nes named entities in addition some high frequency function words are filtered out as stopwords to illustrate the content of these features we emine some attributes of the text hypothesis pair shown earlier these features indicate that all important words in the hypothesis are contained in the text and thus there is some evidence for labeling this as true the module nltk classify rte classify reaches just over accuracy on the combined rte test data using methods like these although this figure is not very impressive it requires significant effort and more linguistic processing to achieve much better results scaling up to large datasets python provides an excellent environment for performing basic text processing and feature extraction however it is not able to perform the numerically intensive calculations required by machine learning methods nearly as quickly as lower level languages such as c thus if you attempt to use the pure python machine learning implementations such as nltk naivebayesclassifier on large datasets you may find that the learning algorithm takes an unreasonable amount of time and memory to complete if you plan to train classifiers with large amounts of training data or a large number of features we recommend that you explore nltk facilities for interfacing with external machine learning packages once these packages have been installed nltk can transparently invoke them via system calls to train classifier models significantly faster than the pure python classifier implementations see the nltk webpage for a list of recommended machine learning packages that are supported by nltk evaluation in order to decide whether a classification model is accurately capturing a pattern we must evaluate that model the result of this evaluation is important for deciding how trustworthy the model is and for what purposes we can use it evaluation can also be an effective tool for guiding us in making future improvements to the model the test set most evaluation techniques calculate a score for a model by comparing the labels that it generates for the inputs in a test set or evaluation set with the correct labels for those inputs this test set typically has the same format as the training set however it is very important that the test set be distinct from the training corpus if we simply re used the training set as the test set then a model that simply memorized its input without learning how to generalize to new emples would receive misleadingly high scores when building the test set there is often a trade off between the amount of data available for testing and the amount available for training for classification tasks that have a small number of well balanced labels and a diverse test set a meaningful evaluation can be performed with as few as evaluation instances but if a classification task has a large number of labels or includes very infrequent labels then the size of the test set should be chosen to ensure that the least frequent label occurs at least times additionally if the test set contains many closely related instances such as instances drawn from a single document then the size of the test set should be increased to ensure that this lack of diversity does not skew the evaluation results when large amounts of annotated data are available it is common to err on the side of safety by using of the overall data for evaluation another consideration when choosing the test set is the degree of similarity between instances in the test set and those in the development set the more similar these two datasets are the less confident we can be that evaluation results will generalize to other datasets for emple consider the part of speech tagging task at one extreme we could create the training set and test set by randomly assigning sentences from a data source that reflects a single genre news in this case our test set will be very similar to our training set the training set and test set are taken from the same genre and so we cannot be confident that evaluation results would generalize to other genres what worse because of the call to random shuffle the test set contains sentences that are taken from the same documents that were used for training if there is any consistent pattern within a document say if a given word appears with a particular part of speech tag especially frequently then that difference will be reflected in both the development set and the test set a somewhat better approach is to ensure that the training set and test set are taken from different documents if we want to perform a more stringent evaluation we can draw the test set from documents that are less closely related to those in the training set if we build a classifier that performs well on this test set then we can be confident that it has the power to generalize well beyond the data that it was trained on accuracy the simplest metric that can be used to evaluate a classifier accuracy measures the percentage of inputs in the test set that the classifier correctly labeled for emple a name gender classifier that predicts the correct name times in a test set containing names would have an accuracy of the function nltk classify accuracy will calculate the accuracy of a classifier model on a given test set when interpreting the accuracy score of a classifier it is important to take into consideration the frequencies of the individual class labels in the test set for emple consider a classifier that determines the correct word sense for each occurrence of the word bank if we evaluate this classifier on financial newswire text then we may find that the financial institution sense appears times out of in that case an accuracy of would hardly be impressive since we could achieve that accuracy with a model that always returns the financial institution sense however if we instead evaluate the classifier on a more balanced corpus where the most frequent word sense has a frequency of then a accuracy score would be a much more positive result a similar issue arises when measuring inter annotator agreement in precision and recall another instance where accuracy scores can be misleading is in search tasks such as information retrieval where we are attempting to find documents that are relevant to a particular task since the number of irrelevant documents far outweighs the number of relevant documents the accuracy score for a model that labels every document as irrelevant would be very close to figure true and false positives and negatives it is therefore conventional to employ a different set of measures for search tasks based on the number of items in each of the four categories shown in true positives are relevant items that we correctly identified as relevant true negatives are irrelevant items that we correctly identified as irrelevant false positives or type i errors are irrelevant items that we incorrectly identified as relevant false negatives or type ii errors are relevant items that we incorrectly identified as irrelevant given these four numbers we can define the following metrics precision which indicates how many of the items that we identified were relevant is tp tp fp recall which indicates how many of the relevant items that we identified is tp tp fn the f measure or f score which combines the precision and recall to give a single score is defined to be the harmonic mean of the precision and recall precision recall precision recall confusion matrices decision trees in the next three sections we will take a closer look at three machine learning methods that can be used to automatically build classification models decision trees naive bayes classifiers and maximum entropy classifiers as we have seen it is possible to treat these learning methods as black boxes simply training models and using them for prediction without understanding how they work but there is a lot to be learned from taking a closer look at how these learning methods select models based on the data in a training set an understanding of these methods can help guide our selection of appropriate features and especially our decisions about how those features should be encoded and an understanding of the generated models can allow us to extract information about which features are most informative and how those features relate to one another a decision tree is a simple flowchart that selects labels for input values this flowchart consists of decision nodes which check feature values and leaf nodes which assign labels to choose the label for an input value we begin at the flowchart initial decision node known as its root node this node contains a condition that checks one of the input value features and selects a branch based on that feature value following the branch that describes our input value we arrive at a new decision node with a new condition on the input value features we continue following the branch selected by each node condition until we arrive at a leaf node which provides a label for the input value shows an emple decision tree model for the name gender task figure decision tree model for the name gender task note that tree diagrams are conventionally drawn upside down with the root at the top and the leaves at the bottom once we have a decision tree it is straightforward to use it to assign labels to new input values what s less straightforward is how we can build a decision tree that models a given training set but before we look at the learning algorithm for building decision trees we ll consider a simpler task picking the best decision stump for a corpus a decision stump is a decision tree with a single node that decides how to classify inputs based on a single feature it contains one leaf for each possible feature value specifying the class label that should be assigned to inputs whose features have that value in order to build a decision stump we must first decide which feature should be used the simplest method is to just build a decision stump for each possible feature and see which one achieves the highest accuracy on the training data although there are other alternatives that we will discuss below once we ve picked a feature we can build the decision stump by assigning a label to each leaf based on the most frequent label for the selected emples in the training set i e the emples where the selected feature has that value given the algorithm for choosing decision stumps the algorithm for growing larger decision trees is straightforward we begin by selecting the overall best decision stump for the classification task we then check the accuracy of each of the leaves on the training set leaves that do not achieve sufficient accuracy are then replaced by new decision stumps trained on the subset of the training corpus that is selected by the path to the leaf for emple we could grow the decision tree in by replacing the leftmost leaf with a new decision stump trained on the subset of the training set names that do not start with a k or end with a vowel or an l entropy and information gain as was mentioned before there are several methods for identifying the most informative feature for a decision stump one popular alternative called information gain measures how much more organized the input values become when we divide them up using a given feature to measure how disorganized the original set of input values are we calculate entropy of their labels which will be high if the input values have highly varied labels and low if many input values all have the same label in particular entropy is defined as the sum of the probability of each label times the log probability of that same label figure the entropy of labels in the name gender prediction task as a function of the percentage of names in a given set that are male for emple shows how the entropy of labels in the name gender prediction task depends on the ratio of male to female names note that if most input values have the same label e g if p male is near or near then entropy is low in particular labels that have low frequency do not contribute much to the entropy since p l is small and labels with high frequency also do not contribute much to the entropy since log p l is small on the other hand if the input values have a wide variety of labels then there are many labels with a medium frequency where neither p l nor log p l is small so the entropy is high demonstrates how to calculate the entropy of a list of labels once we have calculated the entropy of the original set of input values labels we can determine how much more organized the labels become once we apply the decision stump to do so we calculate the entropy for each of the decision stump leaves and take the average of those leaf entropy values weighted by the number of samples in each leaf the information gain is then equal to the original entropy minus this new reduced entropy the higher the information gain the better job the decision stump does of dividing the input values into coherent groups so we can build decision trees by selecting the decision stumps with the highest information gain another consideration for decision trees is efficiency the simple algorithm for selecting decision stumps described above must construct a candidate decision stump for every possible feature and this process must be repeated for every node in the constructed decision tree a number of algorithms have been developed to cut down on the training time by storing and reusing information about previously evaluated emples decision trees have a number of useful qualities to begin with they are simple to understand and easy to interpret this is especially true near the top of the decision tree where it is usually possible for the learning algorithm to find very useful features decision trees are especially well suited to cases where many hierarchical categorical distinctions can be made for emple decision trees can be very effective at capturing phylogeny trees however decision trees also have a few disadvantages one problem is that since each branch in the decision tree splits the training data the amount of training data available to train nodes lower in the tree can become quite small as a result these lower decision nodes may overfit the training set learning patterns that reflect idiosyncrasies of the training set rather than linguistically significant patterns in the underlying problem one solution to this problem is to stop dividing nodes once the amount of training data becomes too small another solution is to grow a full decision tree but then to prune decision nodes that do not improve performance on a dev test a second problem with decision trees is that they force features to be checked in a specific order even when features may act relatively independently of one another for emple when classifying documents into topics such as sports automotive or murder mystery features such as hasword football are highly indicative of a specific label regardless of what other the feature values are since there is limited space near the top of the decision tree most of these features will need to be repeated on many different branches in the tree and since the number of branches increases exponentially as we go down the tree the amount of repetition can be very large a related problem is that decision trees are not good at making use of features that are weak predictors of the correct label since these features make relatively small incremental improvements they tend to occur very low in the decision tree but by the time the decision tree learner has descended far enough to use these features there is not enough training data left to reliably determine what effect they should have if we could instead look at the effect of these features across the entire training set then we might be able to make some conclusions about how they should affect the choice of label the fact that decision trees require that features be checked in a specific order limits their ability to exploit features that are relatively independent of one another the naive bayes classification method which we ll discuss next overcomes this limitation by allowing all features to act in parallel naive bayes classifiers in naive bayes classifiers every feature gets a say in determining which label should be assigned to a given input value to choose a label for an input value the naive bayes classifier begins by calculating the prior probability of each label which is determined by checking frequency of each label in the training set the contribution from each feature is then combined with this prior probability to arrive at a likelihood estimate for each label the label whose likelihood estimate is the highest is then assigned to the input value illustrates this process figure an abstract illustration of the procedure used by the naive bayes classifier to choose the topic for a document in the training corpus most documents are automotive so the classifier starts out at a point closer to the automotive label but it then considers the effect of each feature in this emple the input document contains the word dark which is a weak indicator for murder mysteries but it also contains the word football which is a strong indicator for sports documents after every feature has made its contribution the classifier checks which label it is closest to and assigns that label to the input individual features make their contribution to the overall decision by voting against labels that don t occur with that feature very often in particular the likelihood score for each label is reduced by multiplying it by the probability that an input value with that label would have the feature for emple if the word run occurs in of the sports documents of the murder mystery documents and of the automotive documents then the likelihood score for the sports label will be multiplied by the likelihood score for the murder mystery label will be multiplied by and the likelihood score for the automotive label will be multiplied by the overall effect will be to reduce the score of the murder mystery label slightly more than the score of the sports label and to significantly reduce the automotive label with respect to the other two labels this process is illustrated in and figure calculating label likelihoods with naive bayes naive bayes begins by calculating the prior probability of each label based on how frequently each label occurs in the training data every feature then contributes to the likelihood estimate for each label by multiplying it by the probability that input values with that label will have that feature the resulting likelihood score can be thought of as an estimate of the probability that a randomly selected value from the training set would have both the given label and the set of features assuming that the feature probabilities are all independent underlying probabilistic model another way of understanding the naive bayes classifier is that it chooses the most likely label for an input under the assumption that every input value is generated by first choosing a class label for that input value and then generating each feature entirely independent of every other feature of course this assumption is unrealistic features are often highly dependent on one another we will return to some of the consequences of this assumption at the end of this section this simplifying assumption known as the naive bayes assumption or independence assumption makes it much easier to combine the contributions of the different features since we do not need to worry about how they should interact with one another figure a bayesian network graph illustrating the generative process that is assumed by the naive bayes classifier to generate a labeled input the model first chooses a label for the input then it generates each of the input features based on that label every feature is assumed to be entirely independent of every other feature given the label based on this assumption we can calculate an expression for p label features the probability that an input will have a particular label given that it has a particular set of features to choose a label for a new input we can then simply pick the label l that maximizes p l features to begin we note that p label features is equal to the probability that an input has a particular label and the specified set of features divided by the probability that it has the specified set of features next we note that p features will be the same for every choice of label so if we are simply interested in finding the most likely label it suffices to calculate p features label which we will call the label likelihood note if we want to generate a probability estimate for each label rather than just choosing the most likely label then the easiest way to compute p features is to simply calculate the sum over labels of p features label the label likelihood can be expanded out as the probability of the label times the probability of the features given the label furthermore since the features are all independent of one another given the label we can separate out the probability of each individual feature this is ectly the equation we discussed above for calculating the label likelihood p label is the prior probability for a given label and each p f label is the contribution of a single feature to the label likelihood zero counts and smoothing the simplest way to calculate p f label the contribution of a feature f toward the label likelihood for a label label is to take the percentage of training instances with the given label that also have the given feature however this simple approach can become problematic when a feature never occurs with a given label in the training set in this case our calculated value for p f label will be zero which will cause the label likelihood for the given label to be zero thus the input will never be assigned this label regardless of how well the other features fit the label the basic problem here is with our calculation of p f label the probability that an input will have a feature given a label in particular just because we haven t seen a feature label combination occur in the training set doesn t mean it s impossible for that combination to occur for emple we may not have seen any murder mystery documents that contained the word football but we wouldn t want to conclude that it s completely impossible for such documents to exist thus although count f label count label is a good estimate for p f label when count f label is relatively high this estimate becomes less reliable when count f becomes smaller therefore when building naive bayes models we usually employ more sophisticated techniques known as smoothing techniques for calculating p f label the probability of a feature given a label for emple the expected likelihood estimation for the probability of a feature given a label basically adds to each count f label value and the heldout estimation uses a heldout corpus to calculate the relationship between feature frequencies and feature probabilities the nltk probability module provides support for a wide variety of smoothing techniques non binary features we have assumed here that each feature is binary i e that each input either has a feature or does not label valued features e g a color feature which could be red green blue white or orange can be converted to binary features by replacing them with binary features such as color is red numeric features can be converted to binary features by binning which replaces them with features such as x another alternative is to use regression methods to model the probabilities of numeric features for emple if we assume that the height feature has a bell curve distribution then we could estimate p height label by finding the mean and variance of the heights of the inputs with each label in this case p f v label would not be a fixed value but would vary depending on the value of v the naivete of independence the reason that naive bayes classifiers are called naive is that it s unreasonable to assume that all features are independent of one another given the label in particular almost all real world problems contain features with varying degrees of dependence on one another if we had to avoid any features that were dependent on one another it would be very difficult to construct good feature sets that provide the required information to the machine learning algorithm so what happens when we ignore the independence assumption and use the naive bayes classifier with features that are not independent one problem that arises is that the classifier can end up double counting the effect of highly correlated features pushing the classifier closer to a given label than is justified to see how this can occur consider a name gender classifier that contains two identical features f and f in other words f is an ect copy of f and contains no new information when the classifier is considering an input it will include the contribution of both f and f when deciding which label to choose thus the information content of these two features will be given more weight than it deserves of course we do not usually build naive bayes classifiers that contain two identical features however we do build classifiers that contain features which are dependent on one another for emple the features ends with a and ends with vowel are dependent on one another because if an input value has the first feature then it must also have the second feature for features like these the duplicated information may be given more weight than is justified by the training set the cause of double counting the reason for the double counting problem is that during training feature contributions are computed separately but when using the classifier to choose labels for new inputs those feature contributions are combined one solution therefore is to consider the possible interactions between feature contributions during training we could then use those interactions to adjust the contributions that individual features make to make this more precise we can rewrite the equation used to calculate the likelihood of a label separating out the contribution made by each feature or label here w label is the starting score for a given label and w f label is the contribution made by a given feature towards a label s likelihood we call these values w label and w f label the parameters or weights for the model using the naive bayes algorithm we set each of these parameters independently however in the next section we will look at a classifier that considers the possible interactions between these parameters when choosing their values maximum entropy classifiers the maximum entropy classifier uses a model that is very similar to the model employed by the naive bayes classifier but rather than using probabilities to set the model parameters it uses search techniques to find a set of parameters that will maximize the performance of the classifier in particular it looks for the set of parameters that maximizes the total likelihood of the training corpus which is defined as where p label features the probability that an input whose features are features will have class label label is defined as because of the potentially complex interactions between the effects of related features there is no way to directly calculate the model parameters that maximize the likelihood of the training set therefore maximum entropy classifiers choose the model parameters using iterative optimization techniques which initialize the model parameters to random values and then repeatedly refine those parameters to bring them closer to the optimal solution these iterative optimization techniques guarantee that each refinement of the parameters will bring them closer to the optimal values but do not necessarily provide a means of determining when those optimal values have been reached because the parameters for maximum entropy classifiers are selected using iterative optimization techniques they can take a long time to learn this is especially true when the size of the training set the number of features and the number of labels are all large note some iterative optimization techniques are much faster than others when training maximum entropy models avoid the use of generalized iterative scaling gis or improved iterative scaling iis which are both considerably slower than the conjugate gradient cg and the bfgs optimization methods the maximum entropy model the maximum entropy classifier model is a generalization of the model used by the naive bayes classifier like the naive bayes model the maximum entropy classifier calculates the likelihood of each label for a given input value by multiplying together the parameters that are applicable for the input value and label the naive bayes classifier model defines a parameter for each label specifying its prior probability and a parameter for each feature label pair specifying the contribution of individual features towards a label likelihood in contrast the maximum entropy classifier model leaves it up to the user to decide what combinations of labels and features should receive their own parameters in particular it is possible to use a single parameter to associate a feature with more than one label or to associate more than one feature with a given label this will sometimes allow the model to generalize over some of the differences between related labels or features each combination of labels and features that receives its own parameter is called a joint feature note that joint features are properties of labeled values whereas simple features are properties of unlabeled values note in literature that describes and discusses maximum entropy models the term features often refers to joint features the term contexts refers to what we have been calling simple features typically the joint features that are used to construct maximum entropy models ectly mirror those that are used by the naive bayes model in particular a joint feature is defined for each label corresponding to w label and for each combination of simple feature and label corresponding to w f label given the joint features for a maximum entropy model the score assigned to a label for a given input is simply the product of the parameters associated with the joint features that apply to that input and label maximizing entropy the intuition that motivates maximum entropy classification is that we should build a model that captures the frequencies of individual joint features without making any unwarranted assumptions an emple will help to illustrate this principle suppose we are assigned the task of picking the correct word sense for a given word from a list of ten possible senses labeled a j at first we are not told anything more about the word or the senses there are many probability distributions that we could choose for the ten senses such as table although any of these distributions might be correct we are likely to choose distribution i because without any more information there is no reason to believe that any word sense is more likely than any other on the other hand distributions ii and iii reflect assumptions that are not supported by what we know one way to capture this intuition that distribution i is more fair than the other two is to invoke the concept of entropy in the discussion of decision trees we described entropy as a measure of how disorganized a set of labels was in particular if a single label dominates then entropy is low but if the labels are more evenly distributed then entropy is high in our emple we chose distribution i because its label probabilities are evenly distributed in other words because its entropy is high in general the maximum entropy principle states that among the distributions that are consistent with what we know we should choose the distribution whose entropy is highest next suppose that we are told that sense a appears of the time once again there are many distributions that are consistent with this new piece of information such as table but again we will likely choose the distribution that makes the fewest unwarranted assumptions in this case distribution v finally suppose that we are told that the word up appears in the nearby context of the time and that when it does appear in the context there s an chance that sense a or c will be used in this case we will have a harder time coming up with an appropriate distribution by hand however we can verify that the following distribution looks appropriate table in particular the distribution is consistent with what we know if we add up the probabilities in column a we get if we add up the probabilities of row we get and if we add up the boxes for senses a and c in the up row we get or of the up cases furthermore the remaining probabilities appear to be evenly distributed throughout this emple we have restricted ourselves to distributions that are consistent with what we know among these we chose the distribution with the highest entropy this is ectly what the maximum entropy classifier does as well in particular for each joint feature the maximum entropy model calculates the empirical frequency of that feature i e the frequency with which it occurs in the training set it then searches for the distribution which maximizes entropy while still predicting the correct frequency for each joint feature generative vs conditional classifiers an important difference between the naive bayes classifier and the maximum entropy classifier concerns the type of questions they can be used to answer the naive bayes classifier is an emple of a generative classifier which builds a model that predicts p input label the joint probability of a input label pair as a result generative models can be used to answer the following questions what is the most likely label for a given input how likely is a given label for a given input what is the most likely input value how likely is a given input value how likely is a given input value with a given label what is the most likely label for an input that might have one of two values but we do not know which the maximum entropy classifier on the other hand is an emple of a conditional classifier conditional classifiers build models that predict p label input the probability of a label given the input value thus conditional models can still be used to answer questions and however conditional models can not be used to answer the remaining questions in general generative models are strictly more powerful than conditional models since we can calculate the conditional probability p label input from the joint probability p input label but not vice versa however this additional power comes at a price because the model is more powerful it has more free parameters which need to be learned however the size of the training set is fixed thus when using a more powerful model we end up with less data that can be used to train each parameter s value making it harder to find the best parameter values as a result a generative model may not do as good a job at answering questions and as a conditional model since the conditional model can focus its efforts on those two questions however if we do need answers to questions like then we have no choice but to use a generative model the difference between a generative model and a conditional model is analogous to the difference between a topographical map and a picture of a skyline although the topographical map can be used to answer a wider variety of questions it is significantly more difficult to generate an accurate topographical map than it is to generate an accurate skyline modeling linguistic patterns classifiers can help us to understand the linguistic patterns that occur in natural language by allowing us to create explicit models that capture those patterns typically these models are using supervised classification techniques but it is also possible to build analytically motivated models either way these explicit models serve two important purposes they help us to understand linguistic patterns and they can be used to make predictions about new language data the extent to which explicit models can give us insights into linguistic patterns depends largely on what kind of model is used some models such as decision trees are relatively transparent and give us direct information about which factors are important in making decisions and about which factors are related to one another other models such as multi level neural networks are much more opaque although it can be possible to gain insight by studying them it typically takes a lot more work but all explicit models can make predictions about new unseen language data that was not included in the corpus used to build the model these predictions can be evaluated to assess the accuracy of the model once a model is deemed sufficiently accurate it can then be used to automatically predict information about new language data these predictive models can be combined into systems that perform many useful language processing tasks such as document classification automatic translation and question answering what do models tell us it s important to understand what we can learn about language from an automatically constructed model one important consideration when dealing with models of language is the distinction between descriptive models and explanatory models descriptive models capture patterns in the data but they don t provide any information about why the data contains those patterns for emple as we saw in the synonyms absolutely and definitely are not interchangeable we say absolutely adore not definitely adore and definitely prefer not absolutely prefer in contrast explanatory models attempt to capture properties and relationships that cause the linguistic patterns for emple we might introduce the abstract concept of polar verb as one that has an extreme meaning and categorize some verb like adore and detest as polar our explanatory model would contain the constraint that absolutely can only combine with polar verbs and definitely can only combine with non polar verbs in summary descriptive models provide information about correlations in the data while explanatory models go further to postulate causal relationships most models that are automatically constructed from a corpus are descriptive models in other words they can tell us what features are relevant to a given pattern or construction but they ca not necessarily tell us how those features and patterns relate to one another if our goal is to understand the linguistic patterns then we can use this information about which features are related as a starting point for further experiments designed to tease apart the relationships between features and patterns on the other hand if we are just interested in using the model to make predictions e g as part of a language processing system then we can use the model to make predictions about new data without worrying about the details of underlying causal relationships summary modeling the linguistic data found in corpora can help us to understand linguistic patterns and can be used to make predictions about new language data supervised classifiers use labeled training corpora to build models that predict the label of an input based on specific features of that input supervised classifiers can perform a wide variety of nlp tasks including document classification part of speech tagging sentence segmentation dialogue act type identification and determining entailment relations and many other tasks when training a supervised classifier you should split your corpus into three datasets a training set for building the classifier model a dev test set for helping select and tune the model features and a test set for evaluating the final model performance when evaluating a supervised classifier it is important that you use fresh data that was not included in the training or dev test set otherwise your evaluation results may be unrealistically optimistic decision trees are automatically constructed tree structured flowcharts that are used to assign labels to input values based on their features although they are easy to interpret they are not very good at handling cases where feature values interact in determining the proper label in naive bayes classifiers each feature independently contributes to the decision of which label should be used this allows feature values to interact but can be problematic when two or more features are highly correlated with one another maximum entropy classifiers use a basic model that is similar to the model used by naive bayes however they employ iterative optimization to find the set of feature weights that maximizes the probability of the training set most of the models that are automatically constructed from a corpus are descriptive they let us know which features are relevant to a given patterns or construction but they do not give any information about causal relationships between those features and patterns further reading please consult http nltk org for further materials on this chapter and on how to install external machine learning packages such as weka mallet tadm and megam for more emples of classification and machine learning with nltk please see the classification howtos at http nltk org howto for a general introduction to machine learning we recommend alpaydin for a more mathematically intense introduction to the theory of machine learning see hastie tibshirani friedman excellent books on using machine learning techniques for nlp include abney daelemans bosch feldman sanger segaran weiss et al for more on smoothing techniques for language problems see manning schutze for more on sequence modeling and especially hidden markov models see manning schutze or jurafsky martin chapter of manning raghavan schutze discusses the use of naive bayes for classifying texts many of the machine learning algorithms discussed in this chapter are numerically intensive and as a result they will run slowly when coded naively in python for information on increasing the efficiency of numerically intensive algorithms in python see kiusalaas the classification techniques described in this chapter can be applied to a very wide variety of problems for emple agirre edmonds uses classifiers to perform word sense disambiguation and melamed uses classifiers to create parallel texts recent textbooks that cover text classification include manning raghavan schutze and croft metzler strohman much of the current research in the application of machine learning techniques to nlp problems is driven by government sponsored challenges where a set of research organizations are all provided with the same development corpus and asked to build a system and the resulting systems are compared based on a reserved test set emples of these challenge competitions include conll shared tasks the ace competitions the recognizing textual entailment competitions and the aquaint competitions consult http nltk org for a list of pointers to the webpages for these challenges extracting information from text for any given question it s likely that someone has written the answer down somewhere the amount of natural language text that is available in electronic form is truly staggering and is increasing every day however the complexity of natural language can make it very difficult to access the information in that text the state of the art in nlp is still a long way from being able to build general purpose representations of meaning from unrestricted text if we instead focus our efforts on a limited set of questions or entity relations such as where are different facilities located or who is employed by what company we can make significant progress the goal of this chapter is to answer the following questions how can we build a system that extracts structured data such as tables from unstructured text what are some robust methods for identifying the entities and relationships described in a text which corpora are appropriate for this work and how do we use them for training and evaluating our models along the way we will apply techniques from the last two chapters to the problems of chunking and named entity recognition information extraction information comes in many shapes and sizes one important form is structured data where there is a regular and predictable organization of entities and relationships for emple we might be interested in the relation between companies and locations given a particular company we would like to be able to identify the locations where it does business conversely given a location we would like to discover which companies do business in that location if our data is in tabular form such as the emple in then answering these queries is straightforward table locations data if this location data was stored in python as a list of tuples entity relation entity then the question which organizations operate in atlanta could be translated as follows table companies that operate in atlanta things are more tricky if we try to get similar information out of text for emple consider the following snippet from nltk corpus ieer for fileid nyt if you read through you will glean the information required to answer the emple question but how do we get a machine to understand enough about to return the answers in this is obviously a much harder task unlike contains no structure that links organization names with location names one approach to this problem involves building a very general representation of meaning in this chapter we take a different approach deciding in advance that we will only look for very specific kinds of information in text such as the relation between organizations and locations rather than trying to use text like to answer the question directly we first convert the unstructured data of natural language sentences into the structured data of then we reap the benefits of powerful query tools such as sql this method of getting meaning from text is called information extraction information extraction has many applications including business intelligence resume harvesting media analysis sentiment detection patent search and email scanning a particularly important area of current research involves the attempt to extract structured data out of electronically available scientific literature especially in the domain of biology and medicine information extraction architecture shows the architecture for a simple information extraction system it begins by processing a document using several of the procedures discussed in and first the raw text of the document is split into sentences using a sentence segmenter and each sentence is further subdivided into words using a tokenizer next each sentence is tagged with part of speech tags which will prove very helpful in the next step named entity detection in this step we search for mentions of potentially interesting entities in each sentence finally we use relation detection to search for likely relations between different entities in the text figure simple pipeline architecture for an information extraction system this system takes the raw text of a document as its input and generates a list of entity relation entity tuples as its output for emple given a document that indicates that the company georgia pacific is located in atlanta it might generate the tuple org georgia pacific in loc atlanta to perform the first three tasks we can define a simple function that simply connects together nltk default sentence segmenter word tokenizer and part of speech tagger note remember that our program samples assume you begin your interactive session or your program with import nltk re pprint next in named entity detection we segment and label the entities that might participate in interesting relations with one another typically these will be definite noun phrases such as the knights who say ni or proper names such as monty python in some tasks it is useful to also consider indefinite nouns or noun chunks such as every student or cats and these do not necessarily refer to entities in the same way as definite nps and proper names finally in relation extraction we search for specific patterns between pairs of entities that occur near one another in the text and use those patterns to build tuples recording the relationships between the entities chunking the basic technique we will use for entity detection is chunking which segments and labels multi token sequences as illustrated in the smaller boxes show the word level tokenization and part of speech tagging while the large boxes show higher level chunking each of these larger boxes is called a chunk like tokenization which omits whitespace chunking usually selects a subset of the tokens also like tokenization the pieces produced by a chunker do not overlap in the source text figure segmentation and labeling at both the token and chunk levels in this section we will explore chunking in some depth beginning with the definition and representation of chunks we will see regular expression and n gram approaches to chunking and will develop and evaluate chunkers using the conll chunking corpus we will then return in and to the tasks of named entity recognition and relation extraction noun phrase chunking we will begin by considering the task of noun phrase chunking or np chunking where we search for chunks corresponding to individual noun phrases for emple here is some wall street journal text with np chunks marked using brackets as we can see np chunks are often smaller pieces than complete noun phrases for emple the market for system management software for digital hardware is a single noun phrase containing two nested noun phrases but it is captured in np chunks by the simpler chunk the market one of the motivations for this difference is that np chunks are defined so as not to contain other np chunks consequently any prepositional phrases or subordinate clauses that modify a nominal will not be included in the corresponding np chunk since they almost certainly contain further noun phrases one of the most useful sources of information for np chunking is part of speech tags this is one of the motivations for performing part of speech tagging in our information extraction system we demonstrate this approach using an emple sentence that has been part of speech tagged in in order to create an np chunker we will first define a chunk grammar consisting of rules that indicate how sentences should be chunked in this case we will define a simple grammar with a single regular expression rule this rule says that an np chunk should be formed whenever the chunker finds an optional determiner dt followed by any number of adjectives jj and then a noun nn using this grammar we create a chunk parser and test it on our emple sentence the result is a tree which we can either print or display graphically tag patterns the rules that make up a chunk grammar use tag patterns to describe sequences of tagged words a tag pattern is a sequence of part of speech tags delimited using angle brackets e g dt jj nn tag patterns are similar to regular expression patterns now consider the following noun phrases from the wall street journal another dt sharp jj dive nn trade nn figures nns any dt new jj policy nn measures nns earlier jjr stages nns panamanian jj dictator nn manuel nnp noriega nnp we can match these noun phrases using a slight refinement of the first tag pattern above i e dt jj nn this will chunk any sequence of tokens beginning with an optional determiner followed by zero or more adjectives of any type including relative adjectives like earlier jjr followed by one or more nouns of any type however it is easy to find many more complicated emples which this rule will not cover his prp mansion nnp house nnp speech nn the dt price nn cutting vbg cd nn to to cd nn more jjr than in cd nn the dt fastest jjs developing vbg trends nns s pos skill nn note your turn try to come up with tag patterns to cover these cases test them using the graphical interface nltk app chunkparser continue to refine your tag patterns with the help of the feedback given by this tool chunking with regular expressions to find the chunk structure for a given sentence the regexpparser chunker begins with a flat structure in which no tokens are chunked the chunking rules are applied in turn successively updating the chunk structure once all of the rules have been invoked the resulting chunk structure is returned shows a simple chunk grammar consisting of two rules the first rule matches an optional determiner or possessive pronoun zero or more adjectives then a noun the second rule matches one or more proper nouns we also define an emple sentence to be chunked and run the chunker on this input note the symbol is a special character in regular expressions and must be backslash escaped in order to match the tag pp if a tag pattern matches at overlapping locations the leftmost match takes precedence for emple if we apply a rule that matches two consecutive nouns to a text containing three consecutive nouns then only the first two nouns will be chunked once we have created the chunk for money market we have removed the context that would have permitted fund to be included in a chunk this issue would have been avoided with a more permissive chunk rule e g np nn note we have added a comment to each of our chunk rules these are optional when they are present the chunker prints these comments as part of its tracing output exploring text corpora in we saw how we could interrogate a tagged corpus to extract phrases matching a particular sequence of part of speech tags we can do the same work more easily with a chunker as follows note your turn encapsulate the above emple inside a function find chunks that takes a chunk string like chunk v to v as an argument use it to search the corpus for several other patterns such as four or more nouns in a row e g nouns n chinking sometimes it is easier to define what we want to exclude from a chunk we can define a chink to be a sequence of tokens that is not included in a chunk in the following emple barked vbd at in is a chink the dt little jj yellow jj dog nn barked vbd at in the dt cat nn chinking is the process of removing a sequence of tokens from a chunk if the matching sequence of tokens spans an entire chunk then the whole chunk is removed if the sequence of tokens appears in the middle of the chunk these tokens are removed leaving two chunks where there was only one before if the sequence is at the periphery of the chunk these tokens are removed and a smaller chunk remains these three possibilities are illustrated in table three chinking rules applied to the same chunk in we put the entire sentence into a single chunk then excise the chinks representing chunks tags vs trees as befits their intermediate status between tagging and parsing chunk structures can be represented using either tags or trees the most widespread file representation uses iob tags in this scheme each token is tagged with one of three special chunk tags i inside o outside or b begin a token is tagged as b if it marks the beginning of a chunk subsequent tokens within the chunk are tagged i all other tokens are tagged o the b and i tags are suffixed with the chunk type e g b np i np of course it is not necessary to specify a chunk type for tokens that appear outside a chunk so these are just labeled o an emple of this scheme is shown in figure tag representation of chunk structures iob tags have become the standard way to represent chunk structures in files and we will also be using this format here is how the information in would appear in a file we prp b np saw vbd o the dt b np yellow jj i np dog nn i np in this representation there is one token per line each with its part of speech tag and chunk tag this format permits us to represent more than one chunk type so long as the chunks do not overlap as we saw earlier chunk structures can also be represented using trees these have the benefit that each chunk is a constituent that can be manipulated directly an emple is shown in figure tree representation of chunk structures note nltk uses trees for its internal representation of chunks but provides methods for reading and writing such trees to the iob format developing and evaluating chunkers now you have a taste of what chunking does but we have not explained how to evaluate chunkers as usual this requires a suitably annotated corpus we begin by looking at the mechanics of converting iob format into an nltk tree then at how this is done on a larger scale using a chunked corpus we will see how to score the accuracy of a chunker relative to a corpus then look at some more data driven ways to search for np chunks our focus throughout will be on expanding the coverage of a chunker reading iob format and the conll corpus using the corpus module we can load wall street journal text that has been tagged then chunked using the iob notation the chunk categories provided in this corpus are np vp and pp as we have seen each sentence is represented using multiple lines as shown below he prp b np accepted vbd b vp the dt b np position nn i np a conversion function chunk conllstr tree builds a tree representation from one of these multi line strings moreover it permits us to choose any subset of the three chunk types to use here just for np chunks we can use the nltk corpus module to access a larger amount of chunked text the conll corpus contains k words of wall street journal text divided into train and test portions annotated with part of speech tags and chunk tags in the iob format we can access the data using nltk corpus conll here is an emple that reads the th sentence of the train portion of the corpus as you can see the conll corpus contains three chunk types np chunks which we have already seen vp chunks such as has already delivered and pp chunks such as because of since we are only interested in the np chunks right now we can use the chunk types argument to select them simple evaluation and baselines now that we can access a chunked corpus we can evaluate chunkers we start off by establishing a baseline for the trivial chunk parser cp that creates no chunks the iob tag accuracy indicates that more than a third of the words are tagged with o i e not in an np chunk however since our tagger did not find any chunks its precision recall and f measure are all zero now let try a naive regular expression chunker that looks for tags beginning with letters that are characteristic of noun phrase tags e g cd dt and jj as you can see this approach achieves decent results however we can improve on it by adopting a more data driven approach where we use the training corpus to find the chunk tag i o or b that is most likely for each part of speech tag in other words we can build a chunker using a unigram tagger but rather than trying to determine the correct part of speech tag for each word we are trying to determine the correct chunk tag given each word part of speech tag in we define the unigramchunker class which uses a unigram tagger to label sentences with chunk tags most of the code in this class is simply used to convert back and forth between the chunk tree representation used by nltk chunkparseri interface and the iob representation used by the embedded tagger the class defines two methods a constructor which is called when we build a new unigramchunker and the parse method which is used to chunk new sentences the constructor expects a list of training sentences which will be in the form of chunk trees it first converts training data to a form that is suitable for training the tagger using tree conlltags to map each chunk tree to a list of word tag chunk triples it then uses that converted training data to train a unigram tagger and stores it in self tagger for later use the parse method takes a tagged sentence as its input and begins by extracting the part of speech tags from that sentence it then tags the part of speech tags with iob chunk tags using the tagger self tagger that was trained in the constructor next it extracts the chunk tags and combines them with the original sentence to yield conlltags finally it uses conlltags tree to convert the result back into a chunk tree now that we have unigramchunker we can train it using the conll corpus and test its resulting performance this chunker does reasonably well achieving an overall f measure score of let take a look at what it is learned by using its unigram tagger to assign a tag to each of the part of speech tags that appear in the corpus it has discovered that most punctuation marks occur outside of np chunks with the exception of and both of which are used as currency markers it has also found that determiners dt and possessives prp and wp occur at the beginnings of np chunks while noun types nn nnp nnps nns mostly occur inside of np chunks the only piece left to fill in is the feature extractor we begin by defining a simple feature extractor which just provides the part of speech tag of the current token using this feature extractor our classifier based chunker is very similar to the unigram chunker as is reflected in its performance we can also add a feature for the previous part of speech tag adding this feature allows the classifier to model interactions between adjacent tags and results in a chunker that is closely related to the bigram chunker next we will try adding a feature for the current word since we hypothesized that word content should be useful for chunking we find that this feature does indeed improve the chunker performance by about percentage points which corresponds to about a reduction in the error rate finally we can try extending the feature extractor with a variety of additional features such as lookahead features paired features and complex contextual features this last feature called tags since dt creates a string describing the set of all part of speech tags that have been encountered since the most recent determiner or since the beginning of the sentence if there is no determiner before index i note your turn try adding different features to the feature extractor function npchunk features and see if you can further improve the performance of the np chunker recursion in linguistic structure building nested structure with cascaded chunkers so far our chunk structures have been relatively flat trees consist of tagged tokens optionally grouped under a chunk node such as np however it is possible to build chunk structures of arbitrary depth simply by creating a multi stage chunk grammar containing recursive rules has patterns for noun phrases prepositional phrases verb phrases and sentences this is a four stage chunk grammar and can be used to create structures having a depth of at most four unfortunately this result misses the vp headed by saw it has other shortcomings too let see what happens when we apply this chunker to a sentence having deeper nesting notice that it fails to identify the vp chunk starting at the solution to these problems is to get the chunker to loop over its patterns after trying all of them it repeats the process we add an optional second argument loop to specify the number of times the set of patterns should be run note this cascading process enables us to create deep structures however creating and debugging a cascade is difficult and there comes a point where it is more effective to do full parsing see also the cascading process can only produce trees of fixed depth no deeper than the number of stages in the cascade and this is insufficient for complete syntactic analysis trees a tree is a set of connected labeled nodes each reachable by a unique path from a distinguished root node here is an emple of a tree note that they are standardly drawn upside down we use a family metaphor to talk about the relationships of nodes in a tree for emple s is the parent of vp conversely vp is a child of s also since np and vp are both children of s they are also siblings for convenience there is also a text format for specifying trees although we will focus on syntactic trees trees can be used to encode any homogeneous hierarchical structure that spans a sequence of linguistic forms e g morphological structure discourse structure in the general case leaves and node values do not have to be strings in nltk we create a tree by giving a node label and a list of children we can incorporate these into successively larger trees as follows here are some of the methods available for tree objects the bracketed representation for complex trees can be difficult to read in these cases the draw method can be very useful it opens a new window containing a graphical representation of the tree the tree display window allows you to zoom in and out to collapse and expand subtrees and to print the graphical representation to a postscript file for inclusion in a document tree traversal it is standard to use a recursive function to traverse a tree the listing in demonstrates this note we have used a technique called duck typing to detect that t is a tree i e t label is defined named entity recognition at the start of this chapter we briefly introduced named entities nes named entities are definite noun phrases that refer to specific types of individuals such as organizations persons dates and so on lists some of the more commonly used types of nes these should be self explanatory except for facility human made artifacts in the domains of architecture and civil engineering and gpe geo political entities such as city state province and country table commonly used types of named entity the goal of a named entity recognition ner system is to identify all textual mentions of the named entities this can be broken down into two sub tasks identifying the boundaries of the ne and identifying its type while named entity recognition is frequently a prelude to identifying relations in information extraction it can also contribute to other tasks for emple in question answering qa we try to improve the precision of information retrieval by recovering not whole pages but just those parts which contain an answer to the user question most qa systems take the documents returned by standard information retrieval and then attempt to isolate the minimal text snippet in the document containing the answer now suppose the question was who was the first president of the us and one of the documents that was retrieved contained the following passage analysis of the question leads us to expect that an answer should be of the form x was the first president of the us where x is not only a noun phrase but also refers to a named entity of type person this should allow us to ignore the first sentence in the passage while it contains two occurrences of washington named entity recognition should tell us that neither of them has the correct type how do we go about identifying named entities one option would be to look up each word in an appropriate list of names for emple in the case of locations we could use a gazetteer or geographical dictionary such as the alendria gazetteer or the getty gazetteer however doing this blindly runs into problems as shown in figure location detection by simple lookup for a news story looking up every word in a gazetteer is error prone case distinctions may help but these are not always present observe that the gazetteer has good coverage of locations in many countries and incorrectly finds locations like sanchez in the dominican republic and on in vietnam of course we could omit such locations from the gazetteer but then we wo not be able to identify them when they do appear in a document it gets even harder in the case of names for people or organizations any list of such names will probably have poor coverage new organizations come into existence every day so if we are trying to deal with contemporary newswire or blog entries it is unlikely that we will be able to recognize many of the entities using gazetteer lookup another major source of difficulty is caused by the fact that many named entity terms are ambiguous thus may and north are likely to be parts of named entities for date and location respectively but could both be part of a person conversely christian dior looks like a person but is more likely to be of type organization a term like yankee will be ordinary modifier in some contexts but will be marked as an entity of type organization in the phrase yankee infielders further challenges are posed by multi word names like stanford university and by names that contain other names such as cecil h green library and escondido village conference service center in named entity recognition therefore we need to be able to identify the beginning and end of multi token sequences named entity recognition is a task that is well suited to the type of classifier based approach that we saw for noun phrase chunking in particular we can build a tagger that labels each word in a sentence using the iob format where chunks are labeled by their appropriate type here is part of the conll conll dutch training data eddy n b per bonte n i per is v o woordvoerder n o van prep o diezelfde pron o hogeschool n b org punc o in this representation there is one token per line each with its part of speech tag and its named entity tag based on this training corpus we can construct a tagger that can be used to label new sentences and use the nltk chunk conlltags tree function to convert the tag sequences into a chunk tree nltk provides a classifier that has already been trained to recognize named entities accessed with the function nltk ne chunk if we set the parameter binary true then named entities are just tagged as ne otherwise the classifier adds category labels such as person organization and gpe once named entities have been identified in a text we then want to extract the relations that exist between them as indicated earlier we will typically be looking for relations between specified types of named entity one way of approaching this task is to initially look for all triples of the form x y where x and y are named entities of the required types and is the string of words that intervenes between x and y we can then use regular expressions to pull out just those instances of that express the relation that we are looking for the following emple searches for strings that contain the word in the special regular expression b ing b is a negative lookahead assertion that allows us to disregard strings such as success in supervising the transition of where in is followed by a gerund searching for the keyword in works reasonably well though it will also retrieve false positives such as org house transportation committee secured the most money in the loc new york there is unlikely to be simple string based method of excluding filler strings such as this as shown above the conll dutch corpus contains not just named entity annotation but also part of speech tags this allows us to devise patterns that are sensitive to these tags as shown in the next emple the method clause prints out the relations in a clausal form where the binary relation symbol is specified as the value of parameter relsym note your turn replace the last line by print nltk rtuple rel lcon true rcon true this will show you the actual words that intervene between the two nes and also their left and right context within a default word window with the help of a dutch dictionary you might be able to figure out why the result van annie lennox eurythmics is a false hit summary information extraction systems search large bodies of unrestricted text for specific types of entities and relations and use them to populate well organized databases these databases can then be used to find answers for specific questions the typical architecture for an information extraction system begins by segmenting tokenizing and part of speech tagging the text the resulting data is then searched for specific types of entity finally the information extraction system looks at entities that are mentioned near one another in the text and tries to determine whether specific relationships hold between those entities entity recognition is often performed using chunkers which segment multi token sequences and label them with the appropriate entity type common entity types include organization person location date time money and gpe geo political entity chunkers can be constructed using rule based systems such as the regexpparser class provided by nltk or using machine learning techniques such as the consecutivenpchunker presented in this chapter in either case part of speech tags are often a very important feature when searching for chunks although chunkers are specialized to create relatively flat data structures where no two chunks are allowed to overlap they can be cascaded together to build nested structures relation extraction can be performed using either rule based systems which typically look for specific patterns in the text that connect entities and the intervening words or using machine learning systems which typically attempt to learn such patterns automatically from a training corpus further reading extra materials for this chapter are posted at http nltk org including links to freely available resources on the web for more emples of chunking with nltk please see the chunking howto at http nltk org howto the popularity of chunking is due in great part to pioneering work by abney e g church young bloothooft abney cass chunker is described in http www vinartus net spa a pdf the word chink initially meant a sequence of stopwords according to a paper by ross and tukey church young bloothooft the iob format or sometimes bio format was developed for np chunking by ramshaw marcus and was used for the shared np bracketing task run by the conference on natural language learning conll in the same format was adopted by conll for annotating a section of wall street journal text as part of a shared task on np chunking section of jurafsky martin contains a discussion of chunking chapter covers information extraction including named entity recognition for information about text mining in biology and medicine see ananiadou mcnaught exercises the iob format categorizes tagged tokens as i o and b why are three tags necessary what problem would be caused if we used i and o tags exclusively write a tag pattern to match noun phrases containing plural head nouns e g many jj researchers nns two cd weeks nns both dt new jj positions nns try to do this by generalizing the tag pattern that handled singular noun phrases pick one of the three chunk types in the conll corpus inspect the conll corpus and try to observe any patterns in the pos tag sequences that make up this kind of chunk develop a simple chunker using the regular expression chunker nltk regexpparser discuss any tag sequences that are difficult to chunk reliably an early definition of chunk was the material that occurs between chinks develop a chunker that starts by putting the whole sentence in a single chunk and then does the rest of its work solely by chinking determine which tags or tag sequences are most likely to make up chinks with the help of your own utility program compare the performance and simplicity of this approach relative to a chunker based entirely on chunk rules write a tag pattern to cover noun phrases that contain gerunds e g the dt receiving vbg end nn assistant nn managing vbg editor nn add these patterns to the grammar one per line test your work using some tagged sentences of your own devising write one or more tag patterns to handle coordinated noun phrases e g july nnp and cc august nnp all dt your prp managers nns and cc supervisors nns company nn courts nns and cc adjudicators nns carry out the following evaluation tasks for any of the chunkers you have developed earlier note that most chunking corpora contain some internal inconsistencies such that any reasonable rule based approach will produce errors evaluate your chunker on sentences from a chunked corpus and report the precision recall and f measure use the chunkscore missed and chunkscore incorrect methods to identify the errors made by your chunker discuss compare the performance of your chunker to the baseline chunker discussed in the evaluation section of this chapter develop a chunker for one of the chunk types in the conll corpus using a regular expression based chunk grammar regexpchunk use any combination of rules for chunking chinking merging or splitting sometimes a word is incorrectly tagged e g the head noun in cd or cc so rb cases vbz instead of requiring manual correction of tagger output good chunkers are able to work with the erroneous output of taggers look for other emples of correctly chunked noun phrases with incorrect tags the bigram chunker scores about accuracy study its errors and try to work out why it does not get accuracy experiment with trigram chunking are you able to improve the performance any more apply the n gram and brill tagging methods to iob chunk tagging instead of assigning pos tags to words here we will assign iob tags to the pos tags e g if the tag dt determiner often occurs at the start of a chunk it will be tagged b begin evaluate the performance of these chunking methods relative to the regular expression chunking methods covered in this chapter we saw in that it is possible to establish an upper limit to tagging performance by looking for ambiguous n grams n grams that are tagged in more than one possible way in the training data apply the same method to determine an upper bound on the performance of an n gram chunker pick one of the three chunk types in the conll corpus write functions to do the following tasks for your chosen type list all the tag sequences that occur with each instance of this chunk type count the frequency of each tag sequence and produce a ranked list in order of decreasing frequency each line should consist of an integer the frequency and the tag sequence inspect the high frequency tag sequences use these as the basis for developing a better chunker the baseline chunker presented in the evaluation section tends to create larger chunks than it should for emple the phrase every dt time nn she prp sees vbz a dt newspaper nn contains two consecutive chunks and our baseline chunker will incorrectly combine the first two every dt time nn she prp write a program that finds which of these chunk internal tags typically occur at the start of a chunk then devise one or more rules that will split up these chunks combine these with the existing baseline chunker and re evaluate it to see if you have discovered an improved baseline develop an np chunker that converts pos tagged text into a list of tuples where each tuple consists of a verb followed by a sequence of noun phrases and prepositions e g the little cat sat on the mat becomes sat on np the penn treebank contains a section of tagged wall street journal text that has been chunked into noun phrases the format uses square brackets and we have encountered it several times during this chapter the treebank corpus can be accessed using for sent in nltk corpus treebank chunk chunked sents fileid these are flat trees just as we got using nltk corpus conll chunked sents the functions nltk tree pprint and nltk chunk tree conllstr can be used to create treebank and iob strings from a tree write functions chunk brackets and chunk iob that take a single chunk tree as their sole argument and return the required multi line string representation write command line conversion utilities bracket iob py and iob bracket py that take a file in treebank or conll format resp and convert it to the other format obtain some raw treebank or conll data from the nltk corpora save it to a file and then use for line in open filename to access it from python an n gram chunker can use information other than the current part of speech tag and the n previous chunk tags investigate other models of the context such as the n previous part of speech tags or some combination of previous chunk tags along with previous and following part of speech tags consider the way an n gram tagger uses recent tags to inform its tagging choice now observe how a chunker may re use this sequence information for emple both tasks will make use of the information that nouns tend to follow adjectives in english it would appear that the same information is being maintained in two places is this likely to become a problem as the size of the rule sets grows if so speculate about any ways that this problem might be addressed analyzing sentence structure earlier chapters focused on words how to identify them analyze their structure assign them to lexical categories and access their meanings we have also seen how to identify patterns in word sequences or n grams however these methods only scratch the surface of the complex constraints that govern sentences we need a way to deal with the ambiguity that natural language is famous for we also need to be able to cope with the fact that there are an unlimited number of possible sentences and we can only write finite programs to analyze their structures and discover their meanings the goal of this chapter is to answer the following questions how can we use a formal grammar to describe the structure of an unlimited set of sentences how do we represent the structure of sentences using syntax trees how do parsers analyze a sentence and automatically build a syntax tree along the way we will cover the fundamentals of english syntax and see that there are systematic aspects of meaning that are much easier to capture once we have identified the structure of sentences some grammatical dilemmas linguistic data and unlimited possibilities previous chapters have shown you how to process and analyse text corpora and we have stressed the challenges for nlp in dealing with the vast amount of electronic language data that is growing daily let s consider this data more closely and make the thought experiment that we have a gigantic corpus consisting of everything that has been either uttered or written in english over say the last years would we be justified in calling this corpus the language of modern english there are a number of reasons why we might answer no recall that in we asked you to search the web for instances of the pattern the of although it is easy to find emples on the web containing this word sequence such as new man at the of img http www telegraph co uk sport new man at the of img html speakers of english will say that most such emples are errors and therefore not part of english after all accordingly we can argue that the modern english is not equivalent to the very big set of word sequences in our imaginary corpus speakers of english can make judgements about these sequences and will reject some of them as being ungrammatical equally it is easy to compose a new sentence and have speakers agree that it is perfectly good english for emple sentences have an interesting property that they can be embedded inside larger sentences consider the following sentences if we replaced whole sentences with the symbol s we would see patterns like andre said s and i think s these are templates for taking a sentence and constructing a bigger sentence there are other templates we can use like s but s and s when s with a bit of ingenuity we can construct some really long sentences using these templates here is an impressive emple from a winnie the pooh story by a a milne in which piglet is entirely surrounded by water you can imagine piglet s joy when at last the ship came in sight of him in after years he liked to think that he had been in very great danger during the terrible flood but the only danger he had really been in was the last half hour of his imprisonment when owl who had just flown up sat on a branch of his tree to comfort him and told him a very long story about an aunt who had once laid a seagull s egg by mistake and the story went on and on rather like this sentence until piglet who was listening out of his window without much hope went to sleep quietly and naturally slipping slowly out of the window towards the water until he was only hanging on by his toes at which moment luckily a sudden loud squawk from owl which was really part of the story being what his aunt said woke the piglet up and just gave him time to jerk himself back into safety and say how interesting and did she when well you can imagine his joy when at last he saw the good ship brain of pooh captain c robin st mate p bear coming over the sea to rescue him this long sentence actually has a simple structure that begins s but s when s we can see from this emple that language provides us with constructions which seem to allow us to extend sentences indefinitely it is also striking that we can understand sentences of arbitrary length that we have never heard before it is not hard to concoct an entirely novel sentence one that has probably never been used before in the history of the language yet all speakers of the language will understand it the purpose of a grammar is to give an explicit description of a language but the way in which we think of a grammar is closely intertwined with what we consider to be a language is it a large but finite set of observed utterances and written texts is it something more abstract like the implicit knowledge that competent speakers have about grammatical sentences or is it some combination of the two we wo not take a stand on this issue but instead will introduce the main approaches in this chapter we will adopt the formal framework of generative grammar in which a language is considered to be nothing more than an enormous collection of all grammatical sentences and a grammar is a formal notation that can be used for generating the members of this set grammars use recursive productions of the form s s and s as we will explore in in we will extend this to automatically build up the meaning of a sentence out of the meanings of its parts ubiquitous ambiguity a well known emple of ambiguity is shown in from the groucho marx movie animal crackers let take a closer look at the ambiguity in the phrase i shot an elephant in my pajamas first we need to define a simple grammar this grammar permits the sentence to be analyzed in two ways depending on whether the prepositional phrase in my pajamas describes the elephant or the shooting event the program produces two bracketed structures which we can depict as trees as shown in b notice that there is no ambiguity concerning the meaning of any of the words e g the word shot does not refer to the act of using a gun in the first sentence and using a camera in the second sentence note your turn consider the following sentences and see if you can think of two quite different interpretations fighting animals could be dangerous visiting relatives can be tiresome is ambiguity of the individual words to blame if not what is the cause of the ambiguity this chapter presents grammars and parsing as the formal and computational methods for investigating and modeling the linguistic phenomena we have been discussing as we shall see patterns of well formedness and ill formedness in a sequence of words can be understood with respect to the phrase structure and dependencies we can develop formal models of these structures using grammars and parsers as before a key motivation is natural language understanding how much more of the meaning of a text can we access when we can reliably recognize the linguistic structures it contains having read in a text can a program understand it enough to be able to answer simple questions about what happened or who did what to whom also as before we will develop simple programs to process annotated corpora and perform useful tasks what the use of syntax beyond n grams we gave an emple in of how to use the frequency information in bigrams to generate text that seems perfectly acceptable for small sequences of words but rapidly degenerates into nonsense here is another pair of emples that we created by computing the bigrams over the text of a childrens story the adventures of buster brown http www gutenberg org files txt you intuitively know that these sequences are word salad but you probably find it hard to pin down what s wrong with them one benefit of studying grammar is that it provides a conceptual framework and vocabulary for spelling out these intuitions let s take a closer look at the sequence the worst part and clumsy looking this looks like a coordinate structure where two phrases are joined by a coordinating conjunction such as and but or or here s an informal and simplified statement of how coordination works syntactically coordinate structure if v and v are both phrases of grammatical category x then v and v is also a phrase of category x here are a couple of emples in the first two nps noun phrases have been conjoined to make an np while in the second two aps adjective phrases have been conjoined to make an ap what we ca not do is conjoin an np and an ap which is why the worst part and clumsy looking is ungrammatical before we can formalize these ideas we need to understand the concept of constituent structure constituent structure is based on the observation that words combine with other words to form units the evidence that a sequence of words forms such a unit is given by substitutability that is a sequence of words in a well formed sentence can be replaced by a shorter sequence without rendering the sentence ill formed to clarify this idea consider the following sentence the fact that we can substitute he for the little bear indicates that the latter sequence is a unit by contrast we cannot replace little bear saw in the same way in we systematically substitute longer sequences by shorter ones in a way which preserves grammaticality each sequence that forms a unit can in fact be replaced by a single word and we end up with just two elements figure substitution of word sequences working from the top row we can replace particular sequences of words e g the brook with individual words e g it repeating this process we arrive at a grammatical two word sentence in we have added grammatical category labels to the words we saw in the earlier figure the labels np vp and pp stand for noun phrase verb phrase and prepositional phrase respectively figure substitution of word sequences plus grammatical categories this diagram reproduces along with grammatical categories corresponding to noun phrases np verb phrases vp prepositional phrases pp and nominals nom if we now strip out the words apart from the topmost row add an s node and flip the figure over we end up with a standard phrase structure tree shown in each node in this tree including the words is called a constituent the immediate constituents of s are np and vp as we will see in the next section a grammar specifies how the sentence can be subdivided into its immediate constituents and how these can be further subdivided until we reach the level of individual words note as we saw in sentences can have arbitrary length consequently phrase structure trees can have arbitrary depth the cascaded chunk parsers we saw in can only produce structures of bounded depth so chunking methods are not applicable here context free grammar a simple grammar let start off by looking at a simple context free grammar by convention the left hand side of the first production is the start symbol of the grammar typically s and all well formed trees must have this symbol as their root label in nltk context free grammars are defined in the nltk grammar module in we define a grammar and show how to parse a simple sentence admitted by the grammar the grammar in contains productions involving various syntactic categories as laid out in table syntactic categories a production like vp v np v np pp has a disjunction on the righthand side shown by the and is an abbreviation for the two productions vp v np and vp v np pp figure recursive descent parser demo this tool allows you to watch the operation of a recursive descent parser as it grows the parse tree and matches it against the input words note your turn try developing a simple grammar of your own using the recursive descent parser application nltk app rdparser shown in it comes already loaded with a sample grammar but you can edit this as you please using the edit menu change the grammar and the sentence to be parsed and run the parser using the autostep button if we parse the sentence the dog saw a man in the park using the grammar shown in we end up with two trees similar to those we saw for b since our grammar licenses two trees for this sentence the sentence is said to be structurally ambiguous the ambiguity in question is called a prepositional phrase attachment ambiguity as we saw earlier in this chapter as you may recall it is an ambiguity about attachment since the pp in the park needs to be attached to one of two places in the tree either as a child of vp or else as a child of np when the pp is attached to vp the intended interpretation is that the seeing event happened in the park however if the pp is attached to np then it was the man who was in the park and the agent of the seeing the dog might have been sitting on the balcony of an apartment overlooking the park writing your own grammars if you are interested in experimenting with writing cfgs you will find it helpful to create and edit your grammar in a text file say mygrammar cfg you can then load it into nltk and parse with it as follows make sure that you put a cfg suffix on the filename and that there are no spaces in the string file mygrammar cfg if the command print tree produces no output this is probably because your sentence sent is not admitted by your grammar in this case call the parser with tracing set to be on rd parser nltk recursivedescentparser grammar trace you can also check what productions are currently in the grammar with the command for p in grammar productions print p when you write cfgs for parsing in nltk you cannot combine grammatical categories with lexical items on the righthand side of the same production thus a production such as pp of np is disallowed in addition you are not permitted to place multi word lexical items on the righthand side of a production so rather than writing np new york you have to resort to something like np new york instead recursion in syntactic structure a grammar is said to be recursive if a category occurring on the left hand side of a production also appears on the righthand side of a production as illustrated in the production nom adj nom where nom is the category of nominals involves direct recursion on the category nom whereas indirect recursion on s arises from the combination of two productions namely s np vp and vp v s to see how recursion arises from this grammar consider the following trees a involves nested nominal phrases while b contains nested sentences we have only illustrated two levels of recursion here but there is no upper limit on the depth you can experiment with parsing sentences that involve more deeply nested structures beware that the recursivedescentparser is unable to handle left recursive productions of the form x x y we will return to this in parsing with context free grammar a parser processes input sentences according to the productions of a grammar and builds one or more constituent structures that conform to the grammar a grammar is a declarative specification of well formedness it is actually just a string not a program a parser is a procedural interpretation of the grammar it searches through the space of trees licensed by a grammar to find one that has the required sentence along its fringe a parser permits a grammar to be evaluated against a collection of test sentences helping linguists to discover mistakes in their grammatical analysis a parser can serve as a model of psycholinguistic processing helping to explain the difficulties that humans have with processing certain syntactic constructions many natural language applications involve parsing at some point for emple we would expect the natural language questions submitted to a question answering system to undergo parsing as an initial step in this section we see two simple parsing algorithms a top down method called recursive descent parsing and a bottom up method called shift reduce parsing we also see some more sophisticated algorithms a top down method with bottom up filtering called left corner parsing and a dynamic programming technique called chart parsing recursive descent parsing the simplest kind of parser interprets a grammar as a specification of how to break a high level goal into several lower level subgoals the top level goal is to find an s the s np vp production permits the parser to replace this goal with two subgoals find an np then find a vp each of these subgoals can be replaced in turn by sub sub goals using productions that have np and vp on their left hand side eventually this expansion process leads to subgoals such as find the word telescope such subgoals can be directly compared against the input sequence and succeed if the next word is matched if there is no match the parser must back up and try a different alternative the recursive descent parser builds a parse tree during the above process with the initial goal find an s the s root node is created as the above process recursively expands its goals using the productions of the grammar the parse tree is extended downwards hence the name recursive descent we can see this in action using the graphical demonstration nltk app rdparser six stages of the execution of this parser are shown in figure six stages of a recursive descent parser the parser begins with a tree consisting of the node s at each stage it consults the grammar to find a production that can be used to enlarge the tree when a lexical production is encountered its word is compared against the input after a complete parse has been found the parser backtracks to look for more parses during this process the parser is often forced to choose between several possible productions for emple in going from step to step it tries to find productions with n on the left hand side the first of these is n man when this does not work it backtracks and tries other n productions in order until it gets to n dog which matches the next word in the input sentence much later as shown in step it finds a complete parse this is a tree that covers the entire sentence without any dangling edges once a parse has been found we can get the parser to look for additional parses again it will backtrack and explore other choices of production in case any of them result in a parse nltk provides a recursive descent parser note recursivedescentparser takes an optional parameter trace if trace is greater than zero then the parser will report the steps that it takes as it parses a text recursive descent parsing has three key shortcomings first left recursive productions like np np pp send it into an infinite loop second the parser wastes a lot of time considering words and structures that do not correspond to the input sentence third the backtracking process may discard parsed constituents that will need to be rebuilt again later for emple backtracking over vp v np will discard the subtree created for the np if the parser then proceeds with vp v np pp then the np subtree must be created all over again recursive descent parsing is a kind of top down parsing top down parsers use a grammar to predict what the input will be before inspecting the input however since the input is available to the parser all along it would be more sensible to consider the input sentence from the very beginning this approach is called bottom up parsing and we will see an emple in the next section shift reduce parsing a simple kind of bottom up parser is the shift reduce parser in common with all bottom up parsers a shift reduce parser tries to find sequences of words and phrases that correspond to the right hand side of a grammar production and replace them with the left hand side until the whole sentence is reduced to an s the shift reduce parser repeatedly pushes the next input word onto a stack this is the shift operation if the top n items on the stack match the n items on the right hand side of some production then they are all popped off the stack and the item on the left hand side of the production is pushed on the stack this replacement of the top n items with a single item is the reduce operation this operation may only be applied to the top of the stack reducing items lower in the stack must be done before later items are pushed onto the stack the parser finishes when all the input is consumed and there is only one item remaining on the stack a parse tree with an s node as its root the shift reduce parser builds a parse tree during the above process each time it pops n items off the stack it combines them into a partial parse tree and pushes this back on the stack we can see the shift reduce parsing algorithm in action using the graphical demonstration nltk app srparser six stages of the execution of this parser are shown in figure six stages of a shift reduce parser the parser begins by shifting the first input word onto its stack once the top items on the stack match the right hand side of a grammar production they can be replaced with the left hand side of that production the parser succeeds once all input is consumed and one s item remains on the stack nltk provides shiftreduceparser a simple implementation of a shift reduce parser this parser does not implement any backtracking so it is not guaranteed to find a parse for a text even if one exists furthermore it will only find at most one parse even if more parses exist we can provide an optional trace parameter that controls how verbosely the parser reports the steps that it takes as it parses a text note your turn run the above parser in tracing mode to see the sequence of shift and reduce operations using sr parse nltk shiftreduceparser grammar trace a shift reduce parser can reach a dead end and fail to find any parse even if the input sentence is well formed according to the grammar when this happens no input remains and the stack contains items which cannot be reduced to an s the problem arises because there are choices made earlier that cannot be undone by the parser although users of the graphical demonstration can undo their choices there are two kinds of choices to be made by the parser a which reduction to do when more than one is possible b whether to shift or reduce when either action is possible a shift reduce parser may be extended to implement policies for resolving such conflicts for emple it may address shift reduce conflicts by shifting only when no reductions are possible and it may address reduce reduce conflicts by favoring the reduction operation that removes the most items from the stack a generalization of shift reduce parser a lookahead lr parser is commonly used in programming language compilers the advantage of shift reduce parsers over recursive descent parsers is that they only build structure that corresponds to the words in the input furthermore they only build each sub structure once e g np det the n man is only built and pushed onto the stack a single time regardless of whether it will later be used by the vp v np pp reduction or the np np pp reduction the left corner parser one of the problems with the recursive descent parser is that it goes into an infinite loop when it encounters a left recursive production this is because it applies the grammar productions blindly without considering the actual input sentence a left corner parser is a hybrid between the bottom up and top down approaches we have seen grammar grammar allows us to produce the following parse of john saw mary recall that the grammar defined in has the following productions for expanding np suppose we ask you to first look at tree and then decide which of the np productions you would want a recursive descent parser to apply first obviously c is the right choice how do you know that it would be pointless to apply a or b instead because neither of these productions will derive a sequence whose first word is john that is we can easily tell that in a successful parse of john saw mary the parser has to expand np in such a way that np derives the sequence john more generally we say that a category b is a left corner of a tree rooted in a if a b a left corner parser is a top down parser with bottom up filtering unlike an ordinary recursive descent parser it does not get trapped in left recursive productions before starting its work a left corner parser preprocesses the context free grammar to build a table where each row contains two cells the first holding a non terminal and the second holding the collection of possible left corners of that non terminal illustrates this for the grammar from grammar table left corners in grammar each time a production is considered by the parser it checks that the next input word is compatible with at least one of the pre terminal categories in the left corner table well formed substring tables the simple parsers discussed above suffer from limitations in both completeness and efficiency in order to remedy these we will apply the algorithm design technique of dynamic programming to the parsing problem as we saw in dynamic programming stores intermediate results and re uses them when appropriate achieving significant efficiency gains this technique can be applied to syntactic parsing allowing us to store partial solutions to the parsing task and then look them up as necessary in order to efficiently arrive at a complete solution this approach to parsing is known as chart parsing we introduce the main idea in this section see the online materials available for this chapter for more implementation details dynamic programming allows us to build the pp in my pajamas just once the first time we build it we save it in a table then we look it up when we need to use it as a subconstituent of either the object np or the higher vp this table is known as a well formed substring table or wfst for short the term substring refers to a contiguous sequence of words within a sentence we will show how to construct the wfst bottom up so as to systematically record what syntactic constituents have been found let set our input to be the sentence in the numerically specified spans of the wfst are reminiscent of python slice notation another way to think about the data structure is shown in a data structure known as a chart figure the chart data structure words are the edge labels of a linear graph structure in a wfst we record the position of the words by filling in cells in a triangular matrix the vertical axis will denote the start position of a substring while the horizontal axis will denote the end position thus shot will appear in the cell with coordinates to simplify this presentation we will assume each word has a unique lexical category and we will store this not the word in the matrix so cell will contain the entry v more generally if our input string is a a an and our grammar contains a production of the form a ai then we add a to the cell i i system message warning ch rst line backlink inline interpreted text or phrase reference start string without end string so for every word in text we can look up in our grammar what category it belongs to for our wfst we create an n n matrix as a list of lists in python and initialize it with the lexical categories of each token in the init wfst function in we also define a utility function display to pretty print the wfst for us as expected there is a v in cell returning to our tabular representation given that we have det in cell for the word an and n in cell for the word elephant what should we put into cell for an elephant we need to find a production of the form a det n consulting the grammar we know that we can enter np in cell more generally we can enter a in i j if there is a production a b c and we find nonterminal b in i k and c in k j the program in uses this rule to complete the wfst by setting trace to true when calling the function complete wfst we see tracing output that shows the wfst being constructed for emple this says that since we found det at wfst and n at wfst we can add np to wfst note to help us easily retrieve productions by their right hand sides we create an index for the grammar this is an emple of a space time trade off we do a reverse lookup on the grammar instead of having to check through the entire list of productions each time we want to look up via the right hand side figure the chart data structure non terminals are represented as extra edges in the chart we conclude that there is a parse for the whole input string once we have constructed an s node in cell showing that we have found a sentence that covers the whole input the final state of the wfst is depicted in notice that we have not used any built in parsing functions here we have implemented a complete primitive chart parser from the ground up wfst have several shortcomings first as you can see the wfst is not itself a parse tree so the technique is strictly speaking recognizing that a sentence is admitted by a grammar rather than parsing it second it requires every non lexical grammar production to be binary although it is possible to convert an arbitrary cfg into this form we would prefer to use an approach without such a requirement third as a bottom up approach it is potentially wasteful being able to propose constituents in locations that would not be licensed by the grammar finally the wfst did not represent the structural ambiguity in the sentence i e the two verb phrase readings the vp in cell was actually entered twice once for a v np reading and once for a vp pp reading these are different hypotheses and the second overwrote the first as it happens this did not matter since the left hand side was the same chart parsers use a slighly richer data structure and some interesting algorithms to solve these problems see the further reading section at the end of this chapter for details note your turn try out the interactive chart parser application nltk app chartparser dependencies and dependency grammar phrase structure grammar is concerned with how words and sequences of words combine to form constituents a distinct and complementary approach dependency grammar focusses instead on how words relate to other words dependency is a binary asymmetric relation that holds between a head and its dependents the head of a sentence is usually taken to be the tensed verb and every other word is either dependent on the sentence head or connects to it through a path of dependencies a dependency representation is a labeled directed graph where the nodes are the lexical items and the labeled arcs represent dependency relations from heads to dependents illustrates a dependency graph where arrows point from heads to their dependents figure dependency structure arrows point from heads to their dependents labels indicate the grammatical function of the dependent as subject object or modifier the arcs in are labeled with the grammatical function that holds between a dependent and its head for emple i is the sbj subject of shot which is the head of the whole sentence and in is an nmod noun modifier of elephant in contrast to phrase structure grammar therefore dependency grammars can be used to directly express grammatical functions as a type of dependency here is one way of encoding a dependency grammar in nltk note that it only captures bare dependency information without specifying the type of dependency a dependency graph is projective if when all the words are written in linear order the edges can be drawn above the words without crossing this is equivalent to saying that a word and all its descendents dependents and dependents of its dependents etc form a contiguous sequence of words within the sentence is projective and we can parse many sentences in english using a projective dependency parser the next emple shows how groucho dep grammar provides an alternative approach to capturing the attachment ambiguity that we emined earlier with phrase structure grammar these bracketed dependency structures can also be displayed as trees where dependents are shown as children of their heads in languages with more flexible word order than english non projective dependencies are more frequent various criteria have been proposed for deciding what is the head h and what is the dependent d in a construction c some of the most important are the following h determines the distribution class of c or alternatively the external syntactic properties of c are due to h h determines the semantic type of c h is obligatory while d may be optional h selects d and determines whether it is obligatory or optional the morphological form of d is determined by h e g agreement or case government when we say in a phrase structure grammar that the immediate constituents of a pp are p and np we are implicitly appealing to the head dependent distinction a prepositional phrase is a phrase whose head is a preposition moreover the np is a dependent of p the same distinction carries over to the other types of phrase that we have discussed the key point to note here is that although phrase structure grammars seem very different from dependency grammars they implicitly embody a recognition of dependency relations while cfgs are not intended to directly capture dependencies more recent linguistic frameworks have increasingly adopted formalisms which combine aspects of both approaches valency and the lexicon let us take a closer look at verbs and their dependents the grammar in correctly generates emples like d these possibilities correspond to the following productions table vp productions and their lexical heads that is was can occur with a following adj saw can occur with a following np thought can occur with a following s and put can occur with a following np and pp the dependents adj np pp and s are often called complements of the respective verbs and there are strong constraints on what verbs can occur with what complements by contrast with d the word sequences in d are ill formed note with a little imagination it is possible to invent contexts in which unusual combinations of verbs and complements are interpretable however we assume that the above emples are to be interpreted in neutral contexts in the tradition of dependency grammar the verbs in are said to have different valencies valency restrictions are not just applicable to verbs but also to the other classes of heads within frameworks based on phrase structure grammar various techniques have been proposed for excluding the ungrammatical emples in d in a cfg we need some way of constraining grammar productions which expand vp so that verbs only co occur with their correct complements we can do this by dividing the class of verbs into subcategories each of which is associated with a different set of complements for emple transitive verbs such as chased and saw require a following np object complement that is they are subcategorized for np direct objects if we introduce a new category label for transitive verbs namely tv for transitive verb then we can use it in the following productions vp tv np tv chased saw now joe thought the bear is excluded since we have not listed thought as a tv but chatterer saw the bear is still allowed provides more emples of labels for verb subcategories table verb subcategories valency is a property of lexical items and we will discuss it further in complements are often contrasted with modifiers or adjuncts although both are kinds of dependent prepositional phrases adjectives and adverbs typically function as modifiers unlike complements modifiers are optional can often be iterated and are not selected for by heads in the same way as complements for emple the adverb really can be added as a modifer to all the sentence in d the structural ambiguity of pp attachment which we have illustrated in both phrase structure and dependency grammars corresponds semantically to an ambiguity in the scope of the modifier scaling up so far we have only considered toy grammars small grammars that illustrate the key aspects of parsing but there is an obvious question as to whether the approach can be scaled up to cover large corpora of natural languages how hard would it be to construct such a set of productions by hand in general the answer is very hard even if we allow ourselves to use various formal devices that give much more succinct representations of grammar productions it is still extremely difficult to keep control of the complex interactions between the many productions required to cover the major constructions of a language in other words it is hard to modularize grammars so that one portion can be developed independently of the other parts this in turn means that it is difficult to distribute the task of grammar writing across a team of linguists another difficulty is that as the grammar expands to cover a wider and wider range of constructions there is a corresponding increase in the number of analyses which are admitted for any one sentence in other words ambiguity increases with coverage despite these problems some large collaborative projects have achieved interesting and impressive results in developing rule based grammars for several languages emples are the lexical functional grammar lfg pargram project the head driven phrase structure grammar hpsg lingo matrix framework and the lexicalized tree adjoining grammar xtag project grammar development parsing builds trees over sentences according to a phrase structure grammar now all the emples we gave above only involved toy grammars containing a handful of productions what happens if we try to scale up this approach to deal with realistic corpora of language in this section we will see how to access treebanks and look at the challenge of developing broad coverage grammars treebanks and grammars the corpus module defines the treebank corpus reader which contains a sample of the penn treebank corpus we can use this data to help develop a grammar for emple the program in uses a simple filter to find verbs that take sentential complements assuming we already have a production of the form vp vs s this information enables us to identify particular verbs that would be included in the expansion of vs the prepositional phrase attachment corpus nltk corpus ppattach is another source of information about the valency of particular verbs here we illustrate a technique for mining this corpus it finds pairs of prepositional phrases where the preposition and noun are fixed but where the choice of verb determines whether the prepositional phrase is attached to the vp or to the np amongst the output lines of this program we find offer from group n rejected v received which indicates that received expects a separate pp complement attached to the vp while rejected does not as before we can use this information to help construct the grammar the nltk corpus collection includes data from the pe cross framework and cross domain parser evaluation shared task a collection of larger grammars has been prepared for the purpose of comparing different parsers which can be obtained by downloading the large grammars package e g python m nltk downloader large grammars the nltk corpus collection also includes a sample from the sinica treebank corpus consisting of parsed sentences drawn from the academia sinica balanced corpus of modern chinese let load and display one of the trees in this corpus pernicious ambiguity unfortunately as the coverage of the grammar increases and the length of the input sentences grows the number of parse trees grows rapidly in fact it grows at an astronomical rate let s explore this issue with the help of a simple emple the word fish is both a noun and a verb we can make up the sentence fish fish fish meaning fish like to fish for other fish try this with police if you prefer something more sensible here is a toy grammar for the fish sentences now we can try parsing a longer sentence fish fish fish fish fish which amongst other things means fish that other fish fish are in the habit of fishing fish themselves we use the nltk chart parser which was mentioned earlier in this chapter this sentence has two readings as the length of this sentence goes up we get the following numbers of parse trees these are the catalan numbers which we saw in an exercise in the last of these is for a sentence of length the average length of sentences in the wsj section of penn treebank for a sentence of length there would be over parses and this is only half the length of the piglet sentence which young children process effortlessly no practical nlp system could construct millions of trees for a sentence and choose the appropriate one in the context it is clear that humans do not do this either note that the problem is not with our choice of emple church patil point out that the syntactic ambiguity of pp attachment in sentences like also grows in proportion to the catalan numbers so much for structural ambiguity what about lexical ambiguity as soon as we try to construct a broad coverage grammar we are forced to make lexical entries highly ambiguous for their part of speech in a toy grammar a is only a determiner dog is only a noun and runs is only a verb however in a broad coverage grammar a is also a noun e g part a dog is also a verb meaning to follow closely and runs is also a noun e g ski runs in fact all words can be referred to by name e g the verb ate is spelled with three letters in speech we do not need to supply quotation marks furthermore it is possible to verb most nouns thus a parser for a broad coverage grammar will be overwhelmed with ambiguity even complete gibberish will often have a reading e g the a are of i as klavans resnik has pointed out this is not word salad but a grammatical noun phrase in which are is a noun meaning a hundredth of a hectare or sq m and a and i are nouns designating coordinates as shown in figure the a are of i a schematic drawing of paddocks each being one are in size and each identified using coordinates the top left cell is the a are of column i after abney even though this phrase is unlikely it is still grammatical and a broad coverage parser should be able to construct a parse tree for it similarly sentences that seem to be unambiguous such as john saw mary turn out to have other readings we would not have anticipated as abney explains this ambiguity is unavoidable and leads to horrendous inefficiency in parsing seemingly innocuous sentences the solution to these problems is provided by probabilistic parsing which allows us to rank the parses of an ambiguous sentence on the basis of evidence from corpora weighted grammar as we have just seen dealing with ambiguity is a key challenge in developing broad coverage parsers chart parsers improve the efficiency of computing multiple parses of the same sentences but they are still overwhelmed by the sheer number of possible parses weighted grammars and probabilistic parsing algorithms have provided an effective solution to these problems before looking at these we need to understand why the notion of grammaticality could be gradient considering the verb give this verb requires both a direct object the thing being given and an indirect object the recipient these complements can be given in either order as illustrated in in the prepositional dative form in a the direct object appears first followed by a prepositional phrase containing the indirect object in the double object form in b the indirect object appears first followed by the direct object in the above case either order is acceptable however if the indirect object is a pronoun there is a strong preference for the double object construction using the penn treebank sample we can emine all instances of prepositional dative and double object constructions involving give as shown in we can observe a strong tendency for the shortest complement to appear first however this does not account for a form like give np federal judges np a raise where animacy may play a role in fact there turn out to be a large number of contributing factors as surveyed by bresnan hay such preferences can be represented in a weighted grammar a probabilistic context free grammar or pcfg is a context free grammar that associates a probability with each of its productions it generates the same set of parses for a text that the corresponding context free grammar does and assigns a probability to each parse the probability of a parse generated by a pcfg is simply the product of the probabilities of the productions used to generate it the simplest way to define a pcfg is to load it from a specially formatted string consisting of a sequence of weighted productions where weights appear in brackets as shown in it is sometimes convenient to combine multiple productions into a single line e g vp tv np iv datv np np in order to ensure that the trees generated by the grammar form a probability distribution pcfg grammars impose the constraint that all productions with a given left hand side must have probabilities that sum to one the grammar in obeys this constraint for s there is only one production with a probability of for vp and for np the parse tree returned by parse includes probabilities now that parse trees are assigned probabilities it no longer matters that there may be a huge number of possible parses for a given sentence a parser will be responsible for finding the most likely parses summary sentences have internal organization that can be represented using a tree notable features of constituent structure are recursion heads complements and modifiers a grammar is a compact characterization of a potentially infinite set of sentences we say that a tree is well formed according to a grammar or that a grammar licenses a tree a grammar is a formal model for describing whether a given phrase can be assigned a particular constituent or dependency structure given a set of syntactic categories a context free grammar uses a set of productions to say how a phrase of some category a can be analyzed into a sequence of smaller parts n a dependency grammar uses productions to specify what the dependents are of a given lexical head syntactic ambiguity arises when one sentence has more than one syntactic analysis e g prepositional phrase attachment ambiguity a parser is a procedure for finding one or more trees corresponding to a grammatically well formed sentence a simple top down parser is the recursive descent parser which recursively expands the start symbol usually s with the help of the grammar productions and tries to match the input sentence this parser cannot handle left recursive productions e g productions such as np np pp it is inefficient in the way it blindly expands categories without checking whether they are compatible with the input string and in repeatedly expanding the same non terminals and discarding the results a simple bottom up parser is the shift reduce parser which shifts input onto a stack and tries to match the items at the top of the stack with the right hand side of grammar productions this parser is not guaranteed to find a valid parse for the input even if one exists and builds substructure without checking whether it is globally consistent with the grammar further reading extra materials for this chapter are posted at http nltk org including links to freely available resources on the web for more emples of parsing with nltk please see the parsing howto at http nltk org howto there are many introductory books on syntax o grady et al is a general introduction to linguistics while radford provides a gentle introduction to transformational grammar and can be recommended for its coverage of transformational approaches to unbounded dependency constructions the most widely used term in linguistics for formal grammar is generative grammar though it has nothing to do with generation chomsky the framework of x bar syntax is due to jacobs rosenbaum and is explored at greater length in jackendoff the primes we use replace chomsky typographically more demanding horizontal bars burton roberts is a practically oriented textbook on how to analyze constituency in english with extensive exemplification and exercises huddleston pullum provides an up to date and comprehensive analysis of syntactic phenomena in english chapter of jurafsky martin covers formal grammars of english sections cover simple parsing algorithms and techniques for dealing with ambiguity chapter covers statistical parsing chapter covers the chomsky hierarchy and the formal complexity of natural language levin has categorized english verbs into fine grained classes according to their syntactic properties there are several ongoing efforts to build large scale rule based grammars e g the lfg pargram project http www parc com istl groups nltt pargram the hpsg lingo matrix framework http www delph in net matrix and the xtag project http www cis upenn edu xtag exercises can you come up with grammatical sentences that have probably never been uttered before take turns with a partner what does this tell you about human language recall strunk and white s prohibition against sentence initial however used to mean although do a web search for however used at the start of the sentence how widely used is this construction consider the sentence kim arrived or dana left and everyone cheered write down the parenthesized forms to show the relative scope of and and or generate tree structures corresponding to both of these interpretations the tree class implements a variety of other useful methods see the tree help documentation for more details i e import the tree class and then type help tree in this exercise you will manually construct some parse trees write code to produce two trees one for each reading of the phrase old men and women encode any of the trees presented in this chapter as a labeled bracketing and use nltk tree to check that it is well formed now use draw to display the tree as in a above draw a tree for the woman saw a man last thursday write a recursive function to traverse a tree and return the depth of the tree such that a tree with a single node would have depth zero hint the depth of a subtree is the maximum depth of its children plus one analyze the a a milne sentence about piglet by underlining all of the sentences it contains then replacing these with s e g the first sentence becomes s when lx s draw a tree structure for this compressed sentence what are the main syntactic constructions used for building such a long sentence in the recursive descent parser demo experiment with changing the sentence to be parsed by selecting edit text in the edit menu can the grammar in grammar be used to describe sentences that are more than words in length use the graphical chart parser interface to experiment with different rule invocation strategies come up with your own strategy that you can execute manually using the graphical interface describe the steps and report any efficiency improvements it has e g in terms of the size of the resulting chart do these improvements depend on the structure of the grammar what do you think of the prospects for significant performance boosts from cleverer rule invocation strategies with pen and paper manually trace the execution of a recursive descent parser and a shift reduce parser for a cfg you have already seen or one of your own devising we have seen that a chart parser adds but never removes edges from a chart why consider the sequence of words buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo this is a grammatically correct sentence as explained at http en wikipedia org wiki buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo consider the tree diagram presented on this wikipedia page and write down a suitable grammar normalize case to lowercase to simulate the problem that a listener has when hearing this sentence can you find other parses for this sentence how does the number of parse trees grow as the sentence gets longer more emples of these sentences can be found at http en wikipedia org wiki list of homophonous phrases you can modify the grammar in the recursive descent parser demo by selecting edit grammar in the edit menu change the second expansion production namely np det n pp to np np pp using the step button try to build a parse tree what happens extend the grammar in grammar with productions that expand prepositions as intransitive transitive and requiring a pp complement based on these productions use the method of the preceding exercise to draw a tree for the sentence lee ran away home pick some common verbs and complete the following tasks write a program to find those verbs in the prepositional phrase attachment corpus nltk corpus ppattach find any cases where the same verb exhibits two different attachments but where the first noun or second noun or preposition stay unchanged as we saw in our discussion of syntactic ambiguity in devise cfg grammar productions to cover some of these cases write a program to compare the efficiency of a top down chart parser compared with a recursive descent parser use the same grammar and input sentences for both compare their performance using the timeit module see for an emple of how to do this compare the performance of the top down bottom up and left corner parsers using the same grammar and three grammatical test sentences use timeit to log the amount of time each parser takes on the same sentence write a function that runs all three parsers on all three sentences and prints a by grid of times as well as row and column totals discuss your findings read up on garden path sentences how might the computational work of a parser relate to the difficulty humans have with processing these sentences http en wikipedia org wiki garden path sentence to compare multiple trees in a single window we can use the draw trees method define some trees and try it out using tree positions list the subjects of the first sentences in the penn treebank to make the results easier to view limit the extracted subjects to subtrees whose height is inspect the prepositional phrase attachment corpus and try to suggest some factors that influence pp attachment in this section we claimed that there are linguistic regularities that cannot be described simply in terms of n grams consider the following sentence particularly the position of the phrase in his turn does this illustrate a problem for an approach based on n grams what was more the in his turn somewhat youngish nikolay parfenovich also turned out to be the only person in the entire world to acquire a sincere liking to our discriminated against public procurator dostoevsky the brothers karamazov write a recursive function that produces a nested bracketing for a tree leaving out the leaf nodes and displaying the non terminal labels after their subtrees so the above emple about pierre vinken would produce nnp nnp np adjp cd nns np jj adjp np sbj md vb dt nn np in dt jj nn np pp clr nnp cd np tmp vp s consecutive categories should be separated by space download several electronic books from project gutenberg write a program to scan these texts for any extremely long sentences what is the longest sentence you can find what syntactic construction s are responsible for such long sentences modify the functions init wfst and complete wfst so that the contents of each cell in the wfst is a set of non terminal symbols rather than a single non terminal consider the algorithm in can you explain why parsing context free grammar is proportional to n where n is the length of the input sentence process each tree of the treebank corpus sample nltk corpus treebank and extract the productions with the help of tree productions discard the productions that occur only once productions with the same left hand side and similar right hand sides can be collapsed resulting in an equivalent but more compact set of rules write code to output a compact grammar one common way of defining the subject of a sentence s in english is as the noun phrase that is the child of s and the sibling of vp write a function that takes the tree for a sentence and returns the subtree corresponding to the subject of the sentence what should it do if the root node of the tree passed to this function is not s or it lacks a subject write a function that takes a grammar such as the one defined in and returns a random sentence generated by the grammar use grammar start to find the start symbol of the grammar grammar productions lhs to get the list of productions from the grammar that have the specified left hand side and production rhs to get the right hand side of a production implement a version of the shift reduce parser using backtracking so that it finds all possible parses for a sentence what might be called a recursive ascent parser consult the wikipedia entry for backtracking at http en wikipedia org wiki backtracking as we saw in it is possible to collapse chunks down to their chunk label when we do this for sentences involving the word gave we find patterns such as the following gave np gave up np in np gave np up gave np np gave np to np use this method to study the complementation patterns of a verb of interest and write suitable grammar productions this task is sometimes called lexical acquisition identify some english verbs that are near synonyms such as the dumped filled loaded emple from earlier in this chapter use the chunking method to study the complementation patterns of these verbs create a grammar to cover these cases can the verbs be freely substituted for each other or are their constraints discuss your findings develop a left corner parser based on the recursive descent parser and inheriting from parsei extend nltk shift reduce parser to incorporate backtracking so that it is guaranteed to find all parses that exist i e it is complete modify the functions init wfst and complete wfst so that when a non terminal symbol is added to a cell in the wfst it includes a record of the cells from which it was derived implement a function that will convert a wfst in this form to a parse tree building feature based grammars natural languages have an extensive range of grammatical constructions which are hard to handle with the simple methods described in in order to gain more flexibility we change our treatment of grammatical categories like s np and v in place of atomic labels we decompose them into structures like dictionaries where features can take on a range of values the goal of this chapter is to answer the following questions how can we extend the framework of context free grammars with features so as to gain more fine grained control over grammatical categories and productions what are the main formal properties of feature structures and how do we use them computationally what kinds of linguistic patterns and grammatical constructions can we now capture with feature based grammars along the way we will cover more topics in english syntax including phenomena such as agreement subcategorization and unbounded dependency constructions grammatical features in chap data intensive we described how to build classifiers that rely on detecting features of text such features may be quite simple such as extracting the last letter of a word or more complex such as a part of speech tag which has itself been predicted by the classifier in this chapter we will investigate the role of features in building rule based grammars in contrast to feature extractors which record features that have been automatically detected we are now going to declare the features of words and phrases we start off with a very simple emple using dictionaries to store features and their values the objects kim and chase both have a couple of shared features cat grammatical category and orth orthography i e spelling in addition each has a more semantically oriented feature kim ref is intended to give the referent of kim while chase rel gives the relation expressed by chase in the context of rule based grammars such pairings of features and values are known as feature structures and we will shortly see alternative notations for them feature structures contain various kinds of information about grammatical entities the information need not be exhaustive and we might want to add further properties for emple in the case of a verb it is often useful to know what semantic role is played by the arguments of the verb in the case of chase the subject plays the role of agent while the object has the role of patient let s add this information using sbj and obj as placeholders which will get filled once the verb combines with its grammatical arguments if we now process a sentence kim chased lee we want to bind the verb s agent role to the subject and the patient role to the object we do this by linking to the ref feature of the relevant np in the following emple we make the simple minded assumption that the nps immediately to the left and right of the verb are the subject and object respectively we also add a feature structure for lee to complete the emple the same approach could be adopted for a different verb say surprise though in this case the subject would play the role of source src and the object the role of experiencer exp feature structures are pretty powerful but the way in which we have manipulated them is extremely ad hoc our next task in this chapter is to show how the framework of context free grammar and parsing can be expanded to accommodate feature structures so that we can build analyses like this in a more generic and principled way we will start off by looking at the phenomenon of syntactic agreement we will show how agreement constraints can be expressed elegantly using features and illustrate their use in a simple grammar since feature structures are a general data structure for representing information of any kind we will briefly look at them from a more formal point of view and illustrate the support for feature structures offered by nltk in the final part of the chapter we demonstrate that the additional expressiveness of features opens up a wide spectrum of possibilities for describing sophisticated aspects of linguistic structure syntactic agreement the following emples show pairs of word sequences the first of which is grammatical and the second not we use an asterisk at the start of a word sequence to signal that it is ungrammatical in english nouns are usually marked as being singular or plural the form of the demonstrative also varies this singular and these plural emples b and b show that there are constraints on the use of demonstratives and nouns within a noun phrase either both are singular or both are plural a similar constraint holds between subjects and predicates here we can see that morphological properties of the verb co vary with syntactic properties of the subject noun phrase this co variance is called agreement if we look further at verb agreement in english we will see that present tense verbs typically have two inflected forms one for third person singular and another for every other combination of person and number as shown in table agreement paradigm for english regular verbs we can make the role of morphological properties a bit more explicit as illustrated in ex runs and ex run these representations indicate that the verb agrees with its subject in person and number we use as an abbreviation for rd person sg for singular and pl for plural system message error ch rst line error in gloss directive may contain a single table only system message error ch rst line error in gloss directive may contain a single table only let see what happens when we encode these agreement constraints in a context free grammar we will begin with the simple cfg in grammar allows us to generate the sentence this dog runs however what we really want to do is also generate these dogs run while blocking unwanted sequences like this dogs run and these dog runs the most straightforward approach is to add new non terminals and productions to the grammar in place of a single production expanding s we now have two productions one covering the sentences involving singular subject nps and vps the other covering sentences with plural subject nps and vps in fact every production in has two counterparts in with a small grammar this is not really such a problem although it is aesthetically unappealing however with a larger grammar that covers a reasonable subset of english constructions the prospect of doubling the grammar size is very unattractive let s suppose now that we used the same approach to deal with first second and third person agreement for both singular and plural this would lead to the original grammar being multiplied by a factor of which we definitely want to avoid can we do better than this in the next section we will show that capturing number and person agreement need not come at the cost of blowing up the number of productions using attributes and constraints we spoke informally of linguistic categories having properties for emple that a noun has the property of being plural let make this explicit in we have introduced some new notation which says that the category n has a grammatical feature called num short for number and that the value of this feature is pl short for plural we can add similar annotations to other categories and use them in lexical entries does this help at all so far it looks just like a slightly more verbose alternative to what was specified in things become more interesting when we allow variables over feature values and use these to state constraints we are using n as a variable over values of num it can be instantiated either to sg or pl within a given production we can read the first production as saying that whatever value np takes for the feature num vp must take the same value in order to understand how these feature constraints work it is helpful to think about how one would go about building a tree lexical productions will admit the following local trees trees of depth one now s np num n vp num n says that whatever the num values of n and det are they have to be the same consequently np num n det num n n num n will permit a and a to be combined into an np as shown in a and it will also allow b and b to be combined as in b by contrast a and b are prohibited because the roots of their subtrees differ in their values for the num feature this incompatibility of values is indicated informally with a fail value at the top node production vp num n v num n says that the num value of the head verb has to be the same as the num value of the vp parent combined with the production for expanding s we derive the consequence that if the num value of the subject head noun is pl then so is the num value of the vp head verb grammar illustrated lexical productions for determiners like this and these which require a singular or plural head noun respectively however other determiners in english are not choosy about the grammatical number of the noun they combine with one way of describing this would be to add two lexical entries to the grammar one each for the singular and plural versions of determiner such as the det num sg the some any det num pl the some any however a more elegant solution is to leave the num value underspecified and letting it agree in number with whatever noun it combines with assigning a variable value to num is one way of achieving this result det num n the some any but in fact we can be even more economical and just omit any specification for num in such productions we only need to explicitly enter a variable value when this constrains another value elsewhere in the same production the grammar in illustrates most of the ideas we have introduced so far in this chapter plus a couple of new ones notice that a syntactic category can have more than one feature for emple v tense pres num pl in general we can add as many features as we like a final detail about is the statement start s this directive tells the parser to take s as the start symbol for the grammar in general when we are trying to develop even a very small grammar it is convenient to put the productions in a file where they can be edited tested and revised we have saved as a file named feat fcfg in the nltk data distribution you can make your own copy of this for further experimentation using nltk data load illustrates the operation of a chart parser with a feature based grammar after tokenizing the input we import the load parser function which takes a grammar filename as input and returns a chart parser cp calling the parser parse method will iterate over the resulting parse trees trees will be empty if the grammar fails to parse the input and will contain one or more parse trees depending on whether the input is syntactically ambiguous or not the details of the parsing procedure are not that important for present purposes however there is an implementation issue which bears on our earlier discussion of grammar size one possible approach to parsing productions containing feature constraints is to compile out all admissible values of the features in question so that we end up with a large fully specified cfg along the lines of by contrast the parser process illustrated above works directly with the underspecified productions given by the grammar feature values flow upwards from lexical entries and variable values are then associated with those values via bindings i e dictionaries such as n sg t pres as the parser assembles information about the nodes of the tree it is building these variable bindings are used to instantiate values in these nodes thus the underspecified vp num n tense t tv num n tense t np becomes instantiated as vp num sg tense pres tv num sg tense pres np by looking up the values of n and t in the bindings finally we can inspect the resulting parse trees in this case a single one terminology so far we have only seen feature values like sg and pl these simple values are usually called atomic that is they ca not be decomposed into subparts a special case of atomic values are boolean values that is values that just specify whether a property is true or false for emple we might want to distinguish auxiliary verbs such as can may will and do with the boolean feature aux for emple the production v tense pres aux can means that can receives the value pres for tense and or true for aux there is a widely adopted convention which abbreviates the representation of boolean features f instead of aux or aux we use aux and aux respectively these are just abbreviations however and the parser interprets them as though and are like any other atomic value shows some representative productions we have spoken of attaching feature annotations to syntactic categories a more radical approach represents the whole category that is the non terminal symbol plus the annotation as a bundle of features for emple n num sg contains part of speech information which can be represented as pos n an alternative notation for this category therefore is pos n num sg in addition to atomic valued features features may take values that are themselves feature structures for emple we can group together agreement features e g person number and gender as a distinguished part of a category grouped together as the value of agr in this case we say that agr has a complex value depicts the structure in a format known as an attribute value matrix avm figure rendering a feature structure as an attribute value matrix in passing we should point out that there are alternative approaches for displaying avms shows an emple athough feature structures rendered in the style of are less visually pleasing we will stick with this format since it corresponds to the output we will be getting from nltk on the topic of representation we also note that feature structures like dictionaries assign no particular significance to the order of features so is equivalent to once we have the possibility of using features like agr we can refactor a grammar like so that agreement features are bundled together a tiny grammar illustrating this idea is shown in processing feature structures in this section we will show how feature structures can be constructed and manipulated in nltk we will also discuss the fundamental operation of unification which allows us to combine the information contained in two different feature structures feature structures in nltk are declared with the featstruct constructor atomic feature values can be strings or integers a feature structure is actually just a kind of dictionary and so we access its values by indexing in the usual way we can use our familiar syntax to assign values to features we can also define feature structures that have complex values as discussed earlier an alternative method of specifying feature structures is to use a bracketed string consisting of feature value pairs in the format feature value where values may themselves be feature structures feature structures are not inherently tied to linguistic objects they are general purpose structures for representing knowledge for emple we could encode information about a person in a feature structure in the next couple of pages we are going to use emples like this to explore standard operations over feature structures this will briefly divert us from processing natural language but we need to lay the groundwork before we can get back to talking about grammars hang on tight it is often helpful to view feature structures as graphs more specifically directed acyclic graphs dags is equivalent to the above avm the feature names appear as labels on the directed arcs and feature values appear as labels on the nodes that are pointed to by the arcs just as before feature values can be complex when we look at such graphs it is natural to think in terms of paths through the graph a feature path is a sequence of arcs that can be followed from the root node we will represent paths as tuples thus address street is a feature path whose value in is the node labeled rue pascal now let consider a situation where lee has a spouse named kim and kim address is the same as lee we might represent this as however rather than repeating the address information in the feature structure we can share the same sub graph between different arcs in other words the value of the path address in is identical to the value of the path spouse address dags such as are said to involve structure sharing or reentrancy when two paths have the same value they are said to be equivalent in order to indicate reentrancy in our matrix style representations we will prefix the first occurrence of a shared feature structure with an integer in parentheses such as any later reference to that structure will use the notation as shown below the bracketed integer is sometimes called a tag or a coindex the choice of integer is not significant there can be any number of tags within a single feature structure if we unify two feature structures which stand in the subsumption relationship then the result of unification is the most informative of the two for emple the result of unifying b with c is c unification between fs and fs will fail if the two feature structures share a path but the value of in fs is a distinct atom from the value of in fs this is implemented by setting the result of unification to be none now if we look at how unification interacts with structure sharing things become really interesting first let define in python what happens when we augment kim address with a specification for city notice that fs needs to include the whole path from the root of the feature structure down to city by contrast the result is very different if fs is unified with the structure sharing version fs also shown earlier as the graph rather than just updating what was in effect kim s copy of lee s address we have now updated both their addresses at the same time more generally if a unification adds information to the value of some path then that unification simultaneously updates the value of any path that is equivalent to as we have already seen structure sharing can also be stated using variables such as x extending a feature based grammar in this section we return to feature based grammar and explore a variety of linguistic issues and demonstrate the benefits of incorporating features into the grammar subcategorization in we augmented our category labels to represent different kinds of verb and used the labels iv and tv for intransitive and transitive verbs respectively this allowed us to write productions like the following although we know that iv and tv are two kinds of v they are just atomic nonterminal symbols from a cfg as distinct from each other as any other pair of symbols this notation doesn t let us say anything about verbs in general e g we cannot say all lexical items of category v can be marked for tense since walk say is an item of category iv not v so can we replace category labels such as tv and iv by v along with a feature that tells us whether the verb combines with a following np object or whether it can occur without any complement a simple approach originally developed for a grammar framework called generalized phrase structure grammar gpsg tries to solve this problem by allowing lexical categories to bear a subcat which tells us what subcategorization class the item belongs to while gpsg used integer values for subcat the emple below adopts more mnemonic values namely intrans trans and clause when we see a lexical category like v subcat trans we can interpret the subcat specification as a pointer to a production in which v subcat trans is introduced as the head child in a vp production by convention there is a correspondence between the values of subcat and the productions that introduce lexical heads on this approach subcat can only appear on lexical categories it makes no sense for emple to specify a subcat value on vp as required walk and like both belong to the category v nevertheless walk will only occur in vps expanded by a production with the feature subcat intrans on the right hand side as opposed to like which requires a subcat trans in our third class of verbs above we have specified a category sbar this is a label for subordinate clauses such as the complement of claim in the emple you claim that you like children we require two further productions to analyze such sentences the resulting structure is the following an alternative treatment of subcategorization due originally to a framework known as categorial grammar is represented in feature based frameworks such as patr and head driven phrase structure grammar rather than using subcat values as a way of indexing productions the subcat value directly encodes the valency of a head the list of arguments that it can combine with for emple a verb like put that takes np and pp complements put the book on the table might be represented as this says that the verb can combine with three arguments the leftmost element in the list is the subject np while everything else an np followed by a pp in this case comprises the subcategorized for complements when a verb like put is combined with appropriate complements the requirements which are specified in the subcat are discharged and only a subject np is needed this category which corresponds to what is traditionally thought of as vp might be represented as follows finally a sentence is a kind of verbal category that has no requirements for further arguments and hence has a subcat whose value is the empty list the tree shows how these category assignments combine in a parse of kim put the book on the table heads revisited we noted in the previous section that by factoring subcategorization information out of the main category label we could express more generalizations about properties of verbs another property of this kind is the following expressions of category v are heads of phrases of category vp similarly ns are heads of nps as i e adjectives are heads of aps and ps i e prepositions are heads of pps not all phrases have heads for emple it is standard to say that coordinate phrases e g the book and the bell lack heads nevertheless we would like our grammar formalism to express the parent head child relation where it holds at present v and vp are just atomic symbols and we need to find a way to relate them using features as we did earlier to relate iv and tv x bar syntax addresses this issue by abstracting out the notion of phrasal level it is usual to recognize three such levels if n represents the lexical level then n represents the next level up corresponding to the more traditional category nom while n represents the phrasal level corresponding to the category np a illustrates a representative structure while b is the more conventional counterpart the head of the structure a is n while n and n are called phrasal projections of n n is the maximal projection and n is sometimes called the zero projection one of the central claims of x bar syntax is that all constituents share a structural similarity using x as a variable over n v a and p we say that directly subcategorized complements of a lexical head x are always placed as siblings of the head whereas adjuncts are placed as siblings of the intermediate category x thus the configuration of the two p adjuncts in contrasts with that of the complement p in a the productions in illustrate how bar levels can be encoded using feature structures the nested structure in is achieved by two applications of the recursive rule expanding n bar auxiliary verbs and inversion inverted clauses where the order of subject and verb is switched occur in english interrogatives and also after negative adverbs however we cannot place just any verb in pre subject position verbs that can be positioned initially in inverted clauses belong to the class known as auxiliaries and as well as do can and have include be will and shall one way of capturing such structures is with the following production that is a clause marked as inv consists of an auxiliary verb followed by a vp in a more detailed grammar we would need to place some constraints on the form of the vp depending on the choice of auxiliary illustrates the structure of an inverted clause unbounded dependency constructions consider the following contrasts the verb like requires an np complement while put requires both a following np and pp and show that these complements are obligatory omitting them leads to ungrammaticality yet there are contexts in which obligatory complements can be omitted as and illustrate that is an obligatory complement can be omitted if there is an appropriate filler in the sentence such as the question word who in a the preposed topic this music in b or the wh phrases which card slot in it is common to say that sentences like contain gaps where the obligatory complements have been omitted and these gaps are sometimes made explicit using an underscore so a gap can occur if it is licensed by a filler conversely fillers can only occur if there is an appropriate gap elsewhere in the sentence as shown by the following emples the mutual co occurence between filler and gap is sometimes termed a dependency one issue of considerable importance in theoretical linguistics has been the nature of the material that can intervene between a filler and the gap that it licenses in particular can we simply list a finite set of sequences that separate the two the answer is no there is no upper bound on the distance between filler and gap this fact can be easily illustrated with constructions involving sentential complements as shown in since we can have indefinitely deep recursion of sentential complements the gap can be embedded indefinitely far inside the whole sentence this constellation of properties leads to the notion of an unbounded dependency construction that is a filler gap dependency where there is no upper bound on the distance between filler and gap a variety of mechanisms have been suggested for handling unbounded dependencies in formal grammars here we illustrate the approach due to generalized phrase structure grammar that involves slash categories a slash category has the form y xp we interpret this as a phrase of category y that is missing a sub constituent of category xp for emple s np is an s that is missing an np the use of slash categories is illustrated in the top part of the tree introduces the filler who treated as an expression of category np wh together with a corresponding gap containing constituent s np the gap information is then percolated down the tree via the vp np category until it reaches the category np np at this point the dependency is discharged by realizing the gap information as the empty string immediately dominated by np np do we need to think of slash categories as a completely new kind of object fortunately we can accommodate them within our existing feature based framework by treating slash as a feature and the category to its right as a value that is s np is reducible to s slash np in practice this is also how the parser interprets slash categories the grammar shown in illustrates the main principles of slash categories and also includes productions for inverted clauses to simplify presentation we have omitted any specification of tense on the verbs the grammar in contains one gap introduction production namely s inv np s np in order to percolate the slash feature correctly we need to add slashes with variable values to both sides of the arrow in productions that expand s vp and np for emple vp x v sbar x is the slashed version of vp v sbar and says that a slash value can be specified on the vp parent of a constituent if the same value is also specified on the sbar child finally np np allows the slash information on np to be discharged as the empty string using we can parse the sequence who do you claim that you like a more readable version of this tree is shown in the grammar in will also allow us to parse sentences without gaps in addition it admits inverted sentences which do not involve wh constructions case and gender in german compared with english german has a relatively rich morphology for agreement for emple the definite article in german varies with case gender and number as shown in table morphological paradigm for the german definite article subjects in german take the nominative case and most verbs govern their objects in the accusative case however there are exceptions like helfen that govern the dative case the grammar in illustrates the interaction of agreement comprising person number and gender with case as you can see the feature objcase is used to specify the case that a verb governs on its object the next emple illustrates the parse tree for a sentence containing a verb which governs dative case in developing grammars excluding ungrammatical word sequences is often as challenging as parsing grammatical ones in order to get an idea where and why a sequence fails to parse setting the trace parameter of the load parser method can be crucial consider the following parse failure the last two scanner lines in the trace show that den is recognized as admitting two possible categories det agr gnd masc num sg per case acc and det agr num pl per case dat we know from the grammar in that katze has category n agr gnd fem num sg per thus there is no binding for the variable a in production np case c agr a det case c agr a n case c agr a which will satisfy these constraints since the agr value of katze will not unify with either of the agr values of den that is with either gnd masc num sg per or num pl per summary the traditional categories of context free grammar are atomic symbols an important motivation for feature structures is to capture fine grained distinctions that would otherwise require a massive multiplication of atomic categories by using variables over feature values we can express constraints in grammar productions that allow the realization of different feature specifications to be inter dependent typically we specify fixed values of features at the lexical level and constrain the values of features in phrases to unify with the corresponding values in their children feature values are either atomic or complex a particular sub case of atomic value is the boolean value represented by convention as f two features can share a value either atomic or complex structures with shared values are said to be re entrant shared values are represented by numerical indexes or tags in avms a path in a feature structure is a tuple of features corresponding to the labels on a sequence of arcs from the root of the graph representation two paths are equivalent if they share a value feature structures are partially ordered by subsumption fs subsumes fs when all the information contained in fs is also present in fs the unification of two structures fs and fs if successful is the feature structure fs that contains the combined information of both fs and fs if unification adds information to a path in fs then it also adds information to every path equivalent to we can use feature structures to build succinct analyses of a wide variety of linguistic phenomena including verb subcategorization inversion constructions unbounded dependency constructions and case government further reading please consult http nltk org for further materials on this chapter including feature structures feature grammars and grammar test suites x bar syntax jacobs rosenbaum jackendoff the primes we use replace chomsky typographically more demanding horizontal bars for an excellent introduction to the phenomenon of agreement see corbett the earliest use of features in theoretical linguistics was designed to capture phonological properties of phonemes for emple a sound like b might be decomposed into the structure labial voice an important motivation was to capture generalizations across classes of segments for emple that n gets realized as m preceding any labial consonant within chomskyan grammar it was standard to use atomic features for phenomena like agreement and also to capture generalizations across syntactic categories by analogy with phonology a radical expansion of the use of features in theoretical syntax was advocated by generalized phrase structure grammar gpsg gazdar klein and particularly in the use of features with complex values coming more from the perspective of computational linguistics dahl saint dizier proposed that functional aspects of language could be captured by unification of attribute value structures and a similar approach was elaborated by grosz stickel within the patr ii formalism early work in lexical functional grammar lfg bresnan introduced the notion of an f structure that was primarily intended to represent the grammatical relations and predicate argument structure associated with a constituent structure parse shieber provides an excellent introduction to this phase of research into feature based grammars one conceptual difficulty with algebraic approaches to feature structures arose when researchers attempted to model negation an alternative perspective pioneered by kasper rounds and johnson argues that grammars involve descriptions of feature structures rather than the structures themselves these descriptions are combined using logical operations such as conjunction and negation is just the usual logical operation over feature descriptions this description oriented perspective was integral to lfg from the outset cf huang chen and was also adopted by later versions of head driven phrase structure grammar hpsg sag wasow a comprehensive bibliography of hpsg literature can be found at http www cl uni bremen de hpsg bib feature structures as presented in this chapter are unable to capture important constraints on linguistic information for emple there is no way of saying that the only permissible values for num are sg and pl while a specification such as num masc is anomalous similarly we cannot say that the complex value of agr must contain specifications for the features per num and gnd but cannot contain a specification such as subcat trans typed feature structures were developed to remedy this deficiency to begin with we stipulate that feature values are always typed in the case of atomic values the values just are types for emple we would say that the value of num is the type num moreover num is the most general type of value for num since types are organized hierarchically we can be more informative by specifying the value of num is a subtype of num namely either sg or pl in the case of complex values we say that feature structures are themselves typed so for emple the value of agr will be a feature structure of type agr we also stipulate that all and only per num and gnd are appropriate features for a structure of type agr a good early review of work on typed feature structures is emele zajac a more comprehensive emination of the formal foundations can be found in carpenter while copestake focuses on implementing an hpsg oriented approach to typed feature structures there is a copious literature on the analysis of german within feature based grammar frameworks nerbonne netter pollard is a good starting point for the hpsg literature on this topic while m u ller gives a very extensive and detailed analysis of german syntax in hpsg chapter of jurafsky martin discusses feature structures the unification algorithm and the integration of unification into parsing algorithms exercises what constraints are required to correctly parse word sequences like i am happy and she is happy but not you is happy or they am happy implement two solutions for the present tense paradigm of the verb be in english first taking grammar as your starting point and then taking grammar as the starting point develop a variant of grammar in that uses a feature count to make the distinctions shown below write a function subsumes which holds of two feature structures fs and fs just in case fs subsumes fs modify the grammar illustrated in to incorporate a bar feature for dealing with phrasal projections modify the german grammar in to incorporate the treatment of subcategorization presented in develop a feature based grammar that will correctly describe the following spanish noun phrases system message error ch rst line error in gloss directive may contain a single table only system message error ch rst line error in gloss directive may contain a single table only system message error ch rst line error in gloss directive may contain a single table only system message error ch rst line error in gloss directive may contain a single table only develop your own version of the earleychartparser which only prints a trace if the input sequence fails to parse consider the feature structures shown in work out on paper what the result is of the following unifications hint you might find it useful to draw the graph structures fs and fs fs and fs fs and fs fs and fs fs and fs fs and fs fs and fs check your answers using python list two feature structures that subsume a x b x ignoring structure sharing give an informal algorithm for unifying two feature structures extend the german grammar in so that it can handle so called verb second structures like the following seemingly synonymous verbs have slightly different syntactic properties levin consider the patterns of grammaticality for the verbs loaded filled and dumped below can you write grammar productions to handle such data morphological paradigms are rarely completely regular in the sense of every cell in the matrix having a different realization for emple the present tense conjugation of the lexeme walk only has two distinct forms walks for the rd person singular and walk for all other combinations of person and number a successful analysis should not require redundantly specifying that out of the possible morphological combinations have the same realization propose and implement a method for dealing with this so called head features are shared between the parent node and head child for emple tense is a head feature that is shared between a vp and its head v child see gazdar klein and for more details most of the features we have looked at are head features exceptions are subcat and slash since the sharing of head features is predictable it should not need to be stated explicitly in the grammar productions develop an approach that automatically accounts for this regular behavior of head features extend nltk treatment of feature structures to allow unification into list valued features and use this to implement an hpsg style analysis of subcategorization whereby the subcat of a head category is the concatenation its complements categories with the subcat value of its immediate parent extend nltk treatment of feature structures to allow productions with underspecified categories such as s inv x s x extend nltk treatment of feature structures to allow typed feature structures pick some grammatical constructions described in huddleston pullum and develop a feature based grammar to account for them analyzing the meaning of sentences we have seen how useful it is to harness the power of a computer to process text on a large scale however now that we have the machinery of parsers and feature based grammars can we do anything similarly useful by analyzing the meaning of sentences the goal of this chapter is to answer the following questions how can we represent natural language meaning so that a computer can process these representations how can we associate meaning representations with an unlimited set of sentences how can we use programs that connect the meaning representations of sentences to stores of knowledge along the way we will learn some formal techniques in the field of logical semantics and see how these can be used for interrogating databases that store facts about the world natural language understanding querying a database suppose we have a program that lets us type in a natural language question and gives us back the right answer how hard is it to write such a program and can we just use the same techniques that we have encountered so far in this book or does it involve something new in this section we will show that solving the task in a restricted domain is pretty straightforward but we will also see that to address the problem in a more general way we have to open up a whole new box of ideas and techniques involving the representation of meaning so let start off by assuming that we have data about cities and countries in a structured form to be concrete we will use a database table whose first few rows are shown in note the data illustrated in is drawn from the chat system warren pereira population figures are given in thousands but note that the data used in these emples dates back at least to the s and was already somewhat out of date at the point when warren pereira was published table city table a table of cities countries and populations the obvious way to retrieve answers from this tabular data involves writing queries in a database query language such as sql note sql structured query language is a language designed for retrieving and managing data in relational databases if you want to find out more about sql http www w schools com sql is a convenient online reference for emple executing the query will pull out the value greece this specifies a result set consisting of all values for the column country in data rows where the value of the city column is athens how can we get the same effect using english as our input to the query system the feature based grammar formalism described in makes it easy to translate from english to sql the grammar sql fcfg illustrates how to assemble a meaning representation for a sentence in tandem with parsing the sentence each phrase structure rule is supplemented with a recipe for constructing a value for the feature sem you can see that these recipes are extremely simple in each case we use the string concatenation operation to splice the values for the child constituents to make a value for the parent constituent this allows us to parse a query into sql note your turn run the parser with maximum tracing on i e cp load parser grammars book grammars sql fcfg trace and emine how the values of sem are built up as complete edges are added to the chart finally we execute the query over the database city db and retrieve some results since each row r is a one element tuple we print out the member of the tuple rather than tuple itself to summarize we have defined a task where the computer returns useful data in response to a natural language query and we implemented this by translating a small subset of english into sql we can say that our nltk code already understands sql given that python is able to execute sql queries against a database and by extension it also understands queries such as what cities are located in china this parallels being able to translate from dutch into english as an emple of natural language understanding suppose that you are a native speaker of english and have started to learn dutch your teacher asks if you understand what means if you know the meanings of the individual words in and know how these meanings are combined to make up the meaning of the whole sentence you might say that means the same as margrietje loves brunoke an observer let call her olga might well take this as evidence that you do grasp the meaning of but this would depend on olga herself understanding english if she does not then your translation from dutch to english is not going to convince her of your ability to understand dutch we will return to this issue shortly the grammar sql fcfg together with the nltk earley parser is instrumental in carrying out the translation from english to sql how adequate is this grammar you saw that the sql translation for the whole sentence was built up from the translations of the components however there does not seem to be a lot of justification for these component meaning representations for emple if we look at the analysis of the noun phrase which cities the determiner and noun correspond respectively to the sql fragments select and city from city table but neither of these have a well defined meaning in isolation from the other there is another criticism we can level at the grammar we have hard wired an embarrassing amount of detail about the database into it we need to know the name of the relevant table e g city table and the names of the fields but our database could have contained ectly the same rows of data yet used a different table name and different field names in which case the sql queries would not be executable equally we could have stored our data in a different format such as xml in which case retrieving the same results would require us to translate our english queries into an xml query language rather than sql these considerations suggest that we should be translating english into something that is more abstract and generic than sql in order to sharpen the point let consider another english query and its translation note your turn extend the grammar sql fcfg so that it will translate a into b and check the values returned by the query you will probably find it easiest to first extend the grammar to handle queries like what cities have populations above before tackling conjunction after you have had a go at this task you can compare your solution to grammars book grammars sql fcfg in the nltk data distribution observe that the and conjunction in a is translated into an and in the sql counterpart b the latter tells us to select results from rows where two conditions are true together the value of the country column is china and the value of the population column is greater than this interpretation for and involves a new idea it talks about what is true in some particular situation and tells us that cond and cond is true in situation s just in case that condition cond is true in s and condition cond is true in s although this does not account for the full range of meanings of and in english it has the nice property that it is independent of any query language in fact we have given it the standard interpretation from classical logic in the following sections we will explore an approach in which sentences of natural language are translated into logic instead of an executable query language such as sql one advantage of logical formalisms is that they are more abstract and therefore more generic if we wanted to once we had our translation into logic we could then translate it into various other special purpose languages in fact most serious attempts to query databases via natural language have used this methodology natural language semantics and logic we started out trying to capture the meaning of a by translating it into a query in another language sql which the computer could interpret and execute but this still begged the question whether the translation was correct stepping back from database query we noted that the meaning of and seems to depend on being able to specify when statements are true or not in a particular situation instead of translating a sentence s from one language to another we try to say what s is about by relating it to a situation in the world let pursue this further imagine there is a situation s where there are two entities margrietje and her favourite doll brunoke in addition there is a relation holding between the two entities which we will call the love relation if you understand the meaning of then you know that it is true in situation s in part you know this because you know that margrietje refers to margrietje brunoke refers to brunoke and houdt van refers to the love relation we have introduced two fundamental notions in semantics the first is that declarative sentences are true or false in certain situations the second is that definite noun phrases and proper nouns refer to things in the world so is true in a situation where margrietje loves the doll brunoke here illustrated in figure depiction of a situation in which margrietje loves brunoke once we have adopted the notion of truth in a situation we have a powerful tool for reasoning in particular we can look at sets of sentences and ask whether they could be true together in some situation for emple the sentences in can be both true while those in and cannot be in other words the sentences in are consistent while those in and are inconsistent we have chosen sentences about fictional countries featured in the marx brothers movie duck soup to emphasize that your ability to reason about these emples does not depend on what is true or false in the actual world if you know the meaning of the word no and also know that the capital of a country is a city in that country then you should be able to conclude that the two sentences in are inconsistent regardless of where freedonia is or what the population of its capital is that is there is no possible situation in which both sentences could be true similarly if you know that the relation expressed by to the north of is asymmetric then you should be able to conclude that the two sentences in are inconsistent broadly speaking logic based approaches to natural language semantics focus on those aspects of natural language which guide our judgments of consistency and inconsistency the syntax of a logical language is designed to make these features formally explicit as a result determining properties like consistency can often be reduced to symbolic manipulation that is to a task that can be carried out by a computer in order to pursue this approach we first want to develop a technique for representing a possible situation we do this in terms of something that logicians call a model a model for a set w of sentences is a formal representation of a situation in which all the sentences in w are true the usual way of representing models involves set theory the domain d of discourse all the entities we currently care about is a set of individuals while relations are treated as sets built up from d let look at a concrete emple our domain d will consist of three children stefan klaus and evi represented respectively as s k and e we write this as d s k e the expression boy denotes the set consisting of stefan and klaus the expression girl denotes the set consisting of evi and the expression is running denotes the set consisting of stefan and evi is a graphical rendering of the model figure diagram of a model containing a domain d and subsets of d corresponding to the predicates boy girl and is running later in this chapter we will use models to help evaluate the truth or falsity of english sentences and in this way to illustrate some methods for representing meaning however before going into more detail let s put the discussion into a broader perspective and link back to a topic that we briefly raised in can a computer understand the meaning of a sentence and how could we tell if it did this is similar to asking can a computer think alan turing famously proposed to answer this by emining the ability of a computer to hold sensible conversations with a human turing suppose you are having a chat session with a person and a computer but you are not told at the outset which is which if you cannot identify which of your partners is the computer after chatting with each of them then the computer has successfully imitated a human if a computer succeeds in passing itself off as human in this imitation game or turing test as it is popularly known then according to turing we should be prepared to say that the computer can think and can be said to be intelligent so turing side stepped the question of somehow emining the internal states of a computer by instead using its behavior as evidence of intelligence by the same reasoning we have assumed that in order to say that a computer understands english it just needs to behave as though it did what is important here is not so much the specifics of turing s imitation game but rather the proposal to judge a capacity for natural language understanding in terms of observable behavior propositional logic a logical language is designed to make reasoning formally explicit as a result it can capture aspects of natural language which determine whether a set of sentences is consistent as part of this approach we need to develop logical representations of a sentence which formally capture the truth conditions of we will start off with a simple emple let replace the two sub sentences in by and respectively and put for the logical operator corresponding to the english word and this structure is the logical form of propositional logic allows us to represent just those parts of linguistic structure which correspond to certain sentential connectives we have just looked at and other such connectives are not or and if then in the formalization of propositional logic the counterparts of such connectives are sometimes called boolean operators the basic expressions of propositional logic are propositional symbols often written as p q r etc there are varying conventions for representing boolean operators since we will be focusing on ways of exploring logic within nltk we will stick to the following ascii versions of the operators from the propositional symbols and the boolean operators we can build an infinite set of well formed formulas or just formulas for short of propositional logic first every propositional letter is a formula then if is a formula so is and if and are formulas then so are the specifies the truth conditions for formulas containing these operators as before we use and as variables over sentences and abbreviate if and only if as iff table truth conditions for the boolean operators in propositional logic these rules are generally straightforward though the truth conditions for implication departs in many cases from our usual intuitions about the conditional in english a formula of the form p q is only false when p is true and q is false if p is false say p corresponds to the moon is made of green cheese and q is true say q corresponds to two plus two equals four then p q will come out true nltks expression object can process logical expressions into various subclasses of expression from a computational perspective logics give us an important tool for performing inference suppose you state that freedonia is not to the north of sylvania and you give as your reasons that sylvania is to the north of freedonia in this case you have produced an argument the sentence sylvania is to the north of freedonia is the assumption of the argument while freedonia is not to the north of sylvania is the conclusion the step of moving from one or more assumptions to a conclusion is called inference informally it is common to write arguments in a format where the conclusion is preceded by therefore an argument is valid if there is no possible situation in which its premises are all true and its conclusion is not true now the validity of crucially depends on the meaning of the phrase to the north of in particular the fact that it is an asymmetric relation unfortunately we can t express such rules in propositional logic the smallest elements we have to play with are atomic propositions and we cannot look inside these to talk about relations between individuals x and y the best we can do in this case is capture a particular case of the asymmetry let s use the propositional symbol snf to stand for sylvania is to the north of freedonia and fns for freedonia is to the north of sylvania to say that freedonia is not to the north of sylvania we write fns that is we treat not as equivalent to the phrase it is not the case that and translate this as the one place boolean operator so now we can write the implication in as how about giving a version of the complete argument we will replace the first sentence of by two formulas of propositional logic snf and also the implication in which expresses rather poorly our background knowledge of the meaning of to the north of we will write a an c to represent the argument that conclusion c follows from assumptions a an this leads to the following as a representation of argument this is a valid argument if snf and snf fns are both true in a situation s then fns must also be true in s by contrast if fns were true this would conflict with our understanding that two objects cannot both be to the north of each other in any possible situation equivalently the list snf snf fns fns is inconsistent these sentences cannot all be true together arguments can be tested for syntactic validity by using a proof system we will say a little bit more about this later on in logical proofs can be carried out with nltk s inference module for emple via an interface to the third party theorem prover prover the inputs to the inference mechanism first have to be converted into logical expressions here s another way of seeing why the conclusion follows snf fns is semantically equivalent to snf fns where is the two place operator corresponding to or in general is true in a situation s if either is true in s or is true in s now suppose both snf and snf fns are true in situation s if snf is true then snf cannot also be true a fundamental assumption of classical logic is that a sentence cannot be both true and false in a situation consequently fns must be true recall that we interpret sentences of a logical language relative to a model which is a very simplified version of the world a model for propositional logic needs to assign the values true or false to every possible formula we do this inductively first every propositional symbol is assigned a value and then we compute the value of complex formulas by consulting the meanings of the boolean operators i e and applying them to the values of the formula components a valuation is a mapping from basic expressions of the logic to their values here is an emple we initialize a valuation with a list of pairs each of which consists of a semantic symbol and a semantic value the resulting object is essentially just a dictionary that maps logical expressions treated as strings to appropriate values as we will see later our models need to be somewhat more complicated in order to handle the more complex logical forms discussed in the next section for the time being just ignore the dom and g parameters in the following declarations now let initialize a model m that uses val every model comes with an evaluate method which will determine the semantic value of logical expressions such as formulas of propositional logic of course these values depend on the initial truth values we assigned to propositional symbols such as p q and r note your turn experiment with evaluating different formulas of propositional logic does the model give the values that you expected up until now we have been translating our english sentences into propositional logic because we are confined to representing atomic sentences with letters like p and q we cannot dig into their internal structure in effect we are saying that there is nothing of logical interest to dividing atomic sentences into subjects objects and predicates however this seems wrong if we want to formalize arguments such as we have to be able to look inside basic sentences as a result we will move beyond propositional logic to a something more expressive namely first order logic this is what we turn to in the next section first order logic in the remainder of this chapter we will represent the meaning of natural language expressions by translating them into first order logic not all of natural language semantics can be expressed in first order logic but it is a good choice for computational semantics because it is expressive enough to represent a good deal and on the other hand there are excellent systems available off the shelf for carrying out automated inference in first order logic our next step will be to describe how formulas of first order logic are constructed and then how such formulas can be evaluated in a model syntax first order logic keeps all the boolean operators of propositional logic but it adds some important new mechanisms to start with propositions are analyzed into predicates and arguments which takes us a step closer to the structure of natural languages the standard construction rules for first order logic recognize terms such as individual variables and individual constants and predicates which take differing numbers of arguments for emple angus walks might be formalized as walk angus and angus sees bertie as see angus bertie we will call walk a unary predicate and see a binary predicate the symbols used as predicates do not have intrinsic meaning although it is hard to remember this returning to one of our earlier emples there is no logical difference between a and b by itself first order logic has nothing substantive to say about lexical semantics the meaning of individual words although some theories of lexical semantics can be encoded in first order logic whether an atomic predication like see angus bertie is true or false in a situation is not a matter of logic but depends on the particular valuation that we have chosen for the constants see angus and bertie for this reason such expressions are called non logical constants by contrast logical constants such as the boolean operators always receive the same interpretation in every model for first order logic we should mention here that one binary predicate has special status namely equality as in formulas such as angus aj equality is regarded as a logical constant since for individual terms t and t the formula t t is true if and only if t and t refer to one and the same entity it is often helpful to inspect the syntactic structure of expressions of first order logic and the usual way of doing this is to assign types to expressions following the tradition of montague grammar we will use two basic types e is the type of entities while t is the type of formulas i e expressions which have truth values given these two basic types we can form complex types for function expressions that is given any types and is a complex type corresponding to functions from things to things for emple e t is the type of expressions from entities to truth values namely unary predicates the logical expression can be processed with type checking why do we see e at the end of this emple although the type checker will try to infer as many types as possible in this case it has not managed to fully specify the type of walk since its result type is unknown although we are intending walk to receive type e t as far as the type checker knows in this context it could be of some other type such as e e or e e t to help the type checker we need to specify a signature implemented as a dictionary that explicitly associates types with non logical constants a binary predicate has type e e t although this is the type of something which combines first with an argument of type e to make a unary predicate we represent binary predicates as combining directly with their two arguments for emple the predicate see in the translation of angus sees cyril will combine with its arguments to give the result see angus cyril in first order logic arguments of predicates can also be individual variables such as x y and z in nltk we adopt the convention that variables of type e are all lowercase individual variables are similar to personal pronouns like he she and it in that we need to know about the context of use in order to figure out their denotation one way of interpreting the pronoun in is by pointing to a relevant individual in the local context another way is to supply a textual antecedent for the pronoun he for emple by uttering a prior to here we say that he is coreferential with the noun phrase cyril as a result is semantically equivalent to b consider by contrast the occurrence of he in a in this case it is bound by the indefinite np a dog and this is a different relationship than coreference if we replace the pronoun he by a dog the result b is not semantically equivalent to a corresponding to a we can construct an open formula b with two occurrences of the variable x we ignore tense to simplify exposition by placing an existential quantifier x for some x in front of b we can bindthese variables as in a which means b or more idiomatically c the nltk rendering of a in addition to the existential quantifier first order logic offers us the universal quantifier x for all x illustrated in the nltk syntax for a although a is the standard first order logic translation of c the truth conditions are not necessarily what you expect the formula says that if some x is a dog then x disappears but it does not say that there are any dogs so in a situation where there are no dogs a will still come out true remember that p q is true when p is false now you might argue that every dog disappeared does presuppose the existence of dogs and that the logic formalization is simply wrong but it is possible to find other emples which lack such a presupposition for instance we might explain that the value of the python expression astring replace ate is the result of replacing every occurrence of ate in astring by even though there may in fact be no such occurrences we have seen a number of emples where variables are bound by quantifiers what happens in formulas such as the following exists x dog x bark x the scope of the exists x quantifier is dog x so the occurrence of x in bark x is unbound consequently it can become bound by some other quantifier for emple all x in the next formula all x exists x dog x bark x in general an occurrence of a variable x in a formula is free in if that occurrence does not fall within the scope of all x or some x in conversely if x is free in formula then it is bound in all x and exists x if all variable occurrences in a formula are bound the formula is said to be closed we mentioned before that the expression object can process strings and returns objects of class expression each instance expr of this class comes with a method free which returns the set of variables that are free in expr first order theorem proving recall the constraint on to the north of which we proposed earlier as we observed that propositional logic is not expressive enough to represent generalizations about binary predicates and as a result we did not properly capture the argument sylvania is to the north of freedonia therefore freedonia is not to the north of sylvania you have no doubt realized that first order logic by contrast is ideal for formalizing such rules all x all y north of x y north of y x even better we can perform automated inference to show the validity of the argument the general case in theorem proving is to determine whether a formula that we want to prove a proof goal can be derived by a finite sequence of inference steps from a list of assumed formulas we write this as s g where s is a possibly empty list of assumptions and g is a proof goal we will illustrate this with nltk interface to the theorem prover prover first we parse the required proof goal and the two assumptions then we create a prover instance and call its prove method on the goal given the list of assumptions happily the theorem prover agrees with us that the argument is valid by contrast it concludes that it is not possible to infer north of f s from our assumptions summarizing the language of first order logic we will take this opportunity to restate our earlier syntactic rules for propositional logic and add the formation rules for quantifiers together these give us the syntax of first order logic in addition we make explicit the types of the expressions involved we will adopt the convention that en t is the type of a predicate which combines with n arguments of type e to yield an expression of type t in this case we say that n is the arity of the predicate if p is a predicate of type en t and n are terms of type e then p n is of type t if and are both of type e then and are of type t if is of type t then so is if and are of type t then so are and if is of type t and x is a variable of type e then exists x and all x are of type t summarizes the new logical constants of the logic module and two of the methods of expressions table summary of new logical relations and operators required for first order logic together with two useful methods of the expression class truth in model we have looked at the syntax of first order logic and in we will emine the task of translating english into first order logic yet as we argued in this only gets us further forward if we can give a meaning to sentences of first order logic in other words we need to give a truth conditional semantics to first order logic from the point of view of computational semantics there are obvious limits in how far one can push this approach although we want to talk about sentences being true or false in situations we only have the means of representing situations in the computer in a symbolic manner despite this limitation it is still possible to gain a clearer picture of truth conditional semantics by encoding models in nltk given a first order logic language l a model m for l is a pair d val where d is an nonempty set called the domain of the model and val is a function called the valuation function which assigns values from d to expressions of l as follows for every individual constant c in l val c is an element of d for every predicate symbol p of arity n val p is a function from dn to true false if the arity of p is then val p is simply a truth value the p is regarded as a propositional symbol according to ii if p is of arity then val p will be a function f from pairs of elements of d to true false in the models we shall build in nltk we will adopt a more convenient alternative in which val p is a set s of pairs defined as follows such an f is called the characteristic function of s as discussed in the further readings relations are represented semantically in nltk in the standard set theoretic way as sets of tuples for emple let suppose we have a domain of discourse consisting of the individuals bertie olive and cyril where bertie is a boy olive is a girl and cyril is a dog for mnemonic reasons we use b o and c as the corresponding labels in the model we can declare the domain as follows we will use the utility function valuation fromstring to convert a list of strings of the form symbol value into a valuation object so according to this valuation the value of see is a set of tuples such that bertie sees olive cyril sees bertie and olive sees cyril note your turn draw a picture of the domain of m and the sets corresponding to each of the unary predicates by analogy with the diagram shown in you may have noticed that our unary predicates i e boy girl dog also come out as sets of singleton tuples rather than just sets of individuals this is a convenience which allows us to have a uniform treatment of relations of any arity a predication of the form p n where p is of arity n comes out true just in case the tuple of values corresponding to n belongs to the set of tuples in the value of p individual variables and assignments in our models the counterpart of a context of use is a variable assignment this is a mapping from individual variables to entities in the domain assignments are created using the assignment constructor which also takes the model domain of discourse as a parameter we are not required to actually enter any bindings but if we do they are in a variable value format similar to what we saw earlier for valuations in addition there is a print format for assignments which uses a notation closer to that often found in logic textbooks let now look at how we can evaluate an atomic formula of first order logic first we create a model then we call the evaluate method to compute the truth value what happening here we are evaluating a formula which is similar to our earlier emplle see olive cyril however when the interpretation function encounters the variable y rather than checking for a value in val it asks the variable assignment g to come up with a value since we already know that individuals o and c stand in the see relation the value true is what we expected in this case we can say that assignment g satisfies the formula see olive y by contrast the following formula evaluates to false relative to g check that you see why this is in our approach though not in standard first order logic variable assignments are partial for emple g says nothing about any variables apart from x and y the method purge clears all bindings from an assignment if we now try to evaluate a formula such as see olive y relative to g it is like trying to interpret a sentence containing a him when we do not know what him refers to in this case the evaluation function fails to deliver a truth value since our models already contain rules for interpreting boolean operators arbitrarily complex formulas can be composed and evaluated the general process of determining truth or falsity of a formula in a model is called model checking quantification one of the crucial insights of modern logic is that the notion of variable satisfaction can be used to provide an interpretation to quantified formulas let use as an emple when is it true let think about all the individuals in our domain i e in dom we want to check whether any of these individuals have the property of being a girl and walking in other words we want to know if there is some u in dom such that g u x satisfies the open formula consider the following evaluate returns true here because there is some u in dom such that is satisfied by an assignment which binds x to u in fact o is such a u one useful tool offered by nltk is the satisfiers method this returns a set of all the individuals that satisfy an open formula the method parameters are a parsed formula a variable and an assignment here are a few emples it is useful to think about why fmla and fmla receive the values they do the truth conditions for mean that fmla is equivalent to girl x walk x which is satisfied by something which either is not a girl or walks since neither b bertie nor c cyril are girls according to model m they both satisfy the whole formula and of course o satisfies the formula because o satisfies both disjuncts now since every member of the domain of discourse satisfies fmla the corresponding universally quantified formula is also true in other words a universally quantified formula x is true with respect to g just in case for every u is true with respect to g u x note your turn try to figure out first with pencil and paper and then using m evaluate what the truth values are for all x girl x walk x and exists x boy x walk x make sure you understand why they receive these values quantifier scope ambiguity what happens when we want to give a formal representation of a sentence with two quantifiers such as the following there are at least two ways of expressing in first order logic can we use both of these the answer is yes but they have different meanings b is logically stronger than a it claims that there is a unique person say bruce who is admired by everyone a on the other hand just requires that for every person u we can find some person u whom u admires but this could be a different person u in each case we distinguish between a and b in terms of the scope of the quantifiers in the first has wider scope than while in b the scope ordering is reversed so now we have two ways of representing the meaning of and they are both quite legitimate in other words we are claiming that is ambiguous with respect to quantifier scope and the formulas in give us a way to make the two readings explicit however we are not just interested in associating two distinct representations with we also want to show in detail how the two representations lead to different conditions for truth in a model in order to emine the ambiguity more closely let fix our valuation as follows the admire relation can be visualized using the mapping diagram shown in in an arrow between two individuals x and y indicates that x admires y so j and b both admire b bruce is very vain while e admires m and m admires e in this model formula a above is true but b is false one way of exploring these results is by using the satisfiers method of model objects this shows that fmla holds of every individual in the domain by contrast consider the formula fmla below this has no satisfiers for the variable y that is there is no person that is admired by everybody taking a different open formula fmla we can verify that there is a person namely bruce who is admired by both julia and bruce note your turn devise a new model based on m such that a comes out false in your model similarly devise a new model such that b comes out true model building we have been assuming that we already had a model and wanted to check the truth of a sentence in the model by contrast model building tries to create a new model given some set of sentences if it succeeds then we know that the set is consistent since we have an existence proof of the model we invoke the mace model builder by creating an instance of mace and calling its build model method in an analogous way to calling the prover theorem prover one option is to treat our candidate set of sentences as assumptions while leaving the goal unspecified the following interaction shows how both a c and a c are consistent lists since mace succeeds in building a model for each of them while c c is inconsistent we can also use the model builder as an adjunct to the theorem prover let suppose we are trying to prove s g i e that g is logically derivable from assumptions s s s sn we can feed this same input to mace and the model builder will try to find a counteremple that is to show that g does not follow from s so given this input mace will try to find a model for the set s together with the negation of g namely the list s s s sn g if g fails to follow from s then mace may well return with a counteremple faster than prover concludes that it cannot find the required proof conversely if g is provable from s mace may take a long time unsuccessfully trying to find a countermodel and will eventually give up let consider a concrete scenario our assumptions are the list there is a woman that every man loves adam is a man eve is a woman our conclusion is adam loves eve can mace find a model in which the premises are true but the conclusion is false in the following code we use macecommand which will let us inspect the model that has been built so the answer is yes mace found a countermodel in which there is some woman other than eve that adam loves but let have a closer look at mace s model converted to the format we use for valuations the general form of this valuation should be familiar to you it contains some individual constants and predicates each with an appropriate kind of value what might be puzzling is the c this is a skolem constant that the model builder introduces as a representative of the existential quantifier that is when the model builder encountered the exists y part of a above it knew that there is some individual b in the domain which satisfies the open formula in the body of a however it doesn t know whether b is also the denotation of an individual constant anywhere else in its input so it makes up a new name for b on the fly namely c now since our premises said nothing about the individual constants adam and eve the model builder has decided there is no reason to treat them as denoting different entities and they both get mapped to a moreover we didn t specify that man and woman denote disjoint sets so the model builder lets their denotations overlap this illustrates quite dramatically the implicit knowledge that we bring to bear in interpreting our scenario but which the model builder knows nothing about so let s add a new assumption which makes the sets of men and women disjoint the model builder still produces a countermodel but this time it is more in accord with our intuitions about the situation on reflection we can see that there is nothing in our premises which says that eve is the only woman in the domain of discourse so the countermodel in fact is acceptable if we wanted to rule it out we would have to add a further assumption such as exists y all x woman x x y to ensure that there is only one woman in the model the semantics of english sentences compositional semantics in feature based grammar at the beginning of the chapter we briefly illustrated a method of building semantic representations on the basis of a syntactic parse using the grammar framework developed in this time rather than constructing an sql query we will build a logical form one of our guiding ideas for designing such grammars is the principle of compositionality also known as frege principle see gleitman liberman for the formulation given below principle of compositionality the meaning of a whole is a function of the meanings of the parts and of the way they are syntactically combined we will assume that the semantically relevant parts of a complex expression are given by a theory of syntactic analysis within this chapter we will take it for granted that expressions are parsed against a context free grammar however this is not entailed by the principle of compositionality our goal now is integrate the construction of a semantic representation in a manner that can be smoothly with the process of parsing illustrates a first approximation to the kind of analyses we would like to build in the sem value at the root node shows a semantic representation for the whole sentence while the sem values at lower nodes show semantic representations for constituents of the sentence since the values of sem have to be treated in special manner they are distinguished from other feature values by being enclosed in angle brackets so far so good but how do we write grammar rules which will give us this kind of result our approach will be similar to that adopted for the grammar sql fcfg at the start of this chapter in that we will assign semantic representations to lexical nodes and then compose the semantic representations for each phrase from those of its child nodes however in the present case we will use function application rather than string concatenation as the mode of composition to be more specific suppose we have a np and vp constituents with appropriate values for their sem nodes then the sem value of an s is handled by a rule like observe that in the case where the value of sem is a variable we omit the angle brackets tells us that given some sem value np for the subject np and some sem value vp for the vp the sem value of the s parent is constructed by applying vp as a function expression to np from this we can conclude that vp has to denote a function which has the denotation of np in its domain is a nice emple of building semantics using the principle of compositionality to complete the grammar is very straightforward all we require are the rules shown below vp sem v iv sem v np sem cyril cyril iv sem x bark x barks the vp rule says that the parent semantics is the same as the head child semantics the two lexical rules provide non logical constants to serve as the semantic values of cyril and barks respectively there is an additional piece of notation in the entry for barks which we will explain shortly before launching into compositional semantic rules in more detail we need to add a new tool to our kit namely the calculus this provides us with an invaluable tool for combining expressions of first order logic as we assemble a meaning representation for an english sentence the calculus in we pointed out that mathematical set notation was a helpful method of specifying properties p of words that we wanted to select from a document we illustrated this with which we glossed as the set of all w such that w is an element of v the vocabulary and w has property p it turns out to be extremely useful to add something to first order logic that will achieve the same effect we do this with the operator pronounced lambda the counterpart to is since we are not trying to do set theory here we just treat v as a unary predicate note expressions were originally designed by alonzo church to represent computable functions and to provide a foundation for mathematics and logic the theory in which expressions are studied is known as the calculus note that the calculus is not part of first order logic both can be used independently of the other is a binding operator just as the first order logic quantifiers are if we have an open formula such as a then we can bind the variable x with the operator as shown in b the corresponding nltk representation is given in c remember that is a special character in python strings we could escape it with another or else use raw strings we have a special name for the result of binding the variables in an expression abstraction when you first encounter abstracts it can be hard to get an intuitive sense of their meaning a couple of english glosses for b are be an x such that x walks and x chews gum or have the property of walking and chewing gum it has often been suggested that abstracts are good representations for verb phrases or subjectless clauses particularly when these occur as arguments in their own right this is illustrated in a and its translation b so the general picture is this given an open formula with free variable x abstracting over x yields a property expression x the property of being an x such that here is a more official version of how abstracts are built b illustrated a case where we say something about a property namely that it is hard but what we usually do with properties is attribute them to individuals and in fact if is an open formula then the abstract x can be used as a unary predicate in b is predicated of the term gerald now says that gerald has the property of walking and chewing gum which has the same meaning as what we have done here is remove the x from the beginning of x walk x chew gum x and replaced all occurrences of x in walk x chew gum x by gerald we will use x as notation for the operation of replacing all free occurrences of x in by the expression so walk x chew gum x gerald x is the same expression as the reduction of to is an extremely useful operation in simplifying semantic representations and we shall use it a lot in the rest of this chapter the operation is often called reduction in order for it to be semantically justified we want it to hold that x has the same semantic values as x this is indeed true subject to a slight complication that we will come to shortly in order to carry of reduction of expressions in nltk we can call the simplify method although we have so far only considered cases where the body of the abstract is an open formula i e of type t this is not a necessary restriction the body can be any well formed expression here is an emple with two s just as b plays the role of a unary predicate works like a binary predicate it can be applied directly to two arguments logical expressions may contain nested s such as x y to be written in the abbreviated form x y all our abstracts so far have involved the familiar first order variables x y and so on variables of type e but suppose we want to treat one abstract say x walk x as the argument of another abstract we might try this y y angus x walk x but since the variable y is stipulated to be of type e y y angus only applies to arguments of type e while x walk x is of type e t instead we need to allow abstraction over variables of higher type let use p and q as variables of type e t and then we can have an abstract such as p p angus since p is of type e t the whole abstract is of type e t t then p p angus x walk x is legal and can be simplified via reduction to x walk x angus and then again to walk angus when carrying out reduction some care has to be taken with variables consider for emple the terms a and b which differ only in the identity of a free variable suppose now that we apply the term p exists x p x to each of these terms we pointed out earlier that the results of the application should be semantically equivalent but if we let the free variable x in a fall inside the scope of the existential quantifier in a then after reduction the results will be different a means there is some x that sees him herself whereas b means that there is some x that sees an unspecified individual z what has gone wrong here clearly we want to forbid the kind of variable capture shown in a in order to deal with this problem let step back a moment does it matter what particular name we use for the variable bound by the existential quantifier in the function expression of a the answer is no in fact given any variable binding expression involving or the name chosen for the bound variable is completely arbitrary for emple exists x p x and exists y p y are equivalent they are called equivalents or alphabetic variants the process of relabeling bound variables is known as conversion when we test for equality of variablebinderexpressions in the logic module i e using we are in fact testing for equivalence when reduction is carried out on an application f a we check whether there are free variables in a which also occur as bound variables in any subterms of f suppose as in the emple discussed above that x is free in a and that f contains the subterm exists x p x in this case we produce an alphabetic variant of exists x p x say exists z p z and then carry on with the reduction this relabeling is carried out automatically by the reduction code in logic and the results can be seen in the following emple note as you work through emples like these in the following sections you may find that the logical expressions which are returned have different variable names for emple you might see z in place of z in the above formula this change in labeling is innocuous in fact it is just an illustration of alphabetic variants after this excursus let return to the task of building logical forms for english sentences quantified nps at the start of this section we briefly described how to build a semantic representation for cyril barks you would be forgiven for thinking this was all too easy surely there is a bit more to building compositional semantics what about quantifiers for instance right this is a crucial issue for emple we want a to be given the logical form in b how can this be accomplished let make the assumption that our only operation for building complex semantic representations is function application then our problem is this how do we give a semantic representation to the quantified nps a dog so that it can be combined with bark to give the result in b as a first step let make the subject sem value act as the function expression rather than the argument this is sometimes called type raising now we are looking for way of instantiating np so that sem np x bark x is equivalent to sem exists x dog x bark x does not this look a bit reminiscent of carrying out reduction in the calculus in other words we want a term m to replace np so that applying m to bark yields b to do this we replace the occurrence of bark in b by a predicate variable p and bind the variable with as shown in we have used a different style of variable in that is p rather than x or y to signal that we are abstracting over a different kind of object not an individual but a function expression of type e t so the type of as a whole is e t t we will take this to be the type of nps in general to illustrate further a universally quantified np will look like we are pretty much done now except that we also want to carry out a further abstraction plus application for the process of combining the semantics of the determiner a namely with the semantics of dog applying as a function expression to dog yields and applying that to bark gives us p exists x dog x p x x bark x finally carrying out reduction yields just what we wanted namely b transitive verbs our next challenge is to deal with sentences containing transitive verbs such as the output semantics that we want to build is exists x dog x chase angus x let look at how we can use abstraction to get this result a significant constraint on possible solutions is to require that the semantic representation of a dog be independent of whether the np acts as subject or object of the sentence in other words we want to get the formula above as our output while sticking to as the np semantics a second constraint is that vps should have a uniform type of interpretation regardless of whether they consist of just an intransitive verb or a transitive verb plus object more specifically we stipulate that vps are always of type e t given these constraints here is a semantic representation for chases a dog which does the trick think of as the property of being a y such that for some dog x y chases x or more colloquially being a y who chases a dog our task now resolves to designing a semantic representation for chases which can combine with so as to allow to be derived let carry out the inverse of reduction on giving rise to may be slightly hard to read at first you need to see that it involves applying the quantified np representation from to z chase y z is equivalent via reduction to exists x dog x chase y x now let replace the function expression in by a variable x of the same type as an np that is of type e t t the representation of a transitive verb will have to apply to an argument of the type of x to yield a function expression of the type of vps that is of type e t we can ensure this by abstracting over both the x variable in and also the subject variable y so the full solution is reached by giving chases the semantic representation shown in if is applied to the result after reduction is equivalent to which is what we wanted all along in order to build a semantic representation for a sentence we also need to combine in the semantics of the subject np if the latter is a quantified expression like every girl everything proceeds in the same way as we showed for a dog barks earlier on the subject is translated as a function expression which is applied to the semantic representation of the vp however we now seem to have created another problem for ourselves with proper names so far these have been treated semantically as individual constants and these cannot be applied as functions to expressions like consequently we need to come up with a different semantic representation for them what we do in this case is re interpret proper names so that they too are function expressions like quantified nps here is the required expression for angus denotes the characteristic function corresponding to the set of all properties which are true of angus converting from an individual constant angus to p p angus is another emple of type raising briefly mentioned earlier and allows us to replace a boolean valued application such as x walk x angus with an equivalent function application p p angus x walk x by reduction both expressions reduce to walk angus the grammar simple sem fcfg contains a small set of rules for parsing and translating simple emples of the kind that we have been looking at here is a slightly more complicated emple nltk provides some utilities to make it easier to derive and inspect semantic interpretations the function interpret sents is intended for interpretation of a list of input sentences it builds a dictionary d where for each sentence sent in the input d sent is a list of pairs synrep semrep consisting of trees and semantic representations for sent the value is a list since sent may be syntactically ambiguous in the following emple however there is only one parse tree per sentence in the list we have seen now how to convert english sentences into logical forms and earlier we saw how logical forms could be checked as true or false in a model putting these two mappings together we can check the truth value of english sentences in a given model let take model m as defined above the utility evaluate sents resembles interpret sents except that we need to pass a model and a variable assignment as parameters the output is a triple synrep semrep value where synrep semrep are as before and value is a truth value for simplicity the following emple only processes a single sentence quantifier ambiguity revisited it should be clearer now why the address variables are an important part of the binding operator recall that during s retrieval we will be taking binding operators off the store list and applying them successively to the core suppose we start with bo p all x girl x p x z which we want to combine with chase z z the quantifier part of binding operator is p all x girl x p x and to combine this with chase z z the latter needs to first be turned into a abstract how do we know which variable to abstract over this is what the address z tells us i e that every girl has the role of chaser rather than chasee the module nltk sem cooper storage deals with the task of turning storage style semantic representations into standard logical forms first we construct a cooperstore instance and inspect its store and core finally we call s retrieve and check the readings discourse semantics a discourse is a sequence of sentences very often the interpretation of a sentence in a discourse depends what preceded it a clear emple of this comes from anaphoric pronouns such as he she and it given discourse such as angus used to have a dog but he recently disappeared you will probably interpret he as referring to angus dog however in angus used to have a dog he took him for walks in new town you are more likely to interpret he as referring to angus himself discourse representation theory that is the np a dog acts like a quantifier which binds the it in the second sentence discourse representation theory drt was developed with the specific goal of providing a means for handling this and other semantic phenomena which seem to be characteristic of discourse a discourse representation structure drs presents the meaning of discourse in terms of a list of discourse referents and a list of conditions the discourse referents are the things under discussion in the discourse and they correspond to the individual variables of first order logic the drs conditionsapply to those discourse referents and correspond to atomic open formulas of first order logic illustrates how drs for the first sentence in a is augmented to become a drs for both sentences figure building a drs the drs on the left hand side represents the result of processing the first sentence in the discourse while the drs on the right hand side shows the effect of processing the second sentence and integrating its content when the second sentence of a is processed it is interpreted in the context of what is already present in the left hand side of the pronoun it triggers the addition of a new discourse referent say u and we need to find an anaphoric antecedent for it that is we want to work out what it refers to in drt the task of finding the antecedent for an anaphoric pronoun involves linking it to a discourse referent already within the current drs and y is the obvious choice we will say more about anaphora resolution shortly this processing step gives rise to a new condition u y the remaining content contributed by the second sentence is also merged with the content of the first and this is shown on the right hand side of illustrates how a drs can represent more than just a single sentence in this case it is a two sentence discourse but in principle a single drs could correspond to the interpretation of a whole text we can inquire into the truth conditions of the right hand drs in informally it is true in some situation s if there are entities a c and i in s corresponding to the discourse referents in the drs such that all the conditions are true in s that is a is named angus c is a dog a owns c i is named irene and c bit i in order to process drss computationally we need to convert them into a linear format here is an emple where the drs is a pair consisting of a list of discourse of referents and a list of drs conditions x y angus x dog y own x y the easiest way to build a drs object in nltk is by parsing a string representation we can use the draw method to visualize the result as shown in figure drs screenshot when we discussed the truth conditions of the drss in we assumed that the topmost discourse referents were interpreted as existential quantifiers while the conditions were interpreted as though they are conjoined in fact every drs can be translated into a formula of first order logic and the fol method implements this translation in addition to the functionality available for first order logic expressions drt expressions have a drs concatenation operator represented as the symbol the concatenation of two drss is a single drs containing the merged discourse referents and the conditions from both arguments drs concatenation automatically converts bound variables to avoid name clashes while all the conditions seen so far have been atomic it is possible to embed one drs within another and this is how universal quantification is handled in drs there are no top level discourse referents and the sole condition is made up of two sub drss connected by an implication again we can use fol to get a handle on the truth conditions we pointed out earlier that drt is designed to allow anaphoric pronouns to be interpreted by linking to existing discourse referents drt sets constraints on which discourse referents are accessible as possible antecedents but is not intended to explain how a particular antecedent is chosen from the set of candidates the module nltk sem drt resolve anaphora adopts a similarly conservative strategy if the drs contains a condition of the form pro x the method resolve anaphora replaces this with a condition of the form x where is a list of possible antecedents since the algorithm for anaphora resolution has been separated into its own module this facilitates swapping in alternative procedures which try to make more intelligent guesses about the correct antecedent our treatment of drss is fully compatible with the existing machinery for handling abstraction and consequently it is straightforward to build compositional semantic representations which are based on drt rather than first order logic this technique is illustrated in the following rule for indefinites which is part of the grammar drt fcfg for ease of comparison we have added the parallel rule for indefinites from simple sem fcfg det num sg sem p q x p x q x a det num sg sem p q exists x p x q x a to get a better idea of how the drt rule works look at this subtree for the np a dog np num sg sem q x dog x q x det num sg sem p q x p x q x a nom num sg sem x dog x n num sg sem x dog x dog the abstract for the indefinite is applied as a function expression to x dog x which leads to q x dog x q x after simplification we get q x dog x q x as the representation for the np as a whole in order to parse with grammar drt fcfg we specify in the call to load parser that sem values in feature structures are to be parsed using drtparser discourse processing when we interpret a sentence we use a rich context for interpretation determined in part by the preceding context and in part by our background assumptions drt provides a theory of how the meaning of a sentence is integrated into a representation of the prior discourse but two things have been glaringly absent from the processing approach just discussed first there has been no attempt to incorporate any kind of inference and second we have only processed individual sentences these omissions are redressed by the module nltk inference discourse whereas a discourse is a sequence s sn of sentences a discourse thread is a sequence s ri sn rj of readings one for each sentence in the discourse the module processes sentences incrementally keeping track of all possible threads when there is ambiguity for simplicity the following emple ignores scope ambiguity when a new sentence is added to the current discourse setting the parameter consistchk true causes consistency to be checked by invoking the model checker for each thread i e sequence of admissible readings in this case the user has the option of retracting the sentence in question in a similar manner we use informchk true to check whether a new sentence is informative relative to the current discourse the theorem prover treats existing sentences in the thread as assumptions and attempts to prove it is informative if no such proof can be found it is also possible to pass in an additional set of assumptions as background knowledge and use these to filter out inconsistent readings see the discourse howto at http nltk org howto for more details the discourse module can accommodate semantic ambiguity and filter out readings that are not admissible the following emple invokes both glue semantics as well as drt since the glue semantics module is configured to use the wide coverage malt dependency parser the input every dog chases a boy he runs needs to be tagged as well as tokenized the first sentence of the discourse has two possible readings depending on the quantfier scoping the unique reading of the second sentence represents the pronoun he via the condition pro x now let look at the discourse threads that result when we emine threads d and d we see that reading s r where every dog out scopes a boy is deemed inadmissible because the pronoun in the second sentence cannot be resolved by contrast in thread d the pronoun relettered to z has been bound via the equation z z inadmissible readings can be filtered out by passing the parameter filter true although this little discourse is extremely limited it should give you a feel for the kind of semantic processing issues that arise when we go beyond single sentences and also a feel for the techniques that can be deployed to address them summary first order logic is a suitable language for representing natural language meaning in a computational setting since it is flexible enough to represent many useful aspects of natural meaning and there are efficient theorem provers for reasoning with first order logic equally there are a variety of phenomena in natural language semantics which are believed to require more powerful logical mechanisms as well as translating natural language sentences into first order logic we can state the truth conditions of these sentences by emining models of first order formulas in order to build meaning representations compositionally we supplement first order logic with the calculus reduction in the calculus corresponds semantically to application of a function to an argument syntactically it involves replacing a variable bound by in the function expression with the expression that provides the argument in the function application a key part of constructing a model lies in building a valuation which assigns interpretations to non logical constants these are interpreted as either n ary predicates or as individual constants an open expression is an expression containing one or more free variables open expressions only receive an interpretation when their free variables receive values from a variable assignment quantifiers are interpreted by constructing for a formula x open in variable x the set of individuals which make x true when an assignment g assigns them as the value of x the quantifier then places constraints on that set a closed expression is one that has no free variables that is the variables are all bound a closed sentence is true or false with respect to all variable assignments if two formulas differ only in the label of the variable bound by binding operator i e or a quantifier they are said to be equivalents the result of relabeling a bound variable in a formula is called conversion given a formula with two nested quantifiers q and q the outermost quantifier q is said to have wide scope or scope over q english sentences are frequently ambiguous with respect to the scope of the quantifiers they contain english sentences can be associated with a semantic representation by treating sem as a feature in a feature based grammar the sem value of a complex expressions typically involves functional application of the sem values of the component expressions further reading consult http nltk org for further materials on this chapter and on how to install the prover theorem prover and mace model builder general information about these two inference tools is given by mccune for more emples of semantic analysis with nltk please see the semantics and logic howtos at http nltk org howto note that there are implementations of two other approaches to scope ambiguity namely hole semanticsas described in blackburn bos and glue semanticsas described in dalrymple there are many phenomena in natural language semantics which have not been touched on in this chapter most notably events tense and aspect semantic roles generalized quantifiers such as most intensional constructions involving for emple verbs like may and believe while and can be dealt with using first order logic and require different logics these issues are covered by many of the references in the readings below a comprehensive overview of results and techniques in building natural language front ends to databases can be found in androutsopoulos ritchie thanisch any introductory book to modern logic will present propositional and first order logic hodges is highly recommended as an entertaining and insightful text with many insightful illustrations from natural language for a wide ranging two volume textbook on logic that also presents contemporary material on the formal semantics of natural language including montague grammar and intensional logic see gamut and gamut kamp reyle provides the definitive account of discourse representation theory and covers a large and interesting fragment of natural language including tense aspect and modality another comprehensive study of the semantics of many natural language constructions is carpenter there are numerous works that introduce logical semantics within the framework of linguistic theory chierchia mcconnell ginet is relatively agnostic about syntax while heim kratzer and larson segal are both more explicitly oriented towards integrating truth conditional semantics into a chomskyan framework blackburn bos is the first textbook devoted to computational semantics and provides an excellent introduction to the area it expands on many of the topics covered in this chapter including underspecification of quantifier scope ambiguity first order inference and discourse processing to gain an overview of more advanced contemporary approaches to semantics including treatments of tense and generalized quantifiers try consulting lappin or benthem meulen exercises translate the following sentences into propositional logic and verify that they can be processed with expression fromstring provide a key which shows how the propositional variables in your translation correspond to expressions of english if angus sings it is not the case that bertie sulks cyril runs and barks it will snow if it does not rain it is not the case that irene will be happy if olive or tofu comes pat did not cough or sneeze if you do not come if i call i wo not come if you call translate the following sentences into predicate argument formula of first order logic angus likes cyril and irene hates cyril tofu is taller than bertie bruce loves himself and pat does too cyril saw bertie but angus did not cyril is a fourlegged friend tofu and olive are near each other translate the following sentences into quantified formulas of first order logic angus likes someone and someone likes julia angus loves a dog who loves him nobody smiles at pat somebody coughs and sneezes nobody coughed or sneezed bruce loves somebody other than bruce nobody other than matthew loves somebody pat cyril likes everyone except for irene ectly one person is asleep translate the following verb phrases using abstracts quantified formulas of first order logic feed cyril and give a capuccino to angus be given war and peace by pat be loved by everyone be loved or detested by everyone be loved by everyone and detested by no one consider the following statements clearly something is missing here namely a declaration of the value of e in order for applicationexpression e e to be convertible to exists y love pat y e must be a abstract which can take pat as an argument your task is to construct such an abstract bind it to e and satisfy yourself that the statements above are all satisfied up to alphabetic variance in addition provide an informal english translation of e simplify now carry on doing this same task for the further cases of e simplify shown below as in the preceding exercise find a abstract e that yields results equivalent to those shown below as in the preceding exercise find a abstract e that yields results equivalent to those shown below develop a method for translating english sentences into formulas with binary generalized quantifiers in such an approach given a generalized quantifier q a quantified formula is of the form q a b where both a and b are expressions of type e t then for emple all a b is true iff a denotes a subset of what b denotes extend the approach in the preceding exercise so that the truth conditions for quantifiers like most and ectly three can be computed in a model modify the sem evaluate code so that it will give a helpful error message if an expression is not in the domain of a model valuation function select three or four contiguous sentences from a book for children a possible source of emples are the collections of stories in nltk corpus gutenberg bryant stories txt burgess busterbrown txt and edgeworth parents txt develop a grammar which will allow your sentences to be translated into first order logic and build a model which will allow those translations to be checked for truth or falsity carry out the preceding exercise but use drt as the meaning representation taking warren pereira as a starting point develop a technique for converting a natural language query into a form that can be evaluated more efficiently in a model for emple given a query of the form p x q x convert it to q x p x if the extension of q is smaller than the extension of p managing linguistic data structured collections of annotated linguistic data are essential in most areas of nlp however we still face many obstacles in using them the goal of this chapter is to answer the following questions how do we design a new language resource and ensure that its coverage balance and documentation support a wide range of uses when existing data is in the wrong format for some analysis tool how can we convert it to a suitable format what is a good way to document the existence of a resource we have created so that others can easily find it along the way we will study the design of existing corpora the typical workflow for creating a corpus and the lifecycle of corpus as in other chapters there will be many emples drawn from practical experience managing linguistic data including data that has been collected in the course of linguistic fieldwork laboratory work and web crawling corpus structure a case study the timit corpus of read speech was the first annotated speech database to be widely distributed and it has an especially clear organization timit was developed by a consortium including tes instruments and mit from which it derives its name it was designed to provide data for the acquisition of acoustic phonetic knowledge and to support the development and evaluation of automatic speech recognition systems the structure of timit like the brown corpus which displays a balanced selection of text genres and sources timit includes a balanced selection of dialects speakers and materials for each of eight dialect regions male and female speakers having a range of ages and educational backgrounds each read ten carefully chosen sentences two sentences read by all speakers were designed to bring out dialect variation the remaining sentences were chosen to be phonetically rich involving all phones sounds and a comprehensive range of diphones phone bigrams additionally the design strikes a balance between multiple speakers saying the same sentence in order to permit comparison across speakers and having a large range of sentences covered by the corpus to get maximal coverage of diphones five of the sentences read by each speaker are also read by six other speakers for comparability the remaining three sentences read by each speaker were unique to that speaker for coverage nltk includes a sample from the timit corpus you can access its documentation in the usual way using help nltk corpus timit print nltk corpus timit fileids to see a list of the recorded utterances in the corpus sample each file name has internal structure as shown in figure structure of a timit identifier each recording is labeled using a string made up of the speaker dialect region gender speaker identifier sentence type and sentence identifier each item has a phonetic transcription which can be accessed using the phones method we can access the corresponding word tokens in the customary way both access methods permit an optional argument offset true which includes the start and end offsets of the corresponding span in the audio file in addition to this text data timit includes a lexicon that provides the canonical pronunciation of every word which can be compared with a particular utterance this gives us a sense of what a speech processing system would have to do in producing or recognizing speech in this particular dialect new england finally timit includes demographic data about the speakers permitting fine grained study of vocal social and gender characteristics notable design features timit illustrates several key features of corpus design first the corpus contains two layers of annotation at the phonetic and orthographic levels in general a text or speech corpus may be annotated at many different linguistic levels including morphological syntactic and discourse levels moreover even at a given level there may be different labeling schemes or even disagreement amongst annotators such that we want to represent multiple versions a second property of timit is its balance across multiple dimensions of variation for coverage of dialect regions and diphones the inclusion of speaker demographics brings in many more independent variables that may help to account for variation in the data and which facilitate later uses of the corpus for purposes that were not envisaged when the corpus was created such as sociolinguistics a third property is that there is a sharp division between the original linguistic event captured as an audio recording and the annotations of that event the same holds true of text corpora in the sense that the original text usually has an external source and is considered to be an immutable artifact any transformations of that artifact which involve human judgment even something as simple as tokenization are subject to later revision thus it is important to retain the source material in a form that is as close to the original as possible figure structure of the published timit corpus the cd rom contains doc train and test directories at the top level the train and test directories both have sub directories one per dialect region each of these contains further subdirectories one per speaker the contents of the directory for female speaker aks are listed showing wav files accompanied by a text transcription a word aligned transcription and a phonetic transcription a fourth feature of timit is the hierarchical structure of the corpus with files per sentence and sentences for each of speakers there are files these are organized into a tree structure shown schematically in at the top level there is a split between training and testing sets which gives away its intended use for developing and evaluating statistical models finally notice that even though timit is a speech corpus its transcriptions and associated data are just text and can be processed using programs just like any other text corpus therefore many of the computational methods described in this book are applicable moreover notice that all of the data types included in the timit corpus fall into the two basic categories of lexicon and text which we will discuss below even the speaker demographics data is just another instance of the lexicon data type this last observation is less surprising when we consider that text and record structures are the primary domains for the two subfields of computer science that focus on data management namely text retrieval and databases a notable feature of linguistic data management is that usually brings both data types together and that it can draw on results and techniques from both fields fundamental data types figure basic linguistic data types lexicons and texts amid their diversity lexicons have a record structure while annotated texts have a temporal organization despite its complexity the timit corpus only contains two fundamental data types namely lexicons and texts as we saw in most lexical resources can be represented using a record structure i e a key plus one or more fields as shown in a lexical resource could be a conventional dictionary or comparative wordlist as illustrated it could also be a phrasal lexicon where the key field is a phrase rather than a single word a thesaurus also consists of record structured data where we look up entries via non key fields that correspond to topics we can also construct special tabulations known as paradigms to illustrate contrasts and systematic variation as shown in for three verbs timit is speaker table is also a kind of lexicon at the most abstract level a text is a representation of a real or fictional speech event and the time course of that event carries over into the text itself a text could be a small unit such as a word or sentence or a complete narrative or dialogue it may come with annotations such as part of speech tags morphological analysis discourse structure and so forth as we saw in the iob tagging technique it is possible to represent higher level constituents using tags on individual words thus the abstraction of text shown in is sufficient despite the complexities and idiosyncrasies of individual corpora at base they are collections of texts together with record structured data the contents of a corpus are often biased towards one or other of these types for emple the brown corpus contains text files but we still use a table to relate the files to different genres at the other end of the spectrum wordnet contains synset records yet it incorporates many emple sentences mini texts to illustrate word usages timit is an interesting mid point on this spectrum containing substantial free standing material of both the text and lexicon types the life cycle of a corpus corpora are not born fully formed but involve careful preparation and input from many people over an extended period raw data needs to be collected cleaned up documented and stored in a systematic structure various layers of annotation might be applied some requiring specialized knowledge of the morphology or syntax of the language success at this stage depends on creating an efficient workflow involving appropriate tools and format converters quality control procedures can be put in place to find inconsistencies in the annotations and to ensure the highest possible level of inter annotator agreement because of the scale and complexity of the task large corpora may take years to prepare and involve tens or hundreds of person years of effort in this section we briefly review the various stages in the life cycle of a corpus three corpus creation scenarios in one type of corpus the design unfolds over in the course of the creator s explorations this is the pattern typical of traditional field linguistics in which material from elicitation sessions is analyzed as it is gathered with tomorrow s elicitation often based on questions that arise in analyzing today s the resulting corpus is then used during subsequent years of research and may serve as an archival resource indefinitely computerization is an obvious boon to work of this type as exemplified by the popular program shoebox now over two decades old and re released as toolbox see other software tools even simple word processors and spreadsheets are routinely used to acquire the data in the next section we will look at how to extract data from these sources another corpus creation scenario is typical of experimental research where a body of carefully designed material is collected from a range of human subjects then analyzed to evaluate a hypothesis or develop a technology it has become common for such databases to be shared and re used within a laboratory or company and often to be published more widely corpora of this type are the basis of the common task method of research management which over the past two decades has become the norm in government funded research programs in language technology we have already encountered many such corpora in the earlier chapters we will see how to write python programs to implement the kinds of curation tasks that are necessary before such corpora are published finally there are efforts to gather a reference corpus for a particular language such as the american national corpus anc and the british national corpus bnc here the goal has been to produce a comprehensive record of the many forms styles and uses of a language apart from the sheer challenge of scale there is a heavy reliance on automatic annotation tools together with post editing to fix any errors however we can write programs to locate and repair the errors and also to analyze the corpus for balance quality control good tools for automatic and manual preparation of data are essential however the creation of a high quality corpus depends just as much on such mundane things as documentation training and workflow annotation guidelines define the task and document the markup conventions they may be regularly updated to cover difficult cases along with new rules that are devised to achieve more consistent annotations annotators need to be trained in the procedures including methods for resolving cases not covered in the guidelines a workflow needs to be established possibly with supporting software to keep track of which files have been initialized annotated validated manually checked and so on there may be multiple layers of annotation provided by different specialists cases of uncertainty or disagreement may require adjudication large annotation tasks require multiple annotators which raises the problem of achieving consistency how consistently can a group of annotators perform we can easily measure consistency by having a portion of the source material independently annotated by two people this may reveal shortcomings in the guidelines or differing abilities with the annotation task in cases where quality is paramount the entire corpus can be annotated twice and any inconsistencies adjudicated by an expert it is considered best practice to report the inter annotator agreement that was achieved for a corpus e g by double annotating of the corpus this score serves as a helpful upper bound on the expected performance of any automatic system that is trained on this corpus caution care should be exercised when interpreting an inter annotator agreement score since annotation tasks vary greatly in their difficulty for emple agreement would be a terrible score for part of speech tagging but an exceptional score for semantic role labeling the kappa coefficient k measures agreement between two people making category judgments correcting for expected chance agreement for emple suppose an item is to be annotated and four coding options are equally likely then two people coding randomly would be expected to agree of the time thus an agreement of will be assigned k and better levels of agreement will be scaled accordingly for an agreement of we would get k as is a third of the way from to many other agreement measures exist see help nltk metrics agreement for details figure three segmentations of a sequence the small rectangles represent characters words sentences in short any sequence which might be divided into linguistic units s and s are in close agreement but both differ significantly from s we can also measure the agreement between two independent segmentations of language input e g for tokenization sentence segmentation named entity detection in we see three possible segmentations of a sequence of items which might have been produced by annotators or programs although none of them agree ectly s and s are in close agreement and we would like a suitable measure windowdiff is a simple algorithm for evaluating the agreement of two segmentations by running a sliding window over the data and awarding partial credit for near misses if we preprocess our tokens into a sequence of zeros and ones to record when a token is followed by a boundary we can represent the segmentations as strings and apply the windowdiff scorer in the above emple the window had a size of the windowdiff computation slides this window across a pair of strings at each position it totals up the number of boundaries found inside this window for both strings then computes the difference these differences are then summed we can increase or shrink the window size to control the sensitivity of the measure curation vs evolution as large corpora are published researchers are increasingly likely to base their investigations on balanced focused subsets that were derived from corpora produced for entirely different reasons for instance the switchboard database originally collected for speaker identification research has since been used as the basis for published studies in speech recognition word pronunciation disfluency syntax intonation and discourse structure the motivations for recycling linguistic corpora include the desire to save time and effort the desire to work on material available to others for replication and sometimes a desire to study more naturalistic forms of linguistic behavior than would be possible otherwise the process of choosing a subset for such a study may count as a non trivial contribution in itself in addition to selecting an appropriate subset of a corpus this new work could involve reformatting a text file e g converting to xml renaming files retokenizing the text selecting a subset of the data to enrich and so forth multiple research groups might do this work independently as illustrated in at a later date should someone want to combine sources of information from different versions the task will probably be extremely onerous figure evolution of a corpus over time after a corpus is published research groups will use it independently selecting and enriching different pieces later research that seeks to integrate separate annotations confronts the difficult challenge of aligning the annotations the task of using derived corpora is made even more difficult by the lack of any record about how the derived version was created and which version is the most up to date an alternative to this chaotic situation is for a corpus to be centrally curated and for committees of experts to revise and extend it at periodic intervals considering submissions from third parties and publishing new releases from time to time print dictionaries and national corpora may be centrally curated in this way however for most corpora this model is simply impractical a middle course is for the original corpus publication to have a scheme for identifying any sub part each sentence tree or lexical entry could have a globally unique identifier and each token node or field respectively could have a relative offset annotations including segmentations could reference the source using this identifier scheme a method which is known as standoff annotation this way new annotations could be distributed independently of the source and multiple independent annotations of the same source could be compared and updated without touching the source if the corpus publication is provided in multiple versions the version number or date could be part of the identification scheme a table of correspondences between identifiers across editions of the corpus would permit any standoff annotations to be updated easily caution sometimes an updated corpus contains revisions of base material that has been externally annotated tokens might be split or merged and constituents may have been rearranged there may not be a one to one correspondence between old and new identifiers it is better to cause standoff annotations to break on such components of the new version than to silently allow their identifiers to refer to incorrect locations acquiring data obtaining data from the web the web is a rich source of data for language analysis purposes we have already discussed methods for accessing individual files rss feeds and search engine results see however in some cases we want to obtain large quantities of web text the simplest approach is to obtain a published corpus of web text the acl special interest group on web as corpus sigwac maintains a list of resources at http www sigwac org uk the advantage of using a well defined web corpus is that they are documented stable and permit reproducible experimentation if the desired content is localized to a particular website there are many utilities for capturing all the accessible contents of a site such as gnu wget http www gnu org software wget for maximal flexibility and control a web crawler can be used such as heritrix http crawler archive org crawlers permit fine grained control over where to look which links to follow and how to organize the results croft metzler strohman for emple if we want to compile a bilingual text collection having corresponding pairs of documents in each language the crawler needs to detect the structure of the site in order to extract the correspondence between the documents and it needs to organize the downloaded pages in such a way that the correspondence is captured it might be tempting to write your own web crawler but there are dozens of pitfalls to do with detecting mime types converting relative to absolute urls avoiding getting trapped in cyclic link structures dealing with network latencies avoiding overloading the site or being banned from accessing the site and so on obtaining data from word processor files word processing software is often used in the manual preparation of texts and lexicons in projects that have limited computational infrastructure such projects often provide templates for data entry though the word processing software does not ensure that the data is correctly structured for emple each text may be required to have a title and date similarly each lexical entry may have certain obligatory fields as the data grows in size and complexity a larger proportion of time may be spent maintaining its consistency how can we extract the content of such files so that we can manipulate it in external programs moreover how can we validate the content of these files to help authors create well structured data so that the quality of the data can be maximized in the context of the original authoring process consider a dictionary in which each entry has a part of speech field drawn from a set of possibilities displayed after the pronunciation field and rendered in point bold no conventional word processor has search or macro functions capable of verifying that all part of speech fields have been correctly entered and displayed this task requires exhaustive manual checking if the word processor permits the document to be saved in a non proprietary format such as text html or xml we can sometimes write programs to do this checking automatically consider the following fragment of a lexical entry sleep sli p v i condition of body and mind we can enter this in msword then save as web page then inspect the resulting html file p class msonormal sleep span style mso spacerun yes span span class spelle sli p span span style mso spacerun yes span b span style font size pt v i span b span style mso spacerun yes span i a condition of body and mind o p o p i p observe that the entry is represented as an html paragraph using the p element and that the part of speech appears inside a span style font size pt element the following program defines the set of legal parts of speech legal pos then it extracts all point content from the dict htm file and stores it in the set used pos observe that the search pattern contains a parenthesized sub expression only the material that matches this sub expression is returned by re findall finally the program constructs the set of illegal parts of speech as used pos legal pos this simple program represents the tip of the iceberg we can develop sophisticated tools to check the consistency of word processor files and report errors so that the maintainer of the dictionary can correct the original file using the original word processor once we know the data is correctly formatted we can write other programs to convert the data into a different format the program in strips out the html markup using the beautifulsoup library extracts the words and their pronunciations and generates output in comma separated value csv format with gzip open fn gz wb as f out f out write bytes s utf obtaining data from spreadsheets and databases spreadsheets are often used for acquiring wordlists or paradigms for emple a comparative wordlist may be created using a spreadsheet with a row for each cognate set and a column for each language cf nltk corpus swadesh and www rosettaproject org most spreadsheet software can export their data in csv comma separated value format as we see below it is easy for python programs to access these using the csv module sometimes lexicons are stored in a full fledged relational database when properly normalized these databases can ensure the validity of the data for emple we can require that all parts of speech come from a specified vocabulary by declaring that the part of speech field is an enumerated type or a foreign key that references a separate part of speech table however the relational model requires the structure of the data the schema be declared in advance and this runs counter to the dominant approach to structuring linguistic data which is highly exploratory fields which were assumed to be obligatory and unique often turn out to be optional and repeatable a relational database can accommodate this when it is fully known in advance however if it is not or if just about every property turns out to be optional or repeatable the relational approach is unworkable nevertheless when our goal is simply to extract the contents from a database it is enough to dump out the tables or sql query results in csv format and load them into our program our program might perform a linguistically motivated query which cannot be expressed in sql e g select all words that appear in emple sentences for which no dictionary entry is provided for this task we would need to extract enough information from a record for it to be uniquely identified along with the headwords and emple sentences let suppose this information was now available in a csv file dict csv sleep sli p v i a condition of body and mind walk wo k v intr progress by lifting and setting down each foot wake weik intrans cease to sleep now we can express this query as shown below this information would then guide the ongoing work to enrich the lexicon work that updates the content of the relational database converting data formats annotated linguistic data rarely arrives in the most convenient format and it is often necessary to perform various kinds of format conversion converting between character encodings has already been discussed see here we focus on the structure of the data in the simplest case the input and output formats are isomorphic for instance we might be converting lexical data from toolbox format to xml and it is straightforward to transliterate the entries one at a time the structure of the data is reflected in the structure of the required program a for loop whose body takes care of a single entry in another common case the output is a digested form of the input such as an inverted file index here it is necessary to build an index structure in memory see then write it to a file in the desired format the following emple constructs an index that maps the words of a dictionary definition to the corresponding lexeme for each lexical entry having tokenized the definition text and discarded short words once the index has been constructed we open a file and then iterate over the index entries to write out the lines in the required format the resulting file dict idx contains the following lines with a larger dictionary we would expect to find multiple lexemes listed for each index entry body sleep cease wake condition sleep down walk each walk foot walk lifting walk mind sleep progress walk setting walk sleep wake in some cases the input and output data both consist of two or more dimensions for instance the input might be a set of files each containing a single column of word frequency data the required output might be a two dimensional table in which the original columns appear as rows in such cases we populate an internal data structure by filling up one column at a time then read off the data one row at a time as we write data to the output file in the most vexing cases the source and target formats have slightly different coverage of the domain and information is unavoidably lost when translating between them for emple we could combine multiple toolbox files to create a single csv file containing a comparative wordlist loosing all but the lx field of the input files if the csv file was later modified it would be a labor intensive process to inject the changes into the original toolbox files a partial solution to this round tripping problem is to associate explicit identifiers each linguistic object and to propagate the identifiers with the objects deciding which layers of annotation to include published corpora vary greatly in the richness of the information they contain at a minimum a corpus will typically contain at least a sequence of sound or orthographic symbols at the other end of the spectrum a corpus could contain a large amount of information about the syntactic structure morphology prosody and semantic content of every sentence plus annotation of discourse relations or dialogue acts these extra layers of annotation may be just what someone needs for performing a particular data analysis task for emple it may be much easier to find a given linguistic pattern if we can search for specific syntactic structures and it may be easier to categorize a linguistic pattern if every word has been tagged with its sense here are some commonly provided annotation layers word tokenization the orthographic form of text does not unambiguously identify its tokens a tokenized and normalized version in addition to the conventional orthographic version may be a very convenient resource sentence segmentation as we saw in sentence segmentation can be more difficult than it seems some corpora therefore use explicit annotations to mark sentence segmentation paragraph segmentation paragraphs and other structural elements headings chapters etc may be explicitly annotated part of speech the syntactic category of each word in a document syntactic structure a tree structure showing the constituent structure of a sentence shallow semantics named entity and coreference annotations semantic role labels dialogue and discourse dialogue act tags rhetorical structure unfortunately there is not much consistency between existing corpora in how they represent their annotations however two general classes of annotation representation should be distinguished inline annotation modifies the original document by inserting special symbols or control sequences that carry the annotated information for emple when part of speech tagging a document the string fly might be replaced with the string fly nn to indicate that the word fly is a noun in this context in contrast standoff annotation does not modify the original document but instead creates a new file that adds annotation information using pointers that reference the original document for emple this new document might contain the string token id pos nn to indicate that token is a noun we would want to be sure that the tokenization itself was not subject to change since it would cause such references to break silently standards and tools for a corpus to be widely useful it needs to be available in a widely supported format however the cutting edge of nlp research depends on new kinds of annotations which by definition are not widely supported in general adequate tools for creation publication and use of linguistic data are not widely available most projects must develop their own set of tools for internal use which is no help to others who lack the necessary resources furthermore we do not have adequate generally accepted standards for expressing the structure and content of corpora without such standards general purpose tools are impossible though at the same time without available tools adequate standards are unlikely to be developed used and accepted one response to this situation has been to forge ahead with developing a generic format which is sufficiently expressive to capture a wide variety of annotation types see for emples the challenge for nlp is to write programs that cope with the generality of such formats for emple if the programming task involves tree data and the file format permits arbitrary directed graphs then input data must be validated to check for tree properties such as rootedness connectedness and acyclicity if the input files contain other layers of annotation the program would need to know how to ignore them when the data was loaded but not invalidate or obliterate those layers when the tree data was saved back to the file another response has been to write one off scripts to manipulate corpus formats such scripts litter the filespaces of many nlp researchers nltk corpus readers are a more systematic approach founded on the premise that the work of parsing a corpus format should only be done once per programming language figure a common format vs a common interface instead of focussing on a common format we believe it is more promising to develop a common interface cf nltk corpus consider the case of treebanks an important corpus type for work in nlp there are many ways to store a phrase structure tree in a file we can use nested parentheses or nested xml elements or a dependency notation with a child id parent id pair on each line or an xml version of the dependency notation etc however in each case the logical structure is almost the same it is much easier to devise a common interface that allows application programmers to write code to access tree data using methods such as children leaves depth and so forth note that this approach follows accepted practice within computer science viz abstract data types object oriented design and the three layer architecture the last of these from the world of relational databases allows end user applications to use a common model the relational model and a common language sql to abstract away from the idiosyncrasies of file storage and allowing innovations in filesystem technologies to occur without disturbing end user applications in the same way a common corpus interface insulates application programs from data formats in this context when creating a new corpus for dissemination it is expedient to use an existing widely used format wherever possible when this is not possible the corpus could be accompanied with software such as an nltk corpus module that supports existing interface methods special considerations when working with endangered languages the importance of language to science and the arts is matched in significance by the cultural treasure embodied in language each of the world human languages is rich in unique respects in its oral histories and creation legends down to its grammatical constructions and its very words and their nuances of meaning threatened remnant cultures have words to distinguish plant subspecies according to therapeutic uses that are unknown to science languages evolve over time as they come into contact with each other and each one provides a unique window onto human pre history in many parts of the world small linguistic variations from one town to the next add up to a completely different language in the space of a half hour drive for its breathtaking complexity and diversity human language is as a colorful tapestry stretching through time and space however most of the world languages face extinction in response to this many linguists are hard at work documenting the languages constructing rich records of this important facet of the world linguistic heritage what can the field of nlp offer to help with this effort developing taggers parsers named entity recognizers etc is not an early priority and there is usually insufficient data for developing such tools in any case instead the most frequently voiced need is to have better tools for collecting and curating data with a focus on texts and lexicons on the face of things it should be a straightforward matter to start collecting texts in an endangered language even if we ignore vexed issues such as who owns the texts and sensitivities surrounding cultural knowledge contained in the texts there is the obvious practical issue of transcription most languages lack a standard orthography when a language has no literary tradition the conventions of spelling and punctuation are not well established therefore it is common practice to create a lexicon in tandem with a text collection continually updating the lexicon as new words appear in the texts this work could be done using a text processor for the texts and a spreadsheet for the lexicon better still sil free linguistic software toolbox and fieldworks provide sophisticated support for integrated creation of texts and lexicons when speakers of the language in question are trained to enter texts themselves a common obstacle is an overriding concern for correct spelling having a lexicon greatly helps this process but we need to have lookup methods that do not assume someone can determine the citation form of an arbitrary word the problem may be acute for languages having a complex morphology that includes prefixes in such cases it helps to tag lexical items with semantic domains and to permit lookup by semantic domain or by gloss permitting lookup by pronunciation similarity is also a big help here is a simple demonstration of how to do this the first step is to identify confusible letter sequences and map complex versions to simpler versions we might also notice that the relative order of letters within a cluster of consonants is a source of spelling errors and so we normalize the order of consonants next we create a mapping from signatures to words for all the words in our lexicon we can use this to get candidate corrections for a given input word but we must first compute that word signature finally we should rank the results in terms of similarity with the original word this is done by the function rank the only remaining function provides a simple interface to the user this is just one illustration where a simple program can facilitate access to lexical data in a context where the writing system of a language may not be standardized or where users of the language may not have a good command of spellings other simple applications of nlp in this area include building indexes to facilitate access to data gleaning wordlists from texts locating emples of word usage in constructing a lexicon detecting prevalent or exceptional patterns in poorly understood data and performing specialized validation on data created using various linguistic software tools we will return to the last of these in working with xml the extensible markup language xml provides a framework for designing domain specific markup languages it is sometimes used for representing annotated text and for lexical resources unlike html with its predefined tags xml permits us to make up our own tags unlike a database xml permits us to create data without first specifying its structure and it permits us to have optional and repeatable elements in this section we briefly review some features of xml that are relevant for representing linguistic data and show how to access data stored in xml files using python programs using xml for linguistic structures thanks to its flexibility and extensibility xml is a natural choice for representing linguistic structures here is an emple of a simple lexical entry it consists of a series of xml tags enclosed in angle brackets each opening tag like gloss is matched with a closing tag like gloss together they constitute an xml element the above emple has been laid out nicely using whitespace but it could equally have been put on a single long line our approach to processing xml will usually not be sensitive to whitespace in order for xml to be well formed all opening tags must have corresponding closing tags at the same level of nesting i e the xml document must be a well formed tree xml permits us to repeat elements e g to add another gloss field as we see below we will use different whitespace to underscore the point that layout does not matter a further step might be to link our lexicon to some external resource such as wordnet using external identifiers in we group the gloss and a synset identifier inside a new element which we have called sense alternatively we could have represented the synset identifier using an xml attribute without the need for any nested structure as in this illustrates some of the flexibility of xml if it seems somewhat arbitrary that s because it is following the rules of xml we can invent new attribute names and nest them as deeply as we like we can repeat elements leave them out and put them in a different order each time we can have fields whose presence depends on the value of some other field e g if the part of speech is verb then the entry can have a past tense element to hold the past tense of the verb but if the part of speech is noun no past tense element is permitted to impose some order over all this freedom we can constrain the structure of an xml file using a schema which is a declaration akin to a context free grammar tools exist for testing the validity of an xml file with respect to a schema the role of xml we can use xml to represent many kinds of linguistic information however the flexibility comes at a price each time we introduce a complication such as by permitting an element to be optional or repeated we make more work for any program that accesses the data we also make it more difficult to check the validity of the data or to interrogate the data using one of the xml query languages thus using xml to represent linguistic structures does not magically solve the data modeling problem we still have to work out how to structure the data then define that structure with a schema and then write programs to read and write the format and convert it to other formats similarly we still need to follow some standard principles concerning data normalization it is wise to avoid making duplicate copies of the same information so that we do not end up with inconsistent data when only one copy is changed for emple a cross reference that was represented as xref headword xref would duplicate the storage of the headword of some other lexical entry and the link would break if the copy of the string at the other location was modified existential dependencies between information types need to be modeled so that we ca not create elements without a home for emple if sense definitions cannot exist independently of a lexical entry the sense element can be nested inside the entry element many to many relations need to be abstracted out of hierarchical structures for emple if a word can have many corresponding senses and a sense can have several corresponding words then both words and senses must be enumerated separately as must the list of word sense pairings this complex structure might even be split across three separate xml files as we can see although xml provides us with a convenient format accompanied by an extensive collection of tools it offers no panacea the elementtree interface python elementtree module provides a convenient way to access data stored in xml files elementtree is part of python standard library since python and is also provided as part of nltk in case you are using python we will illustrate the use of elementtree using a collection of shakespeare plays that have been formatted using xml let load the xml file and inspect the raw data first at the top of the file where we see some xml headers and the name of a schema called play dtd followed by the root element play we pick it up again at the start of act some blank lines have been omitted from the output we have just accessed the xml data as a string as we can see the string at the start of act contains xml tags for title scene stage directions and so forth the next step is to process the file contents as structured xml data using elementtree we are processing a file a multi line string and building a tree so its not surprising that the method name is parse the variable merchant contains an xml element play this element has internal structure we can use an index to get its first child a title element we can also see the text content of this element the title of the play to get a list of all the child elements we use the getchildren method the play consists of a title the personae a scene description a subtitle and five acts each act has a title and some scenes and each scene consists of speeches which are made up of lines a structure with four levels of nesting let dig down into act iv note your turn repeat some of the above methods for one of the other shakespeare plays included in the corpus such as romeo and juliet or macbeth for a list see nltk corpus shakespeare fileids although we can access the entire tree this way it is more convenient to search for sub elements with particular names recall that the elements at the top level have several types we can iterate over just the types we are interested in such as the acts using merchant findall act here is an emple of doing such tag specific searches at every level of nesting instead of navigating each step of the way down the hierarchy we can search for particular embedded elements for emple let emine the sequence of speakers we can use a frequency distribution to see who has the most to say we can also look for patterns in who follows who in the dialogues since there s speakers we need to reduce the vocabulary to a manageable size first using the method described in ignoring the entries for exchanges between people other than the top labeled oth the largest value suggests that portia and bassanio have the most frequent interactions using elementtree for accessing toolbox data in we saw a simple interface for accessing toolbox data a popular and well established format used by linguists for managing data in this section we discuss a variety of techniques for manipulating toolbox data in ways that are not supported by the toolbox software the methods we discuss could be applied to other record structured data regardless of the actual file format we can use the toolbox xml method to access a toolbox file and load it into an elementtree object this file contains a lexicon for the rotokas language of papua new guinea there are two ways to access the contents of the lexicon object by indexes and by paths indexes use the familiar syntax thus lexicon returns entry number which is actually the fourth entry counting from zero lexicon returns its first field the second way to access the contents of the lexicon object uses paths the lexicon is a series of record objects each containing a series of field objects such as lx and ps we can conveniently address all of the lexemes using the path record lx here we use the findall function to search for any matches to the path record lx and we access the text content of the element normalizing it to lowercase let view the toolbox data in xml format the write method of elementtree expects a file object we usually create one of these using python built in open function in order to see the output displayed on the screen we can use a special pre defined file object called stdout standard output defined in python sys module formatting entries we can use the same idea we saw above to generate html tables instead of plain text this would be useful for publishing a toolbox lexicon on the web it produces html elements table tr table row and td table data working with toolbox data given the popularity of toolbox amongst linguists we will discuss some further methods for working with toolbox data many of the methods discussed in previous chapters such as counting building frequency distributions tabulating co occurrences can be applied to the content of toolbox entries for emple we can trivially compute the average number of fields for each entry in this section we will discuss two tasks that arise in the context of documentary linguistics neither of which is supported by the toolbox software adding a field to each entry it is often convenient to add new fields that are derived automatically from existing ones such fields often facilitate search and analysis for instance in we define a function cv which maps a string of consonants and vowels to the corresponding cv sequence e g kakapua would map to cvcvcvv this mapping has four steps first the string is converted to lowercase then we replace any non alphabetic characters a z with an underscore next we replace all vowels with v finally anything that is not a v or an underscore must be a consonant so we replace it with a c now we can scan the lexicon and add a new cv field after every lx field shows what this does to a particular entry note the last line of output which shows the new cv field note if a toolbox file is being continually updated the program in code add cv field will need to be run more than once it would be possible to modify add cv field to modify the contents of an existing entry however it is a safer practice to use such programs to create enriched files for the purpose of data analysis without replacing the manually curated source files validating a toolbox lexicon many lexicons in toolbox format do not conform to any particular schema some entries may include extra fields or may order existing fields in a new way manually inspecting thousands of lexical entries is not practicable however we can easily identify frequent field sequences with the help of a counter after inspecting these field sequences we could devise a context free grammar for lexical entries the grammar in uses the cfg format we saw in such a grammar models the implicit nested structure of toolbox entries and builds a tree structure in which the leaves of the tree are individual field names finally we iterate over the entries and report their conformance with the grammar as shown in those that are accepted by the grammar are prefixed with a and those that are rejected are prefixed with a during the process of developing such a grammar it helps to filter out some of the tags another approach would be to use a chunk parser since these are much more effective at identifying partial structures and can report the partial structures that have been identified in we set up a chunk grammar for the entries of a lexicon then parse each entry a sample of the output from this program is shown in figure xml representation of a lexical entry resulting from chunk parsing a toolbox record describing language resources using olac metadata members of the nlp community have a common need for discovering language resources with high precision and recall the solution which has been developed by the digital libraries community involves metadata aggregation what is metadata the simplest definition of metadata is structured data about data metadata is descriptive information about an object or resource whether it be physical or electronic while the term metadata itself is relatively new the underlying concepts behind metadata have been in use for as long as collections of information have been organized library catalogs represent a well established type of metadata they have served as collection management and resource discovery tools for decades metadata can be generated either by hand or generated automatically using software the dublin core metadata initiative began in to develop conventions for resource discovery on the web the dublin core metadata elements represent a broad interdisciplinary consensus about the core set of elements that are likely to be widely useful to support resource discovery the dublin core consists of metadata elements where each element is optional and repeatable title creator subject description publisher contributor date type format identifier source language relation coverage rights this metadata set can be used to describe resources that exist in digital or traditional formats the open archives initiative oai provides a common framework across digital repositories of scholarly materials regardless of their type including documents data software recordings physical artifacts digital surrogates and so forth each repository consists of a network accessible server offering public access to archived items each item has a unique identifier and is associated with a dublin core metadata record and possibly additional records in other formats the oai defines a protocol for metadata search services to harvest the contents of repositories olac open language archives community the open language archives community olac is an international partnership of institutions and individuals who are creating a worldwide virtual library of language resources by i developing consensus on best current practice for the digital archiving of language resources and ii developing a network of interoperating repositories and services for housing and accessing such resources olac home on the web is at http www language archives org olac metadata is a standard for describing language resources uniform description across repositories is ensured by limiting the values of certain metadata elements to the use of terms from controlled vocabularies olac metadata can be used to describe data and tools in both physical and digital formats olac metadata extends the dublin core metadata set a widely accepted standard for describing resources of all types to this core set olac adds descriptors to cover fundamental properties of language resources such as subject language and linguistic type here is an emple of a complete olac record xml version encoding utf olac olac xmlns olac http www language archives org olac xmlns http purl org dc elements xmlns dcterms http purl org dc terms xmlns xsi http www w org xmlschema instance xsi schemalocation http www language archives org olac http www language archives org olac olac xsd title a grammar of kayardild with comparative notes on tangkic title creator evans nicholas d creator subject kayardild grammar subject subject xsi type olac language olac code gyd kayardild subject language xsi type olac language olac code en english language description kayardild grammar isbn description publisher berlin mouton de gruyter publisher contributor xsi type olac role olac code author nicholas evans contributor format hardcover pages format relation related to isbn relation coverage australia coverage type xsi type olac linguistic type olac code language description type xsi type dcterms dcmitype text type olac olac participating language archives publish their catalogs in an xml format and these records are regularly harvested by olac services using the oai protocol in addition to this software infrastructure olac has documented a series of best practices for describing language resources through a process that involved extended consultation with the language resources community e g see http www language archives org rec bpr html olac repositories can be searched using a query engine on the olac website searching for german lexicon finds the following resources amongst others callhome german lexicon http www language archives org item oai www ldc upenn edu ldc l multilex multilingual lexicon http www language archives org item oai elra icp inpg fr m slelex siemens phonetic lexicon http www language archives org item oai elra icp inpg fr s searching for korean finds a newswire corpus a treebank a lexicon a child language corpus interlinear glossed texts it also finds software including a syntactic analyzer and a morphological analyzer observe that the above urls include a substring of the form oai www ldc upenn edu ldc l this is an oai identifier using a uri scheme registered with icann the internet corporation for assigned names and numbers these identifiers have the format oai archive local id where oai is the name of the uri scheme archive is an archive identifier such as www ldc upenn edu and local id is the resource identifier assigned by the archive e g ldc l given an oai identifier for an olac resource it is possible to retrieve the complete xml record for the resource using a url of the following form http www language archives org static records oai archive local id disseminating language resources the linguistic data consortium hosts the nltk data repository an open access archive where community members can upload corpora and saved models these resources can be easily accessed using nltk downloader tool summary fundamental data types present in most corpora are annotated texts and lexicons texts have a temporal structure while lexicons have a record structure the lifecycle of a corpus includes data collection annotation quality control and publication the lifecycle continues after publication as the corpus is modified and enriched during the course of research corpus development involves a balance between capturing a representative sample of language usage and capturing enough material from any one source or genre to be useful multiplying out the dimensions of variability is usually not feasible because of resource limitations xml provides a useful format for the storage and interchange of linguistic data but provides no shortcuts for solving pervasive data modeling problems toolbox format is widely used in language documentation projects we can write programs to support the curation of toolbox files and to convert them to xml the open language archives community olac provides an infrastructure for documenting and discovering language resources further reading extra materials for this chapter are posted at http nltk org including links to freely available resources on the web the primary sources of linguistic corpora are the linguistic data consortium and the european language resources agency both with extensive online catalogs more details concerning the major corpora mentioned in the chapter are available american national corpus reppen ide suderman british national corpus bnc thesaurus linguae graecae tlg child language data exchange system childes macwhinney timit s lamel william two special interest groups of the association for computational linguistics that organize regular workshops with published proceedings are sigwac which promotes the use of the web as a corpus and has sponsored the cleaneval task for removing html markup and sigann which is encouraging efforts towards interoperability of linguistic annotations full details of the toolbox data format are provided with the distribution buseman buseman early and with the latest distribution freely available from http www sil org computing toolbox for guidelines on the process of constructing a toolbox lexicon see http www sil org computing ddp more emples of our efforts with the toolbox are documented in tamanji hirotani hall robinson aumann bird dozens of other tools for linguistic data management are available some surveyed by bird simons see also the proceedings of the latech workshops on language technology for cultural heritage data there are many excellent resources for xml e g http zvon org and for writing python programs to work with xml many editors have xml modes xml formats for lexical information include olif http www olif net and lift http code google com p lift standard for a survey of linguistic annotation software see the linguistic annotation page at http www ldc upenn edu annotation the initial proposal for standoff annotation was thompson mckelvie an abstract data model for linguistic annotations called annotation graphs was proposed in bird liberman a general purpose ontology for linguistic description gold is documented at http www linguistics ontology org for guidance on planning and constructing a corpus see meyer farghaly more details of methods for scoring inter annotator agreement are available in artstein poesio pevzner hearst rotokas data was provided by stuart robinson and iu mien data was provided by greg aumann for more information about the open language archives community visit http www language archives org or see simons bird exercises in the new field appeared at the bottom of the entry modify this program so that it inserts the new subelement right after the lx field hint create the new cv field using element cv assign a text value to it then use the insert method of the parent element write a function that deletes a specified field from a lexical entry we could use this to sanitize our lexical data before giving it to others e g by removing fields containing irrelevant or uncertain content write a program that scans an html dictionary file to find entries having an illegal part of speech field and reports the headword for each entry write a program to find any parts of speech ps field that occurred less than ten times perhaps these are typing mistakes we saw a method for discovering cases of whole word reduplication write a function to find words that may contain partial reduplication use the re search method and the following regular expression we saw a method for adding a cv field there is an interesting issue with keeping this up to date when someone modifies the content of the lx field on which it is based write a version of this program to add a cv field replacing any existing cv field write a function to add a new field syl which gives a count of the number of syllables in the word write a function which displays the complete entry for a lexeme when the lexeme is incorrectly spelled it should display the entry for the most similarly spelled lexeme write a function that takes a lexicon and finds which pairs of consecutive fields are most frequent e g ps is often followed by pt this might help us to discover some of the structure of a lexical entry create a spreadsheet using office software containing one lexical entry per row consisting of a headword a part of speech and a gloss save the spreadsheet in csv format write python code to read the csv file and print it in toolbox format using lx for the headword ps for the part of speech and gl for the gloss index the words of shakespeare plays with the help of nltk index the resulting data structure should permit lookup on individual words such as music returning a list of references to acts scenes and speeches of the form where indicates act scene speech construct a conditional frequency distribution which records the word length for each speech in the merchant of venice conditioned on the name of the character e g cfd portia would give us the number of speeches by portia consisting of words obtain a comparative wordlist in csv format and write a program that prints those cognates having an edit distance of at least three from each other build an index of those lexemes which appear in emple sentences suppose the lexeme for a given entry is w then add a single cross reference field xrf to this entry referencing the headwords of other entries having emple sentences containing w do this for all entries and save the result as a toolbox format file write a recursive function to produce an xml representation for a tree with non terminals represented as xml elements and leaves represented as text content e g preface this is a book about natural language processing by natural language we mean a language that is used for everyday communication by humans languages like english hindi or portuguese in contrast to artificial languages such as programming languages and mathematical notations natural languages have evolved as they pass from generation to generation and are hard to pin down with explicit rules we will take natural language processing or nlp for short in a wide sense to cover any kind of computer manipulation of natural language at one extreme it could be as simple as counting word frequencies to compare different writing styles at the other extreme nlp involves understanding complete human utterances at least to the extent of being able to give useful responses to them technologies based on nlp are becoming increasingly widespread for emple phones and handheld computers support predictive text and handwriting recognition web search engines give access to information locked up in unstructured text machine translation allows us to retrieve texts written in chinese and read them in spanish text analysis enables us to detect sentiment in tweets and blogs by providing more natural human machine interfaces and more sophisticated access to stored information language processing has come to play a central role in the multilingual information society this book provides a highly accessible introduction to the field of nlp it can be used for individual study or as the textbook for a course on natural language processing or computational linguistics or as a supplement to courses in artificial intelligence text mining or corpus linguistics the book is intensely practical containing hundreds of fully worked emples and graded exercises the book is based on the python programming language together with an open source library called the natural language toolkit nltk nltk includes extensive software data and documentation all freely downloadable from http nltk org distributions are provided for windows macintosh and unix platforms we strongly encourage you to download python and nltk and try out the emples and exercises along the way word audience nlp is important for scientific economic social and cultural reasons nlp is experiencing rapid growth as its theories and methods are deployed in a variety of new language technologies for this reason it is important for a wide range of people to have a working knowledge of nlp within industry this includes people in human computer interaction business information analysis and web software development within academia it includes people in areas from humanities computing and corpus linguistics through to computer science and artificial intelligence to many people in academia nlp is known by the name of computational linguistics this book is intended for a diverse range of people who want to learn how to write programs that analyze written language regardless of previous programming experience new to programming nthe early chapters of the book are suitable for readers with no prior knowledge of programming so long as you are not afraid to tackle new concepts and develop new computing skills the book is full of emples that you can copy and try for yourself together with hundreds of graded exercises if you need a more general introduction to python see the list of python resources at http docs python org nnew to python experienced programmers can quickly learn enough python using this book to get immersed in natural language processing all relevant python features are carefully explained and exemplified and you will quickly come to appreciate python suitability for this application area the language index will help you locate relevant discussions in the book nalready dreaming in python nskim the python emples and dig into the interesting language analysis material that starts in you will soon be applying your skills to this fascinating domain emphasis this book is a practical introduction to nlp you will learn by emple write real programs and grasp the value of being able to test an idea through implementation if you have not learnt already this book will teach you programming unlike other programming books we provide extensive illustrations and exercises from nlp the approach we have taken is also principled in that we cover the theoretical underpinnings and do not shy away from careful linguistic and computational analysis we have tried to be pragmatic in striking a balance between theory and application identifying the connections and the tensions finally we recognize that you wo not get through this unless it is also pleasurable so we have tried to include many applications and emples that are interesting and entertaining sometimes whimsical note that this book is not a reference work its coverage of python and nlp is selective and presented in a tutorial style for reference material please consult the substantial quantity of searchable resources available at http python org and http nltk org this book is not an advanced computer science text the content ranges from introductory to intermediate and is directed at readers who want to learn how to analyze text using python and the natural language toolkit to learn about advanced algorithms implemented in nltk you can emine the python code linked from http nltk org and consult the other materials cited in this book what you will learn by digging into the material presented here you will learn how simple programs can help you manipulate and analyze language data and how to write these programs how key concepts from nlp and linguistics are used to describe and analyse language how data structures and algorithms are used in nlp how language data is stored in standard formats and how data can be used to evaluate the performance of nlp techniques depending on your background and your motivation for being interested in nlp you will gain different kinds of skills and knowledge from this book as set out in iii table iii skills and knowledge to be gained from reading this book depending on readers goals and background goals background in arts and humanities background in science and engineering nlanguage analysis manipulating large corpora exploring linguistic models and testing empirical claims using techniques in data modeling data mining and knowledge discovery to analyze natural language nlanguage technology building robust systems to perform linguistic tasks with technological applications using linguistic algorithms and data structures in robust language processing software organization the early chapters are organized in order of conceptual difficulty starting with a practical introduction to language processing that shows how to explore interesting bodies of text using tiny python programs chapters this is followed by a chapter on structured programming chapter that consolidates the programming topics scattered across the preceding chapters after this the pace picks up and we move on to a series of chapters covering fundamental topics in language processing tagging classification and information extraction chapters the next three chapters look at ways to parse a sentence recognize its syntactic structure and construct representations of meaning chapters the final chapter is devoted to linguistic data and how it can be managed effectively chapter the book concludes with an afterword briefly discussing the past and future of the field within each chapter we switch between different styles of presentation in one style natural language is the driver we analyze language explore linguistic concepts and use programming emples to support the discussion we often employ python constructs that have not been introduced systematically so you can see their purpose before delving into the details of how and why they work this is just like learning idiomatic expressions in a foreign language you are able to buy a nice pastry without first having learnt the intricacies of question formation in the other style of presentation the programming language will be the driver we will analyze programs explore algorithms and the linguistic emples will play a supporting role each chapter ends with a series of graded exercises which are useful for consolidating the material the exercises are graded according to the following scheme is for easy exercises that involve minor modifications to supplied code samples or other simple activities is for intermediate exercises that explore an aspect of the material in more depth requiring careful analysis and design is for difficult open ended tasks that will challenge your understanding of the material and force you to think independently readers new to programming should skip these each chapter has a further reading section and an online extras section at http nltk org with pointers to more advanced materials and online resources online versions of all the code emples are also available there why python python is a simple yet powerful programming language with excellent functionality for processing linguistic data python can be downloaded for free from http python org installers are available for all platforms here is a five line python program that processes file txt and prints all the words ending in ing for line in open file txt for word in line split if word endswith ing print word this program illustrates some of the main features of python first whitespace is used to nest lines of code thus the line starting with if falls inside the scope of the previous line starting with for this ensures that the ing test is performed for each word second python is object oriented each variable is an entity that has certain defined attributes and methods for emple the value of the variable line is more than a sequence of characters it is a string object that has a method or operation called split that we can use to break a line into its words to apply a method to an object we write the object name followed by a period followed by the method name i e line split third methods have arguments expressed inside parentheses for instance in the emple word endswith ing had the argument ing to indicate that we wanted words ending with ing and not something else finally and most importantly python is highly readable so much so that it is fairly easy to guess what the program does even if you have never written a program before we chose python because it has a shallow learning curve its syntax and semantics are transparent and it has good string handling functionality as an interpreted language python facilitates interactive exploration as an object oriented language python permits data and methods to be encapsulated and re used easily as a dynamic language python permits attributes to be added to objects on the fly and permits variables to be typed dynamically facilitating rapid development python comes with an extensive standard library including components for graphical programming numerical processing and web connectivity python is heavily used in industry scientific research and education around the world python is often praised for the way it facilitates productivity quality and maintainability of software a collection of python success stories is posted at http python org about success nltk defines an infrastructure that can be used to build nlp programs in python it provides basic classes for representing data relevant to natural language processing standard interfaces for performing tasks such as part of speech tagging syntactic parsing and text classification and standard implementations for each task which can be combined to solve complex problems nltk comes with extensive documentation in addition to this book the website at http nltk org provides api documentation that covers every module class and function in the toolkit specifying parameters and giving emples of usage python and nltk this version of the book has been updated to support python and nltk python includes some significant changes the print statement is now a function requiring parentheses nmany functions now return iterators instead of lists to save memory usage ninteger division returns a floating point number nall text is now unicode nstrings are formatted using the format method nfor a more detailed list of changes please see https docs python org dev whatsnew html there is a utility called to py which can convert your python code to python for details please see https docs python org library to html nltk also includes some pervasive changes many types are initialised from strings using a fromstring method nmany functions now return iterators instead of lists ncontextfreegrammar is now called cfg and weightedgrammar is now called pcfg nbatch tokenize is now called tokenize sents there are corresponding changes for batch taggers parsers and classifiers nsome implementations have been removed in favour of external packages or because they could not be maintained adequately nfor a more detailed list of changes please see https github com nltk nltk wiki porting your code to nltk software requirements to get the most out of this book you should install several free software packages current download pointers and instructions are available at http nltk org python the material presented in this book assumes that you are using python version or later note that nltk also works with python and nnltk the code emples in this book use nltk version subsequent releases of nltk will be backward compatible with nltk nnltk data this contains the linguistic corpora that are analyzed and processed in the book nnumpy recommended this is a scientific computing library with support for multidimensional arrays and linear algebra required for certain probability tagging clustering and classification tasks nmatplotlib recommended this is a d plotting library for data visualization and is used in some of the book code samples that produce line graphs and bar charts nstanford nlp tools n recommended nltk includes interfaces to the stanford nlp tools which are useful for large scale language processing see http nlp stanford edu software nnetworkx optional this is a library for storing and manipulating network structures consisting of nodes and edges for visualizing semantic networks also install the graphviz library nprover optional this is an automated theorem prover for first order and equational logic used to support inference in language processing natural language toolkit nltk nltk was originally created in as part of a computational linguistics course in the department of computer and information science at the university of pennsylvania since then it has been developed and expanded with the help of dozens of contributors it has now been adopted in courses in dozens of universities and serves as the basis of many research projects see viii for a list of the most important nltk modules table viii language processing tasks and corresponding nltk modules with emples of functionality language processing task nltk modules functionality naccessing corpora corpus standardized interfaces to corpora and lexicons nstring processing tokenize stem tokenizers sentence tokenizers stemmers ncollocation discovery collocations t test chi squared point wise mutual information npart of speech tagging tag n gram backoff brill hmm tnt nmachine learning classify cluster tbl decision tree maximum entropy naive bayes em k means nchunking chunk regular expression n gram named entity nparsing parse ccg chart feature based unification probabilistic dependency nsemantic interpretation sem inference lambda calculus first order logic model checking nevaluation metrics metrics precision recall agreement coefficients nprobability and estimation probability frequency distributions smoothed probability distributions napplications app chat graphical concordancer parsers wordnet browser chatbots nlinguistic fieldwork toolbox manipulate data in sil toolbox format nnltk was designed with four primary goals in mind simplicity to provide an intuitive framework along with substantial building blocks giving users a practical knowledge of nlp without getting bogged down in the tedious house keeping usually associated with processing annotated language data nconsistency to provide a uniform framework with consistent interfaces and data structures and easily guessable method names nextensibility to provide a structure into which new software modules can be easily accommodated including alternative implementations and competing approaches to the same task nmodularity to provide components that can be used independently without needing to understand the rest of the toolkit ncontrasting with these goals are three non requirements potentially useful qualities that we have deliberately avoided first while the toolkit provides a wide range of functions it is not encyclopedic it is a toolkit not a system and it will continue to evolve with the field of nlp second while the toolkit is efficient enough to support meaningful tasks it is not highly optimized for runtime performance such optimizations often involve more complex algorithms or implementations in lower level programming languages such as c or c this would make the software less readable and more difficult to install third we have tried to avoid clever programming tricks since we believe that clear implementations are preferable to ingenious yet indecipherable ones for instructors natural language processing is often taught within the confines of a single semester course at advanced undergraduate level or postgraduate level many instructors have found that it is difficult to cover both the theoretical and practical sides of the subject in such a short span of time some courses focus on theory to the exclusion of practical exercises and deprive students of the challenge and excitement of writing programs to automatically process language other courses are simply designed to teach programming for linguists and do not manage to cover any significant nlp content nltk was originally developed to address this problem making it feasible to cover a substantial amount of theory and practice within a single semester course even if students have no prior programming experience a significant fraction of any nlp syllabus deals with algorithms and data structures on their own these can be rather dry but nltk brings them to life with the help of interactive graphical user interfaces that make it possible to view algorithms step by step most nltk components include a demonstration that performs an interesting task without requiring any special input from the user an effective way to deliver the materials is through interactive presentation of the emples in this book entering them in a python session observing what they do and modifying them to explore some empirical or theoretical issue this book contains hundreds of exercises that can be used as the basis for student assignments the simplest exercises involve modifying a supplied program fragment in a specified way in order to answer a concrete question at the other end of the spectrum nltk provides a flexible framework for graduate level research projects with standard implementations of all the basic data structures and algorithms interfaces to dozens of widely used datasets corpora and a flexible and extensible architecture additional support for teaching using nltk is available on the nltk website we believe this book is unique in providing a comprehensive framework for students to learn about nlp in the context of learning to program what sets these materials apart is the tight coupling of the chapters and exercises with nltk giving students even those with no prior programming experience a practical introduction to nlp after completing these materials students will be ready to attempt one of the more advanced textbooks such as speech and language processing by jurafsky and martin prentice hall this book presents programming concepts in an unusual order beginning with a non trivial data type lists of strings then introducing non trivial control structures such as comprehensions and conditionals these idioms permit us to do useful language processing from the start once this motivation is in place we return to a systematic presentation of fundamental concepts such as strings loops files and so forth in this way we cover the same ground as more conventional approaches without expecting readers to be interested in the programming language for its own sake two possible course plans are illustrated in ix the first one presumes an arts humanities audience whereas the second one presumes a science engineering audience other course plans could cover the first five chapters then devote the remaining amount of time to a single area such as text classification chapters syntax chapters semantics chapter or linguistic data management chapter table ix suggested course plans approximate number of lectures per chapter chapter arts and humanities science and engineering n language processing and python n accessing text corpora and lexical resources n processing raw text n writing structured programs n categorizing and tagging words n learning to classify text n extracting information from text n analyzing sentence structure n building feature based grammars n analyzing the meaning of sentences n managing linguistic data ntotal conventions used in this book the following typographical conventions are used in this book bold indicates new terms italic used within paragraphs to refer to linguistic emples the names of texts and urls also used for filenames and file extensions constant width used for program listings as well as within paragraphs to refer to program elements such as variable or function names statements and keywords also used for program names constant width bold shows commands or other text that should be typed literally by the user constant width italic shows text that should be replaced with user supplied values or by values determined by context also used for metavariables within program code emples note this icon signifies a tip suggestion or general note caution this icon indicates a warning or caution using code emples this book is here to help you get your job done in general you may use the code in this book in your programs and documentation you do not need to contact us for permission unless you re reproducing a significant portion of the code for emple writing a program that uses several chunks of code from this book does not require permission selling or distributing a cd rom of emples from o reilly books does require permission answering a question by citing this book and quoting emple code does not require permission incorporating a significant amount of emple code from this book into your product s documentation does require permission we appreciate but do not require attribution an attribution usually includes the title author publisher and isbn for emple natural language processing with python by steven bird ewan klein and edward loper o areilly media if you feel your use of code emples falls outside fair use or the permission given above feel free to contact us at permissions oreilly com acknowledgments the authors are indebted to the following people for feedback on earlier drafts of this book doug arnold michaela atterer greg aumann kenneth beesley steven bethard ondrej bojar chris cieri robin cooper grev corbett james curran dan garrette jean mark gawron doug hellmann nitin indurkhya mark liberman peter ljungl f stefan m ller robin munn joel nothman adam przepiorkowski brandon rhodes stuart robinson jussi salmela kyle schlansker rob speer and richard sproat we are thankful to many students and colleagues for their comments on the class materials that evolved into these chapters including participants at nlp and linguistics summer schools in brazil india and the usa this book would not exist without the members of the nltk dev developer community named on the nltk website who have given so freely of their time and expertise in building and extending nltk we are grateful to the u s national science foundation the linguistic data consortium an edward clarence dyason fellowship and the universities of pennsylvania edinburgh and melbourne for supporting our work on this book we thank julie steele abby fox loranah dimant and the rest of the o areilly team for organizing comprehensive reviews of our drafts from people across the nlp and python communities for cheerfully customizing o areilly production tools and for meticulous copy editing work in preparing the revised edition for python we are grateful to michael korobov for leading the effort to port nltk to python and to antoine trux for his meticulous feedback on the first edition finally we owe a huge debt of gratitude to mimo and jee for their love patience and support over the many years that we worked on this book we hope that our children andrew alison kirsten leonie and maaike catch our enthusiasm for language and computation from these pages about the authors steven bird is associate professor in the department of computer science and software engineering at the university of melbourne and senior research associate in the linguistic data consortium at the university of pennsylvania he completed a phd on computational phonology at the university of edinburgh in supervised by ewan klein he later moved to cameroon to conduct linguistic fieldwork on the grassfields bantu languages under the auspices of the summer institute of linguistics more recently he spent several years as associate director of the linguistic data consortium where he led an r d team to create models and tools for large databases of annotated text at melbourne university he established a language technology research group and has taught at all levels of the undergraduate computer science curriculum in steven is president of the association for computational linguistics ewan klein is professor of language technology in the school of informatics at the university of edinburgh he completed a phd on formal semantics at the university of cambridge in after some years working at the universities of sussex and newcastle upon tyne ewan took up a teaching position at edinburgh he was involved in the establishment of edinburgh language technology group in and has been closely associated with it ever since from he took leave from the university to act as research manager for the edinburgh based natural language research group of edify corporation santa clara and was responsible for spoken dialogue processing ewan is a past president of the european chapter of the association for computational linguistics and was a founding member and coordinator of the european network of excellence in human language technologies elsnet edward loper has recently completed a phd on machine learning for natural language processing at the the university of pennsylvania edward was a student in steven graduate course on computational linguistics in the fall of and went on to be a ta and share in the development of nltk in addition to nltk he has helped develop two packages for documenting and testing python software epydoc and doctest royalties royalties from the sale of this book are being used to support the development of the natural language toolkit images authors png nfigure xiv edward loper ewan klein and steven bird stanford july about this document